{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- This is the SCRIPT for testing QUANTIZED MOBILENET after RETRAINING -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include\n",
    "import argparse\n",
    "import os, imp, sys\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import copy, math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my script\n",
    "sys.path.append('../')\n",
    "from data import get_dataset, get_num_classes\n",
    "from preprocess import get_transform\n",
    "from utils import *\n",
    "import models\n",
    "import quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = '/home/fariselli/Work/training-mixed-precision-quantized-networks/results-nips/mobilenet_224_0.75_quant_auto_fix/checkpoint.pth.tar'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'models.linear_quantized_modules.ScaledClippedLinearQuantizationChannel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'ScaledClippedLinearQuantizationChannelBias' on <module 'models.linear_quantized_modules' from '/home/fariselli/Work/training-mixed-precision-quantized-networks/models/linear_quantized_modules.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-03c8faca816c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         checkpoint_file, 'model_best.pth.tar')\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcheckpoint_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#checkpoint = checkpoint_loaded['state_dict']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#model.load_state_dict(checkpoint, strict=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'ScaledClippedLinearQuantizationChannelBias' on <module 'models.linear_quantized_modules' from '/home/fariselli/Work/training-mixed-precision-quantized-networks/models/linear_quantized_modules.py'>"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(checkpoint_file):\n",
    "#            results.load(os.path.join(checkpoint_file, 'results.csv'))\n",
    "    checkpoint_file = os.path.join(\n",
    "        checkpoint_file, 'model_best.pth.tar')\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    checkpoint_loaded = torch.load(checkpoint_file)\n",
    "    #checkpoint = checkpoint_loaded['state_dict']\n",
    "    #model.load_state_dict(checkpoint, strict=False)\n",
    "    print('Model pretrained' )\n",
    "else:\n",
    "    print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = checkpoint_loaded['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = checkpoint_loaded['config']\n",
    "activ_bits = model_config['activ_bits']\n",
    "activ_type = model_config['activ_type']\n",
    "dataset = model_config['dataset']\n",
    "input_dim = model_config['input_dim']\n",
    "input_size = input_dim\n",
    "num_classes = model_config['num_classes']\n",
    "type_quant = model_config['type_quant']\n",
    "weight_bits = model_config['weight_bits']\n",
    "width_mult = model_config['width_mult']\n",
    "additional_config = ''\n",
    "quant_add_config = checkpoint_loaded['add_config']\n",
    "fold_type = checkpoint_loaded['fold_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.846"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_loaded['best_prec1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "model = models.__dict__[model_name]\n",
    "\n",
    "nClasses = get_num_classes(dataset)\n",
    "model_config = {'input_size': input_size, 'dataset': dataset, 'num_classes': nClasses, \\\n",
    "                'type_quant': type_quant, 'weight_bits': weight_bits, 'activ_bits': activ_bits,\\\n",
    "                'activ_type': activ_type, 'width_mult': width_mult, 'input_dim': input_size }\n",
    "\n",
    "\n",
    "\n",
    "if additional_config is not '':\n",
    "    model_config = dict(model_config, **literal_eval(additional_config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activ_bits': 8,\n",
       " 'activ_type': 'learned',\n",
       " 'dataset': 'imagenet',\n",
       " 'input_dim': 224.0,\n",
       " 'input_size': 224.0,\n",
       " 'num_classes': 1000,\n",
       " 'type_quant': 'PerLayerSymPACT',\n",
       " 'weight_bits': 8,\n",
       " 'width_mult': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = checkpoint_loaded['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_load = checkpoint_loaded['quantizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n",
      "PerLayerSymPACT folding_weights\n"
     ]
    }
   ],
   "source": [
    "param_list = copy.deepcopy(quantizer_load.param_to_quantize)\n",
    "for item in param_list:\n",
    "    print(item['quant_type'], item['fold_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight', 'act_o_bits'])\n",
      "dict_keys(['quant_act', 'w_bits', 'name', 'batch_norm', 'act', 'quant_conv', 'conv', 'quant_type', 'bias', 'w_max_thr', 'w_clip', 'fold_type', 'bias_bits', 'weight'])\n"
     ]
    }
   ],
   "source": [
    "for item in quantizer_load.param_to_quantize:\n",
    "    print(item.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       " tensor([4.4838], device='cuda:0', requires_grad=True), inplace),\n",
       " 'act_o_bits': 8,\n",
       " 'batch_norm': BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True),\n",
       " 'bias': False,\n",
       " 'bias_bits': 8,\n",
       " 'conv': Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False),\n",
       " 'fold_type': 'folding_weights',\n",
       " 'name': '1.0',\n",
       " 'quant_act': ScaledClippedLinearQuantization(M0=0.6594409346580505, N0=-1, clip_val=255, inplace),\n",
       " 'quant_conv': Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32),\n",
       " 'quant_type': 'PerLayerSymPACT',\n",
       " 'w_bits': 8,\n",
       " 'w_clip': None,\n",
       " 'w_max_thr': Parameter containing:\n",
       " tensor(36.3637, device='cuda:0', requires_grad=True),\n",
       " 'weight': tensor([[[[ 1.9302e+00,  1.3625e+00,  2.1403e+00],\n",
       "           [ 4.5900e-01,  9.9429e-01,  1.5504e+00],\n",
       "           [ 1.7504e+00,  6.5618e-01,  1.3216e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 3.3594e+00,  5.6149e+00,  1.9531e+00],\n",
       "           [ 1.1709e+00,  1.1373e+00,  2.0799e+00],\n",
       "           [-4.6411e+00, -7.0828e+00, -4.4991e+00]]],\n",
       " \n",
       " \n",
       "         [[[-1.1773e+00,  1.1590e+00, -6.3939e-01],\n",
       "           [ 1.1286e+00,  4.0326e+00,  4.1636e-01],\n",
       "           [-5.5403e-01, -5.4693e-02, -9.4290e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.6464e+00, -2.5504e+00, -2.9712e+00],\n",
       "           [-1.2154e+00, -2.2671e+00, -1.7325e+00],\n",
       "           [-1.5393e+00, -2.2813e+00, -1.4683e+00]]],\n",
       " \n",
       " \n",
       "         [[[-2.9541e+00,  1.1815e+00,  1.4593e-01],\n",
       "           [ 3.0258e+00, -1.4839e+00,  1.8188e-02],\n",
       "           [ 1.1214e-01,  5.0811e-02,  2.2616e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 7.1663e+00, -5.3195e+00, -6.2939e-01],\n",
       "           [ 2.7025e+00, -4.1814e+00,  8.4232e-01],\n",
       "           [-8.3415e-01,  1.2132e+00, -1.5693e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.3628e+00, -1.5710e+00, -1.3928e+00],\n",
       "           [-2.7181e+00, -1.2647e+00, -3.3019e-01],\n",
       "           [-2.2736e+00, -2.1398e+00, -1.0234e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 2.9405e-03, -3.1195e-01,  1.7998e-01],\n",
       "           [ 5.2247e-01,  5.4534e+00, -1.6286e-01],\n",
       "           [-3.3249e-01,  2.9195e-01,  5.0215e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.2434e+00,  1.4647e+00,  1.5015e+00],\n",
       "           [ 1.6779e+00,  1.1835e+00,  1.7163e+00],\n",
       "           [ 3.3644e-01,  8.0083e-01,  1.1933e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 2.7602e+00,  2.2959e+00,  2.1474e+00],\n",
       "           [ 2.3600e+00,  2.0164e+00,  1.4759e+00],\n",
       "           [ 4.4810e-01,  1.2496e-01, -3.3640e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.6475e-01, -4.1241e-01,  1.3181e-01],\n",
       "           [ 3.1775e-01,  1.1903e+00,  1.5901e-01],\n",
       "           [ 2.9137e-01,  1.9058e+00,  2.3859e-01]]],\n",
       " \n",
       " \n",
       "         [[[-6.4212e-01,  9.7158e-01,  9.3887e-02],\n",
       "           [ 3.1850e+00, -8.5886e+00,  1.0330e+00],\n",
       "           [-8.5453e-01,  1.1875e+00,  2.7861e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.5837e-02, -3.1338e-01, -3.5518e-01],\n",
       "           [ 2.8429e-01, -3.7081e+00,  5.9810e-01],\n",
       "           [-3.9330e-01,  8.1867e-01, -8.1285e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0984e+00, -1.5712e+00,  6.4551e-01],\n",
       "           [-2.9885e+00,  6.6808e+00, -1.6685e+00],\n",
       "           [ 7.0330e-01,  4.1374e-02,  1.1474e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.1625e-01,  6.3422e-01,  3.0223e-01],\n",
       "           [-4.0187e-01, -1.1246e-01, -2.4416e-01],\n",
       "           [ 1.4486e+00,  5.5398e-03,  2.5987e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.1090e-01, -1.3345e+01,  1.0650e+00],\n",
       "           [ 9.3964e-01,  1.0444e+01, -1.7062e-01],\n",
       "           [-7.0451e-01,  2.5996e+00, -8.8987e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.3557e-01,  1.1502e+00, -2.5471e-01],\n",
       "           [ 2.9569e+00,  4.7997e+00,  1.3441e+00],\n",
       "           [ 1.3397e+00,  3.8283e+00,  1.5314e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 8.0056e-01, -3.3895e-01,  8.0152e-01],\n",
       "           [-1.6240e+00, -6.2955e+00, -1.2871e+00],\n",
       "           [ 1.6102e+00, -2.5934e+00,  5.8168e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 3.1019e-01, -1.6913e+00,  2.5704e-02],\n",
       "           [-1.3242e+00, -2.5321e+00, -1.3742e-01],\n",
       "           [-7.3992e-02, -7.7353e-01, -3.3610e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.1860e-01,  4.5575e+00, -2.0230e-01],\n",
       "           [-5.5682e-01, -3.2670e+00,  1.5002e-01],\n",
       "           [ 8.8282e-02, -2.0213e-01,  1.1701e-01]]],\n",
       " \n",
       " \n",
       "         [[[-4.7689e-01, -1.3723e+00, -1.9359e+00],\n",
       "           [-5.2440e-01, -2.8114e-01, -7.5589e-01],\n",
       "           [ 9.6647e-01,  9.6676e-02,  1.2896e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.5337e+00, -2.6401e+00, -2.1439e-01],\n",
       "           [-1.7797e+00, -1.7211e+00, -5.5455e-01],\n",
       "           [-5.6821e-01, -5.7365e-01,  5.7363e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.7567e+00,  1.6875e+00,  1.2521e+00],\n",
       "           [ 2.0818e+00,  1.5778e+00,  1.0090e+00],\n",
       "           [ 1.0523e+00,  9.5814e-01,  5.4490e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.4240e-01,  2.7024e-01, -4.0988e-02],\n",
       "           [ 1.5161e+01,  5.0276e-01, -1.5809e+01],\n",
       "           [ 5.7349e+00,  1.2099e+00, -6.4757e+00]]],\n",
       " \n",
       " \n",
       "         [[[-1.3317e+00, -1.3011e+00, -1.0505e+00],\n",
       "           [ 6.6771e-01,  1.3866e+00, -8.1926e-02],\n",
       "           [ 1.6689e+00,  1.4735e+00, -5.6742e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.1589e-01,  5.9333e-01, -3.1717e-01],\n",
       "           [-3.5431e+00, -1.8970e+00,  5.1900e-01],\n",
       "           [-3.4487e-01,  8.4970e-01, -3.1706e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 5.0488e-01, -5.5777e-01, -3.6006e-02],\n",
       "           [-2.0237e+00, -4.2978e+00,  6.4834e-01],\n",
       "           [ 8.0912e-01,  7.0244e-01, -9.2257e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2400e-01,  4.6090e-02, -8.4560e-01],\n",
       "           [-5.6461e-01, -7.5807e-01, -1.2660e+00],\n",
       "           [-7.2909e-01, -1.3064e+00, -1.7099e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.1139e-01, -1.6878e+00,  6.1602e-02],\n",
       "           [-2.7223e+00,  6.7600e+00, -9.1250e-01],\n",
       "           [ 4.4669e-01, -1.2188e+00, -3.7053e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.5292e+00, -2.1019e+00,  5.7235e-02],\n",
       "           [-1.0587e+00, -1.8283e+00, -4.9910e-01],\n",
       "           [ 1.0161e+00,  3.8713e-02,  8.5730e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.3533e+00, -1.6044e+00, -6.9005e-01],\n",
       "           [-1.6174e+00, -2.4297e+00, -1.8268e+00],\n",
       "           [-2.2875e+00, -2.7252e+00, -1.3794e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 7.7595e-01, -7.3663e-01,  4.8483e-01],\n",
       "           [-9.0355e+00, -1.0833e+01,  1.0985e+00],\n",
       "           [ 8.8899e+00,  1.0986e+01, -1.0396e+00]]]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "quantizer_load.param_to_quantize[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset='imagenet',num_classes=1000,input_size=224.0\n",
      "8 8 PerLayerSymPACT\n",
      "This is a quantized Mobilenet with alpha=  1.0  input_size:  224.0  activation bits:  8  weight bits:  8  activation type:  learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.weight Error\n",
      "fc.bias Error\n",
      "Model Pretrained Loaded\n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.__dict__[model_name]\n",
    "model = model(**model_config)\n",
    "if model is None :\n",
    "    print('ERORRR')\n",
    "    \n",
    "#logging.info(\"created model with configuration: %s\", model_config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_add_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Folding:  False Batch Folding Delay:  0 Type folding_weights\n",
      "[]\n",
      "This is the quantized network: \n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (2): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (3): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (4): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (5): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (6): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (7): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (8): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (10): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "    (11): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (12): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (13): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (14): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "    (15): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (16): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (17): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (18): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "    (19): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (20): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (21): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (22): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "    (23): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (24): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (25): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (26): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (27): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (28): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (29): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (30): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (31): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (32): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (33): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (34): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (35): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (36): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (37): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (38): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (39): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (40): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (41): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (42): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (43): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (44): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (45): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (46): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "    (47): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (48): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (49): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (50): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "    (51): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (52): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (53): ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quantizer = quantization.QuantOp(model, type_quant, weight_bits, batch_fold_delay=0,batch_fold_type=fold_type, act_bits=activ_bits, add_config=quant_add_config  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.batch_fold = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dateset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "input_eval_transform = transforms.Compose([\n",
    "        transforms.Scale(int(input_size) ),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "input_transform = getattr(model, 'input_transform', input_eval_transform)\n",
    "\n",
    "val_data = get_dataset(dataset, 'val', input_transform['eval'])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=32, shuffle=False,\n",
    "    num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and quantizer!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(quantizer_load.param_to_quantize):\n",
    "    for key in item.keys():\n",
    "        if key in ['bias_bits', 'quant_type','w_min_thr','min_clip', 'w_clip','w_max_thr','max_clip', 'fold_type','w_bits']:\n",
    "            quantizer.param_to_quantize[i][key] = item[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  0  | Sw:  tensor(0.0069)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  0  | Quant Bias Tensor - Error:  tensor(1062880.8750)\n",
      "Layer PerLayerAsymPACT:  0  | M0:  tensor(0.5902) - n_exp:  -8\n",
      "Layer PerLayerAsymPACT:  0  | Error:  tensor(0.)\n",
      "Layer:  1  | Sw:  tensor(0.2852)\n",
      "[WEIGHT] Min:  -112.0 | Max:  127.0\n",
      "Layer:  1  | Quant Bias Tensor - Error:  tensor(4746.5449)\n",
      "Layer PerLayerAsymPACT:  1  | M0:  tensor(0.5704) - n_exp:  -1\n",
      "Layer PerLayerAsymPACT:  1  | Error:  tensor(0.)\n",
      "Layer:  2  | Sw:  tensor(0.0101)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  2  | Quant Bias Tensor - Error:  tensor(973588.1250)\n",
      "Layer PerLayerAsymPACT:  2  | M0:  tensor(0.6490) - n_exp:  -6\n",
      "Layer PerLayerAsymPACT:  2  | Error:  tensor(0.)\n",
      "Layer:  3  | Sw:  tensor(0.3635)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  3  | Quant Bias Tensor - Error:  tensor(4230.6436)\n",
      "Layer PerLayerAsymPACT:  3  | M0:  tensor(0.7271) - n_exp:  -1\n",
      "Layer PerLayerAsymPACT:  3  | Error:  tensor(0.)\n",
      "Layer:  4  | Sw:  tensor(0.0078)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  4  | Quant Bias Tensor - Error:  tensor(1364015.2500)\n",
      "Layer PerLayerAsymPACT:  4  | M0:  tensor(0.9988) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  4  | Error:  tensor(0.)\n",
      "Layer:  5  | Sw:  tensor(0.0502)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  5  | Quant Bias Tensor - Error:  tensor(111704.2031)\n",
      "Layer PerLayerAsymPACT:  5  | M0:  tensor(0.8035) - n_exp:  -4\n",
      "Layer PerLayerAsymPACT:  5  | Error:  tensor(0.)\n",
      "Layer:  6  | Sw:  tensor(0.0068)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  6  | Quant Bias Tensor - Error:  tensor(1340741.)\n",
      "Layer PerLayerAsymPACT:  6  | M0:  tensor(0.8690) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  6  | Error:  tensor(0.)\n",
      "Layer:  7  | Sw:  tensor(0.0127)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  7  | Quant Bias Tensor - Error:  tensor(673083.2500)\n",
      "Layer PerLayerAsymPACT:  7  | M0:  tensor(0.8126) - n_exp:  -6\n",
      "Layer PerLayerAsymPACT:  7  | Error:  tensor(0.)\n",
      "Layer:  8  | Sw:  tensor(0.0067)\n",
      "[WEIGHT] Min:  -128.0 | Max:  116.0\n",
      "Layer:  8  | Quant Bias Tensor - Error:  tensor(2163893.)\n",
      "Layer PerLayerAsymPACT:  8  | M0:  tensor(0.8525) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  8  | Error:  tensor(0.)\n",
      "Layer:  9  | Sw:  tensor(0.0285)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  9  | Quant Bias Tensor - Error:  tensor(385924.8438)\n",
      "Layer PerLayerAsymPACT:  9  | M0:  tensor(0.9125) - n_exp:  -5\n",
      "Layer PerLayerAsymPACT:  9  | Error:  tensor(0.)\n",
      "Layer:  10  | Sw:  tensor(0.0062)\n",
      "[WEIGHT] Min:  -122.0 | Max:  127.0\n",
      "Layer:  10  | Quant Bias Tensor - Error:  tensor(1611383.2500)\n",
      "Layer PerLayerAsymPACT:  10  | M0:  tensor(0.7987) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  10  | Error:  tensor(0.)\n",
      "Layer:  11  | Sw:  tensor(0.0090)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  11  | Quant Bias Tensor - Error:  tensor(1683485.6250)\n",
      "Layer PerLayerAsymPACT:  11  | M0:  tensor(0.5775) - n_exp:  -6\n",
      "Layer PerLayerAsymPACT:  11  | Error:  tensor(0.)\n",
      "Layer:  12  | Sw:  tensor(0.0043)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  12  | Quant Bias Tensor - Error:  tensor(6261183.)\n",
      "Layer PerLayerAsymPACT:  12  | M0:  tensor(0.5442) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  12  | Error:  tensor(0.)\n",
      "Layer:  13  | Sw:  tensor(0.0253)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  13  | Quant Bias Tensor - Error:  tensor(675264.2500)\n",
      "Layer PerLayerAsymPACT:  13  | M0:  tensor(0.8080) - n_exp:  -5\n",
      "Layer PerLayerAsymPACT:  13  | Error:  tensor(0.)\n",
      "Layer:  14  | Sw:  tensor(0.0039)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  14  | Quant Bias Tensor - Error:  tensor(4600697.)\n",
      "Layer PerLayerAsymPACT:  14  | M0:  tensor(0.9887) - n_exp:  -8\n",
      "Layer PerLayerAsymPACT:  14  | Error:  tensor(0.)\n",
      "Layer:  15  | Sw:  tensor(0.0234)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  15  | Quant Bias Tensor - Error:  tensor(1020012.0625)\n",
      "Layer PerLayerAsymPACT:  15  | M0:  tensor(0.7502) - n_exp:  -5\n",
      "Layer PerLayerAsymPACT:  15  | Error:  tensor(0.)\n",
      "Layer:  16  | Sw:  tensor(0.0037)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  16  | Quant Bias Tensor - Error:  tensor(4996324.5000)\n",
      "Layer PerLayerAsymPACT:  16  | M0:  tensor(0.9429) - n_exp:  -8\n",
      "Layer PerLayerAsymPACT:  16  | Error:  tensor(0.)\n",
      "Layer:  17  | Sw:  tensor(0.0205)\n",
      "[WEIGHT] Min:  -127.0 | Max:  127.0\n",
      "Layer:  17  | Quant Bias Tensor - Error:  tensor(1116113.2500)\n",
      "Layer PerLayerAsymPACT:  17  | M0:  tensor(0.6555) - n_exp:  -5\n",
      "Layer PerLayerAsymPACT:  17  | Error:  tensor(0.)\n",
      "Layer:  18  | Sw:  tensor(0.0048)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  18  | Quant Bias Tensor - Error:  tensor(3646931.2500)\n",
      "Layer PerLayerAsymPACT:  18  | M0:  tensor(0.6157) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  18  | Error:  tensor(0.)\n",
      "Layer:  19  | Sw:  tensor(0.0196)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  19  | Quant Bias Tensor - Error:  tensor(1163960.)\n",
      "Layer PerLayerAsymPACT:  19  | M0:  tensor(0.6277) - n_exp:  -5\n",
      "Layer PerLayerAsymPACT:  19  | Error:  tensor(0.)\n",
      "Layer:  20  | Sw:  tensor(0.0049)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  20  | Quant Bias Tensor - Error:  tensor(4318374.)\n",
      "Layer PerLayerAsymPACT:  20  | M0:  tensor(0.6247) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  20  | Error:  tensor(0.)\n",
      "Layer:  21  | Sw:  tensor(0.0168)\n",
      "[WEIGHT] Min:  -122.0 | Max:  127.0\n",
      "Layer:  21  | Quant Bias Tensor - Error:  tensor(1340597.2500)\n",
      "Layer PerLayerAsymPACT:  21  | M0:  tensor(0.5369) - n_exp:  -5\n",
      "Layer PerLayerAsymPACT:  21  | Error:  tensor(0.)\n",
      "Layer:  22  | Sw:  tensor(0.0032)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  22  | Quant Bias Tensor - Error:  tensor(7418407.)\n",
      "Layer PerLayerAsymPACT:  22  | M0:  tensor(0.8213) - n_exp:  -8\n",
      "Layer PerLayerAsymPACT:  22  | Error:  tensor(0.)\n",
      "Layer:  23  | Sw:  tensor(0.0078)\n",
      "[WEIGHT] Min:  -126.0 | Max:  127.0\n",
      "Layer:  23  | Quant Bias Tensor - Error:  tensor(2511362.7500)\n",
      "Layer PerLayerAsymPACT:  23  | M0:  tensor(0.9964) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  23  | Error:  tensor(0.)\n",
      "Layer:  24  | Sw:  tensor(0.0044)\n",
      "[WEIGHT] Min:  -128.0 | Max:  127.0\n",
      "Layer:  24  | Quant Bias Tensor - Error:  tensor(10383246.)\n",
      "Layer PerLayerAsymPACT:  24  | M0:  tensor(0.5673) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  24  | Error:  tensor(0.)\n",
      "Layer:  25  | Sw:  tensor(0.1682)\n",
      "[WEIGHT] Min:  -128.0 | Max:  34.0\n",
      "Layer:  25  | Quant Bias Tensor - Error:  tensor(117041.3281)\n",
      "Layer PerLayerAsymPACT:  25  | M0:  tensor(0.6727) - n_exp:  -2\n",
      "Layer PerLayerAsymPACT:  25  | Error:  tensor(0.)\n",
      "Layer:  26  | Sw:  tensor(0.0040)\n",
      "[WEIGHT] Min:  -127.0 | Max:  127.0\n",
      "Layer:  26  | Quant Bias Tensor - Error:  tensor(35378324.)\n",
      "Layer PerLayerAsymPACT:  26  | M0:  tensor(0.5087) - n_exp:  -7\n",
      "Layer PerLayerAsymPACT:  26  | Error:  tensor(0.)\n",
      "Layer:  27  | Sw:  tensor(0.0037)\n",
      "Layer:  27  | Quant Bias Tensor - Error:  tensor(2455247.7500)\n"
     ]
    }
   ],
   "source": [
    "quantizer.generate_deployment_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data_loader, model,  epoch=0, training=False, quantizer=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    max_i = 1\n",
    "    min_i = -1\n",
    "    bit_i = 8\n",
    "    n_steps = (2**bit_i)-1\n",
    "    eps = (max_i-min_i) / n_steps\n",
    "    \n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = Variable(inputs.cuda(), volatile=not training)\n",
    "        target_var = Variable(target)\n",
    "        \n",
    "        input_var = input_var.clamp(min_i,max_i).div(eps).round()\n",
    "        \n",
    "        if quantizer is not None:\n",
    "            input_var = input_var.mul(eps)\n",
    "            quantizer.store_and_quantize(training=False )\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "        values, indices = output.max(1)\n",
    "        #print(indices, target)\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if quantizer is not None:\n",
    "            quantizer.restore_real_value()\n",
    "\n",
    "            \n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, top1=top1, top5=top5))\n",
    "\n",
    "    print('Top1: ', top1.avg)\n",
    "    return top1.avg, top5.avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/1563]\tTime 0.829 (0.829)\tData 0.772 (0.772)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
      "EVALUATING - Epoch: [0][100/1563]\tTime 0.014 (0.043)\tData 0.000 (0.029)\tPrec@1 43.750 (61.541)\tPrec@5 68.750 (82.704)\n",
      "EVALUATING - Epoch: [0][200/1563]\tTime 0.024 (0.048)\tData 0.000 (0.033)\tPrec@1 62.500 (63.884)\tPrec@5 84.375 (83.831)\n",
      "EVALUATING - Epoch: [0][300/1563]\tTime 0.028 (0.045)\tData 0.000 (0.028)\tPrec@1 53.125 (63.715)\tPrec@5 81.250 (84.956)\n",
      "EVALUATING - Epoch: [0][400/1563]\tTime 0.020 (0.042)\tData 0.000 (0.025)\tPrec@1 43.750 (62.562)\tPrec@5 78.125 (85.419)\n",
      "EVALUATING - Epoch: [0][500/1563]\tTime 0.017 (0.041)\tData 0.000 (0.024)\tPrec@1 75.000 (62.625)\tPrec@5 96.875 (85.778)\n",
      "EVALUATING - Epoch: [0][600/1563]\tTime 0.014 (0.041)\tData 0.000 (0.023)\tPrec@1 78.125 (63.192)\tPrec@5 87.500 (85.935)\n",
      "EVALUATING - Epoch: [0][700/1563]\tTime 0.027 (0.040)\tData 0.000 (0.023)\tPrec@1 53.125 (61.992)\tPrec@5 81.250 (84.950)\n",
      "EVALUATING - Epoch: [0][800/1563]\tTime 0.022 (0.040)\tData 0.000 (0.022)\tPrec@1 68.750 (60.565)\tPrec@5 87.500 (83.684)\n",
      "EVALUATING - Epoch: [0][900/1563]\tTime 0.015 (0.039)\tData 0.000 (0.022)\tPrec@1 81.250 (60.024)\tPrec@5 93.750 (83.040)\n",
      "EVALUATING - Epoch: [0][1000/1563]\tTime 0.014 (0.039)\tData 0.000 (0.022)\tPrec@1 68.750 (59.103)\tPrec@5 84.375 (82.293)\n",
      "EVALUATING - Epoch: [0][1100/1563]\tTime 0.026 (0.039)\tData 0.000 (0.022)\tPrec@1 65.625 (58.447)\tPrec@5 81.250 (81.744)\n",
      "EVALUATING - Epoch: [0][1200/1563]\tTime 0.023 (0.040)\tData 0.000 (0.022)\tPrec@1 71.875 (57.874)\tPrec@5 78.125 (81.227)\n",
      "EVALUATING - Epoch: [0][1300/1563]\tTime 0.014 (0.041)\tData 0.000 (0.023)\tPrec@1 71.875 (57.341)\tPrec@5 78.125 (80.777)\n",
      "EVALUATING - Epoch: [0][1400/1563]\tTime 0.014 (0.042)\tData 0.000 (0.024)\tPrec@1 68.750 (56.799)\tPrec@5 87.500 (80.237)\n",
      "EVALUATING - Epoch: [0][1500/1563]\tTime 0.015 (0.048)\tData 0.000 (0.031)\tPrec@1 43.750 (56.920)\tPrec@5 78.125 (80.411)\n",
      "Top1:  tensor(56.9220, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(56.9220, device='cuda:0'), tensor(80.3720, device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.generate_deployment_model()\n",
    "quantizer.deployment_model.eval()\n",
    "forward(val_loader, quantizer.deployment_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.imagenet.mobilenet.mobilenet_quant_devel'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5902066230773926 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -112.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5704110860824585 | [N0]:  -1\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6489582657814026 | [N0]:  -6\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -113.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.7270773649215698 | [N0]:  -1\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9988483190536499 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8034564852714539 | [N0]:  -4\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8689922094345093 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.812638521194458 | [N0]:  -6\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  116.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8525304198265076 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.912525475025177 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -122.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.798747181892395 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.577545166015625 | [N0]:  -6\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5441548824310303 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8080103993415833 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9887223839759827 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.7502198815345764 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9428861141204834 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -127.0 | Max:  127.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6555109024047852 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.61571204662323 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6276821494102478 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6247415542602539 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -122.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5368722081184387 | [N0]:  -5\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.8213418126106262 | [N0]:  -8\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -126.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.9964178204536438 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  128.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5672622919082642 | [N0]:  -7\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -128.0 | Max:  34.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.6727413535118103 | [N0]:  -2\n",
      "<class 'models.linear_quantized_modules.Conv2d_SAME'>\n",
      "[WEIGHT] Min:  -127.0 | Max:  127.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n",
      "<class 'models.linear_quantized_modules.ScaledClippedLinearQuantization'>\n",
      "[M0]  0.5086777210235596 | [N0]:  -7\n",
      "<class 'torch.nn.modules.pooling.AvgPool2d'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "[WEIGHT] Min:  -100.0 | Max:  127.0\n",
      "[BIAS] Min:  -128.0 | Max:  127.0\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(quantizer.deployment_model.modules() ):\n",
    "    print(type(item))\n",
    "    if type(item) in [models.linear_quantized_modules.Conv2d_SAME,torch.nn.modules.linear.Linear]:\n",
    "        print('[WEIGHT] Min: ',item.weight.data.min().item(),'| Max: ',item.weight.data.max().item())\n",
    "        print('[BIAS] Min: ',item.bias.data.min().item(),'| Max: ',item.bias.data.max().item())\n",
    "    if type(item) in [models.linear_quantized_modules.ScaledClippedLinearQuantization]:\n",
    "        print('[M0] ',item.M_ZERO,'| [N0]: ',item.N_ZERO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.999999999328193"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(2**31-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test trained model\n",
    "forward(val_loader, quantizer_load.deployment_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([        nan,         nan,         nan,  2.3262e-43,         nan,\n",
       "                nan,         nan,         nan,         nan,         nan,\n",
       "                nan, -2.3262e-43,         nan,         nan,  4.4491e+00,\n",
       "                nan,         nan, -2.3262e-43,  2.3262e-43, -2.3262e-43,\n",
       "         2.3262e-43,         nan,         nan, -2.3262e-43], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model[1][1].running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.deployment_model.fc[0].bias.data.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.module.fc.bias.data/quantizer.deployment_model.fc[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/1563]\tTime 10.679 (10.679)\tData 0.815 (0.815)\tPrec@1 90.625 (90.625)\tPrec@5 93.750 (93.750)\n",
      "EVALUATING - Epoch: [0][100/1563]\tTime 0.099 (0.308)\tData 0.001 (0.105)\tPrec@1 9.375 (59.561)\tPrec@5 68.750 (85.210)\n",
      "EVALUATING - Epoch: [0][200/1563]\tTime 0.143 (0.295)\tData 0.001 (0.142)\tPrec@1 56.250 (59.142)\tPrec@5 87.500 (84.593)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-13:\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-11:\n",
      "Process Process-10:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 875, in convert\n",
      "    self.load()\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 103, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\", line 236, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/functional.py\", line 200, in resize\n",
      "    return img.resize((ow, oh), interpolation)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 1745, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "KeyboardInterrupt\n",
      "Process Process-14:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2539, in open\n",
      "    prefix = fp.read(16)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 875, in convert\n",
      "    self.load()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\", line 214, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "Process Process-16:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2539, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 2539, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-4e564a0c285f>\", line 2, in <module>\n",
      "    forward(val_loader, model,quantizer=quantizer )\n",
      "  File \"<ipython-input-25-07ea60390c3a>\", line 18, in forward\n",
      "    for i, (inputs, target) in enumerate(data_loader):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 330, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 309, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 293, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "RuntimeError: DataLoader worker (pid 11953) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 715, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 684, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 669, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.5/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "SystemError: <built-in function _error_if_any_worker_fails> returned a result with an error set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "forward(val_loader, model,quantizer=quantizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = model.module.model[0][0].weight.data.clone().clamp(quantizer.param_to_quantize[0]['w_min_thr'].data,quantizer.param_to_quantize[0]['w_max_thr'].data)\n",
    "w_min,w_max = weight_tensor.min().item(),weight_tensor.max().item()\n",
    "nlevels = ((2**8)-1)\n",
    "Sw= (w_max-w_min)/nlevels\n",
    "Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize(training=False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_load.deployment_model.model[0].weight.data.mul(Sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.model[0][0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore_real_value()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.param_to_quantize[27]['batch_norm'] is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = './results' #results dir\n",
    "save = 'imagenet/mobilenet_test_8bit' #saved folder\n",
    "dataset = 'imagenet' #dataset name or folder\n",
    "model_name = 'mobilenet' #model architecture\n",
    "\n",
    "input_size = 224 #image input size\n",
    "additional_config = str() #additional architecture configuration (need to be string str() )\n",
    "ttype = 'torch.cuda.FloatTensor' #type of tensor - e.g torch.cuda.HalfTensor\n",
    "gpus = '0' #gpus used for training - e.g 0,1,3\n",
    "workers = 8 #number of data loading workers\n",
    "epochs = 60 # number of total epochs to run\n",
    "start_epoch = 0 # manual epoch number (useful on restarts)\n",
    "batch_size = 1 # mini-batch size \n",
    "optimizer = 'SGD' # optimizer function used\n",
    "lr = 0.1 # initial learning rate\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4 #weight decay\n",
    "print_freq = 10 # print frequency\n",
    "resume = '' #path to latest checkpoint\n",
    "evaluate = False # evaluate model FILE on validation set\n",
    "save_check = False # saving the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup GPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cuda' in ttype:\n",
    "    gpus = [int(i) for i in gpus.split(',')]\n",
    "    print('Selected GPUs: ', gpus)\n",
    "    torch.cuda.set_device(gpus[0])\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    gpus = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Real model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_quant = 'LinearAsymmetricLayer' #type of binarization process\n",
    "#type_quant = None\n",
    "weight_bits = 4\n",
    "activ_bits = 4\n",
    "activ_type = 'learned' \n",
    "i_dim = 224\n",
    "width_mult = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size=224,num_classes=1000,dataset='imagenet'\n",
      "4 4 LinearAsymmetricLayer\n",
      "This is a quantized Mobilenet with alpha=  1  input_size:  224  activation bits:  4  weight bits:  4  activation type:  learned\n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=2, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
      "      tensor([6.], requires_grad=True), inplace)\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.__dict__[model_name]\n",
    "nClasses = get_num_classes(dataset)\n",
    "model_config = {'input_size': input_size, 'dataset': dataset, 'num_classes': nClasses, \\\n",
    "                'type_quant': type_quant, 'weight_bits': weight_bits, 'activ_bits': activ_bits,\\\n",
    "                'activ_type': activ_type, 'width_mult': width_mult, 'input_dim': i_dim }\n",
    "\n",
    "if additional_config is not '':\n",
    "    model_config = dict(model_config, **literal_eval(additional_config))\n",
    "\n",
    "model = model(**model_config)\n",
    "if model is None :\n",
    "    print('ERORRR')\n",
    "    \n",
    "#logging.info(\"created model with configuration: %s\", model_config)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=2, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=4, clip_val=Parameter containing:\n",
       "      tensor([6.], device='cuda:0', requires_grad=True), inplace)\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.type(ttype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.type(ttype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "default_transform = {\n",
    "    'train': get_transform(dataset,\n",
    "                           input_size=input_size, augment=True),\n",
    "    'eval': get_transform(dataset,\n",
    "                          input_size=input_size, augment=False)\n",
    "}\n",
    "transform = getattr(model, 'input_transform', default_transform)\n",
    "regime = getattr(model, 'regime', {0: {'optimizer': optimizer,\n",
    "                                       'lr': lr,\n",
    "                                       'momentum': momentum,\n",
    "                                       'weight_decay': weight_decay}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = get_dataset(dataset, 'val', transform['eval'])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "\n",
    "\n",
    "train_data = get_dataset(dataset, 'train', transform['train'])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = dict(model.named_parameters())\n",
    "params = []\n",
    "for key, value in params_dict.items():\n",
    "    if 'clip_val' in key:\n",
    "        params += [{'params':value,'weight_decay': 5e-4}]\n",
    "    else:\n",
    "        params += [{'params':value}]\n",
    "optimizer = torch.optim.SGD(params, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = adjust_optimizer(optimizer, 0, regime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 3\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 4\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 5\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 6\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 7\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 8\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 9\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 10\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 11\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 12\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 13\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 14\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 15\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 16\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 17\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 18\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 19\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 20\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 21\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 22\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 23\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 24\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 25\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 26\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 27\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 28\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 29\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 30\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 31\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 32\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 33\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 34\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 35\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 36\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 37\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 38\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 39\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 40\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 41\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 42\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 43\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 44\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 45\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 46\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 47\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 48\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 49\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 50\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 51\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 52\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 53\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 54\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 55\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 56\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 57\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 58\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 59\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 60\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 61\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 62\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 63\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 64\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 65\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 66\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 67\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 68\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 69\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 70\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 71\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 72\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 73\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 74\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 75\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 76\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 77\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 78\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 79\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 80\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 81\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 82\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 83\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 84\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 85\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 86\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 87\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 88\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 89\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 90\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 91\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 92\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 93\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 94\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 95\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 96\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 97\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 98\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 99\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 100\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 101\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 102\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 103\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 104\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 105\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 106\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       "\n",
       "Parameter Group 107\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 108\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 109\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    dampening: 0\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_config = [{'layer':0,'a_bits':4,'w_bits':8},{'layer':5,'a_bits':5,'w_bits':8}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "test_str = '[{\"layer\":0,\"a_bits\":4,\"w_bits\":8},{\"layer\":5,\"a_bits\":5,\"w_bits\":8}]'\n",
    "print(type(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'layer': 0, 'a_bits': 4, 'w_bits': 8}, {'layer': 5, 'a_bits': 5, 'w_bits': 8}]\n"
     ]
    }
   ],
   "source": [
    "my_dict = json.loads(test_str)\n",
    "print (my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Folding:  False Batch Folding Delay:  0 Type folding_thresh\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "Tipe of Quantization:  LinearAsymmetricLayer\n",
      "mobilenet_quant_devel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (3): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (4): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (5): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (7): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (8): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (9): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "    (11): ScaledThresholdsQuantization4d(M=1, n_thresholds=31, inplace)\n",
      "    (12): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (13): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "    (15): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (16): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (17): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "    (19): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (20): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (21): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "    (23): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (24): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (25): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (27): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (29): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (31): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (33): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (35): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (36): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (37): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (38): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (39): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (40): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (41): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (43): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (44): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (45): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "    (47): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (48): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (49): ScaledThresholdsQuantization4d(M=1, n_thresholds=3, inplace)\n",
      "    (50): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "    (51): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (52): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (53): ScaledThresholdsQuantization4d(M=1, n_thresholds=15, inplace)\n",
      "    (54): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quantizer = quantization.QuantOp(model, type_quant, weight_bits, batch_fold_delay=0, act_bits=activ_bits,add_config=add_config  )\n",
    "\n",
    "quantizer.deployment_model.type(ttype)\n",
    "quantizer.add_params_to_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.init_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 module.model.0.0.weight\n",
      "1 module.model.0.1.weight\n",
      "2 module.model.0.1.bias\n",
      "3 module.model.0.1.running_mean\n",
      "4 module.model.0.1.running_var\n",
      "5 module.model.0.1.num_batches_tracked\n",
      "6 module.model.1.0.weight\n",
      "7 module.model.1.1.weight\n",
      "8 module.model.1.1.bias\n",
      "9 module.model.1.1.running_mean\n",
      "10 module.model.1.1.running_var\n",
      "11 module.model.1.1.num_batches_tracked\n",
      "12 module.model.2.0.weight\n",
      "13 module.model.2.1.weight\n",
      "14 module.model.2.1.bias\n",
      "15 module.model.2.1.running_mean\n",
      "16 module.model.2.1.running_var\n",
      "17 module.model.2.1.num_batches_tracked\n",
      "18 module.model.3.0.weight\n",
      "19 module.model.3.1.weight\n",
      "20 module.model.3.1.bias\n",
      "21 module.model.3.1.running_mean\n",
      "22 module.model.3.1.running_var\n",
      "23 module.model.3.1.num_batches_tracked\n",
      "24 module.model.4.0.weight\n",
      "25 module.model.4.1.weight\n",
      "26 module.model.4.1.bias\n",
      "27 module.model.4.1.running_mean\n",
      "28 module.model.4.1.running_var\n",
      "29 module.model.4.1.num_batches_tracked\n",
      "30 module.model.5.0.weight\n",
      "31 module.model.5.1.weight\n",
      "32 module.model.5.1.bias\n",
      "33 module.model.5.1.running_mean\n",
      "34 module.model.5.1.running_var\n",
      "35 module.model.5.1.num_batches_tracked\n",
      "36 module.model.6.0.weight\n",
      "37 module.model.6.1.weight\n",
      "38 module.model.6.1.bias\n",
      "39 module.model.6.1.running_mean\n",
      "40 module.model.6.1.running_var\n",
      "41 module.model.6.1.num_batches_tracked\n",
      "42 module.model.7.0.weight\n",
      "43 module.model.7.1.weight\n",
      "44 module.model.7.1.bias\n",
      "45 module.model.7.1.running_mean\n",
      "46 module.model.7.1.running_var\n",
      "47 module.model.7.1.num_batches_tracked\n",
      "48 module.model.8.0.weight\n",
      "49 module.model.8.1.weight\n",
      "50 module.model.8.1.bias\n",
      "51 module.model.8.1.running_mean\n",
      "52 module.model.8.1.running_var\n",
      "53 module.model.8.1.num_batches_tracked\n",
      "54 module.model.9.0.weight\n",
      "55 module.model.9.1.weight\n",
      "56 module.model.9.1.bias\n",
      "57 module.model.9.1.running_mean\n",
      "58 module.model.9.1.running_var\n",
      "59 module.model.9.1.num_batches_tracked\n",
      "60 module.model.10.0.weight\n",
      "61 module.model.10.1.weight\n",
      "62 module.model.10.1.bias\n",
      "63 module.model.10.1.running_mean\n",
      "64 module.model.10.1.running_var\n",
      "65 module.model.10.1.num_batches_tracked\n",
      "66 module.model.11.0.weight\n",
      "67 module.model.11.1.weight\n",
      "68 module.model.11.1.bias\n",
      "69 module.model.11.1.running_mean\n",
      "70 module.model.11.1.running_var\n",
      "71 module.model.11.1.num_batches_tracked\n",
      "72 module.model.12.0.weight\n",
      "73 module.model.12.1.weight\n",
      "74 module.model.12.1.bias\n",
      "75 module.model.12.1.running_mean\n",
      "76 module.model.12.1.running_var\n",
      "77 module.model.12.1.num_batches_tracked\n",
      "78 module.model.13.0.weight\n",
      "79 module.model.13.1.weight\n",
      "80 module.model.13.1.bias\n",
      "81 module.model.13.1.running_mean\n",
      "82 module.model.13.1.running_var\n",
      "83 module.model.13.1.num_batches_tracked\n",
      "84 module.model.14.0.weight\n",
      "85 module.model.14.1.weight\n",
      "86 module.model.14.1.bias\n",
      "87 module.model.14.1.running_mean\n",
      "88 module.model.14.1.running_var\n",
      "89 module.model.14.1.num_batches_tracked\n",
      "90 module.model.15.0.weight\n",
      "91 module.model.15.1.weight\n",
      "92 module.model.15.1.bias\n",
      "93 module.model.15.1.running_mean\n",
      "94 module.model.15.1.running_var\n",
      "95 module.model.15.1.num_batches_tracked\n",
      "96 module.model.16.0.weight\n",
      "97 module.model.16.1.weight\n",
      "98 module.model.16.1.bias\n",
      "99 module.model.16.1.running_mean\n",
      "100 module.model.16.1.running_var\n",
      "101 module.model.16.1.num_batches_tracked\n",
      "102 module.model.17.0.weight\n",
      "103 module.model.17.1.weight\n",
      "104 module.model.17.1.bias\n",
      "105 module.model.17.1.running_mean\n",
      "106 module.model.17.1.running_var\n",
      "107 module.model.17.1.num_batches_tracked\n",
      "108 module.model.18.0.weight\n",
      "109 module.model.18.1.weight\n",
      "110 module.model.18.1.bias\n",
      "111 module.model.18.1.running_mean\n",
      "112 module.model.18.1.running_var\n",
      "113 module.model.18.1.num_batches_tracked\n",
      "114 module.model.19.0.weight\n",
      "115 module.model.19.1.weight\n",
      "116 module.model.19.1.bias\n",
      "117 module.model.19.1.running_mean\n",
      "118 module.model.19.1.running_var\n",
      "119 module.model.19.1.num_batches_tracked\n",
      "120 module.model.20.0.weight\n",
      "121 module.model.20.1.weight\n",
      "122 module.model.20.1.bias\n",
      "123 module.model.20.1.running_mean\n",
      "124 module.model.20.1.running_var\n",
      "125 module.model.20.1.num_batches_tracked\n",
      "126 module.model.21.0.weight\n",
      "127 module.model.21.1.weight\n",
      "128 module.model.21.1.bias\n",
      "129 module.model.21.1.running_mean\n",
      "130 module.model.21.1.running_var\n",
      "131 module.model.21.1.num_batches_tracked\n",
      "132 module.model.22.0.weight\n",
      "133 module.model.22.1.weight\n",
      "134 module.model.22.1.bias\n",
      "135 module.model.22.1.running_mean\n",
      "136 module.model.22.1.running_var\n",
      "137 module.model.22.1.num_batches_tracked\n",
      "138 module.model.23.0.weight\n",
      "139 module.model.23.1.weight\n",
      "140 module.model.23.1.bias\n",
      "141 module.model.23.1.running_mean\n",
      "142 module.model.23.1.running_var\n",
      "143 module.model.23.1.num_batches_tracked\n",
      "144 module.model.24.0.weight\n",
      "145 module.model.24.1.weight\n",
      "146 module.model.24.1.bias\n",
      "147 module.model.24.1.running_mean\n",
      "148 module.model.24.1.running_var\n",
      "149 module.model.24.1.num_batches_tracked\n",
      "150 module.model.25.0.weight\n",
      "151 module.model.25.1.weight\n",
      "152 module.model.25.1.bias\n",
      "153 module.model.25.1.running_mean\n",
      "154 module.model.25.1.running_var\n",
      "155 module.model.25.1.num_batches_tracked\n",
      "156 module.model.26.0.weight\n",
      "157 module.model.26.1.weight\n",
      "158 module.model.26.1.bias\n",
      "159 module.model.26.1.running_mean\n",
      "160 module.model.26.1.running_var\n",
      "161 module.model.26.1.num_batches_tracked\n",
      "162 module.fc.weight\n",
      "163 module.fc.bias\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(checkpoint):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----   End Initialization --------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = model.module.model[1][0].weight.data.clone()\n",
    "batch_layer = model.module.model[1][1]\n",
    "weight_tensor_size = model.module.model[1][0].weight.data.size()\n",
    "v0,v1,v2,v3 = weight_tensor_size\n",
    "eps = batch_layer.eps\n",
    "gamma_tensor = batch_layer.weight.data \n",
    "beta_tensor = batch_layer.bias.data \n",
    "mu_tensor = batch_layer.running_mean\n",
    "var_tensor = batch_layer.running_var\n",
    "\n",
    "#assuming convolution bias == False\n",
    "bias_tensor = mu_tensor.clone() \n",
    "bias_tensor = bias_tensor.mul(-1)\n",
    "\n",
    "#folded weight\n",
    "#var_tensor[var_tensor.le(0)] = 0       #correction to prevent case sigma >= 0\n",
    "if var_tensor.le(0).sum() >0:\n",
    "    print('erorr')\n",
    "sigma_tensor = var_tensor.add(eps).sqrt()\n",
    "gamma_over_sigma = gamma_tensor / sigma_tensor\n",
    "gamma_over_sigma_tensor = gamma_over_sigma.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(v0,v1,v2,v3 )\n",
    "weight_tensor = weight_tensor * gamma_over_sigma_tensor\n",
    "bias_tensor = (bias_tensor*gamma_over_sigma)+beta_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2468,  1.6286,  2.2804, -1.0486, 11.8503,  1.6791,  1.5009, -0.4797,\n",
       "         1.7677,  1.2378,  0.9371,  0.0826,  0.1209, -0.3763,  5.4261, 12.1442,\n",
       "        -0.0959,  1.0655,  1.7398,  8.4671,  2.4145,  1.1532,  9.0097,  4.7418,\n",
       "         0.9083,  0.6108,  1.9244, -2.9012,  1.7007,  0.9227,  0.9232,  0.7745],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.2468,  2.2468,  2.2468],\n",
       "          [ 2.2468,  2.2468,  2.2468],\n",
       "          [ 2.2468,  2.2468,  2.2468]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6286,  1.6286,  1.6286],\n",
       "          [ 1.6286,  1.6286,  1.6286],\n",
       "          [ 1.6286,  1.6286,  1.6286]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2804,  2.2804,  2.2804],\n",
       "          [ 2.2804,  2.2804,  2.2804],\n",
       "          [ 2.2804,  2.2804,  2.2804]]],\n",
       "\n",
       "\n",
       "        [[[-1.0486, -1.0486, -1.0486],\n",
       "          [-1.0486, -1.0486, -1.0486],\n",
       "          [-1.0486, -1.0486, -1.0486]]],\n",
       "\n",
       "\n",
       "        [[[11.8503, 11.8503, 11.8503],\n",
       "          [11.8503, 11.8503, 11.8503],\n",
       "          [11.8503, 11.8503, 11.8503]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6791,  1.6791,  1.6791],\n",
       "          [ 1.6791,  1.6791,  1.6791],\n",
       "          [ 1.6791,  1.6791,  1.6791]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5009,  1.5009,  1.5009],\n",
       "          [ 1.5009,  1.5009,  1.5009],\n",
       "          [ 1.5009,  1.5009,  1.5009]]],\n",
       "\n",
       "\n",
       "        [[[-0.4797, -0.4797, -0.4797],\n",
       "          [-0.4797, -0.4797, -0.4797],\n",
       "          [-0.4797, -0.4797, -0.4797]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7677,  1.7677,  1.7677],\n",
       "          [ 1.7677,  1.7677,  1.7677],\n",
       "          [ 1.7677,  1.7677,  1.7677]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2378,  1.2378,  1.2378],\n",
       "          [ 1.2378,  1.2378,  1.2378],\n",
       "          [ 1.2378,  1.2378,  1.2378]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9371,  0.9371,  0.9371],\n",
       "          [ 0.9371,  0.9371,  0.9371],\n",
       "          [ 0.9371,  0.9371,  0.9371]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0826,  0.0826,  0.0826],\n",
       "          [ 0.0826,  0.0826,  0.0826],\n",
       "          [ 0.0826,  0.0826,  0.0826]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1209,  0.1209,  0.1209],\n",
       "          [ 0.1209,  0.1209,  0.1209],\n",
       "          [ 0.1209,  0.1209,  0.1209]]],\n",
       "\n",
       "\n",
       "        [[[-0.3763, -0.3763, -0.3763],\n",
       "          [-0.3763, -0.3763, -0.3763],\n",
       "          [-0.3763, -0.3763, -0.3763]]],\n",
       "\n",
       "\n",
       "        [[[ 5.4261,  5.4261,  5.4261],\n",
       "          [ 5.4261,  5.4261,  5.4261],\n",
       "          [ 5.4261,  5.4261,  5.4261]]],\n",
       "\n",
       "\n",
       "        [[[12.1442, 12.1442, 12.1442],\n",
       "          [12.1442, 12.1442, 12.1442],\n",
       "          [12.1442, 12.1442, 12.1442]]],\n",
       "\n",
       "\n",
       "        [[[-0.0959, -0.0959, -0.0959],\n",
       "          [-0.0959, -0.0959, -0.0959],\n",
       "          [-0.0959, -0.0959, -0.0959]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0655,  1.0655,  1.0655],\n",
       "          [ 1.0655,  1.0655,  1.0655],\n",
       "          [ 1.0655,  1.0655,  1.0655]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7398,  1.7398,  1.7398],\n",
       "          [ 1.7398,  1.7398,  1.7398],\n",
       "          [ 1.7398,  1.7398,  1.7398]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4671,  8.4671,  8.4671],\n",
       "          [ 8.4671,  8.4671,  8.4671],\n",
       "          [ 8.4671,  8.4671,  8.4671]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4145,  2.4145,  2.4145],\n",
       "          [ 2.4145,  2.4145,  2.4145],\n",
       "          [ 2.4145,  2.4145,  2.4145]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1532,  1.1532,  1.1532],\n",
       "          [ 1.1532,  1.1532,  1.1532],\n",
       "          [ 1.1532,  1.1532,  1.1532]]],\n",
       "\n",
       "\n",
       "        [[[ 9.0097,  9.0097,  9.0097],\n",
       "          [ 9.0097,  9.0097,  9.0097],\n",
       "          [ 9.0097,  9.0097,  9.0097]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7418,  4.7418,  4.7418],\n",
       "          [ 4.7418,  4.7418,  4.7418],\n",
       "          [ 4.7418,  4.7418,  4.7418]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9083,  0.9083,  0.9083],\n",
       "          [ 0.9083,  0.9083,  0.9083],\n",
       "          [ 0.9083,  0.9083,  0.9083]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6108,  0.6108,  0.6108],\n",
       "          [ 0.6108,  0.6108,  0.6108],\n",
       "          [ 0.6108,  0.6108,  0.6108]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9244,  1.9244,  1.9244],\n",
       "          [ 1.9244,  1.9244,  1.9244],\n",
       "          [ 1.9244,  1.9244,  1.9244]]],\n",
       "\n",
       "\n",
       "        [[[-2.9012, -2.9012, -2.9012],\n",
       "          [-2.9012, -2.9012, -2.9012],\n",
       "          [-2.9012, -2.9012, -2.9012]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7007,  1.7007,  1.7007],\n",
       "          [ 1.7007,  1.7007,  1.7007],\n",
       "          [ 1.7007,  1.7007,  1.7007]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9227,  0.9227,  0.9227],\n",
       "          [ 0.9227,  0.9227,  0.9227],\n",
       "          [ 0.9227,  0.9227,  0.9227]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9232,  0.9232,  0.9232],\n",
       "          [ 0.9232,  0.9232,  0.9232],\n",
       "          [ 0.9232,  0.9232,  0.9232]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7745,  0.7745,  0.7745],\n",
       "          [ 0.7745,  0.7745,  0.7745],\n",
       "          [ 0.7745,  0.7745,  0.7745]]]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.BatchNorm2d(5,momentum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(5, eps=1e-05, momentum=1, affine=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0.post4'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(5, eps=1e-05, momentum=1, affine=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9503\n",
       " 0.9339\n",
       " 0.7016\n",
       " 0.6154\n",
       " 0.1521\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma=net.weight.data\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=net.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.3295\n",
       " 0.3252\n",
       " 0.2393\n",
       " 0.3410\n",
       " 0.5642\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=net.running_mean\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=net.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3724e03b482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'var' is not defined"
     ]
    }
   ],
   "source": [
    "sigma = var.add(net.eps).sqrt()\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3.5944\n",
       " 2.8608\n",
       " 2.3078\n",
       " 2.3921\n",
       " 0.3541\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma = gamma / sigma\n",
    "gamma_over_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.8459\n",
       "-0.4420\n",
       "-0.3899\n",
       " 0.5558\n",
       " 0.1298\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((x.data[0]-mean)*gamma_over_sigma)+beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0941  0.1706  0.0703  0.5733  0.9306\n",
       " 0.2950  0.1742  0.1702  0.1021  0.0853\n",
       " 0.2228  0.1414  0.0300  0.1352  0.9231\n",
       " 0.7059  0.8143  0.6866  0.5534  0.3178\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.rand(4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.9767 -0.5104 -0.4502  0.6417  0.1498\n",
       "-0.1429 -0.4987 -0.1842 -0.6599 -0.1958\n",
       "-0.4428 -0.6068 -0.5577 -0.5684  0.1467\n",
       " 1.5624  1.6159  1.1921  0.5865 -0.1008\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = net(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen Batch Normalization Layers statistics!\n",
      "Layer:  0\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  1\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  2\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  3\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  4\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  5\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  6\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  7\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  8\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  9\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  10\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  11\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  12\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  13\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  14\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  15\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  16\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  17\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  18\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  19\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  20\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  21\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  22\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  23\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  24\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  25\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n",
      "Layer:  26\n",
      "A bits:  4  | a_min:  0  | a_max:  6.0\n"
     ]
    }
   ],
   "source": [
    "training = False\n",
    "model.eval()\n",
    "quantizer.apply_graph_transforms(0)\n",
    "quantizer.generate_deployment_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single input\n",
    "inputs = torch.randn(1,3,32,32)\n",
    "input_var = Variable(inputs.type(ttype))\n",
    "quantizer.store_and_quantize(training=training)\n",
    "output_1 = model(input_var)\n",
    "quantizer.restore_real_value()            \n",
    "output_2 = quantizer.deployment_model(input_var)\n",
    "values_1, indices_1 = output_1.max(1)\n",
    "values_2, indices_2 = output_2.max(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.4000, 0.0000, 0.0000,  ..., 0.0000, 0.4000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8000, 0.0000],\n",
       "          [0.8000, 0.0000, 0.0000,  ..., 0.8000, 0.8000, 0.0000],\n",
       "          ...,\n",
       "          [1.6000, 0.4000, 1.6000,  ..., 3.2000, 0.0000, 0.0000],\n",
       "          [1.6000, 0.0000, 0.8000,  ..., 3.6000, 0.8000, 0.0000],\n",
       "          [2.4000, 0.0000, 0.4000,  ..., 2.4000, 1.6000, 0.0000]],\n",
       "\n",
       "         [[2.0000, 0.0000, 0.4000,  ..., 0.0000, 0.8000, 0.0000],\n",
       "          [3.6000, 0.0000, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [2.8000, 0.0000, 0.0000,  ..., 0.0000, 0.4000, 0.4000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4000,  ..., 0.0000, 0.4000, 0.0000],\n",
       "          [0.0000, 0.4000, 0.8000,  ..., 0.0000, 0.8000, 0.8000]]]],\n",
       "       device='cuda:0', grad_fn=<LearnedClippedLinearQuantizeSTEBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(model.module.model[0],model.module.model[1])(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
       "          [ 1.,  0.,  0.,  ...,  3.,  2.,  0.],\n",
       "          ...,\n",
       "          [ 5.,  0.,  4.,  ..., 10.,  0.,  0.],\n",
       "          [ 5.,  0.,  2.,  ..., 10.,  2.,  0.],\n",
       "          [ 6.,  0.,  0.,  ...,  6.,  3.,  0.]],\n",
       "\n",
       "         [[ 3.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 6.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 5.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  2.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = nn.Sequential(quantizer.deployment_model.model[0],quantizer.deployment_model.model[1],quantizer.deployment_model.model[2],quantizer.deployment_model.model[3])\n",
    "first(input_var)\n",
    "#quantizer.deployment_model.model[0](input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0269, -0.6061,  0.3412,  0.7959,  0.3926, -0.1135, -0.1445,  0.3798,\n",
       "        -0.5357, -0.5702], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.fc[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "0\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3\n",
      "Different!!!!  tensor([4.7786], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "4\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "5\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "6\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "7\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "8\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "9\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "10\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "11\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "12\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "13\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "14\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "15\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "16\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "17\n",
      "Different!!!!  tensor([9.5760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "18\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "19\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "20\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "21\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "22\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "23\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "24\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "25\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "26\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "27\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "28\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "29\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "30\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "31\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "32\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "33\n",
      "Different!!!!  tensor([3.1857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "34\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "35\n",
      "Different!!!!  tensor([8.3696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "36\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "37\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "38\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "39\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "40\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "41\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "42\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "43\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "44\n",
      "Different!!!!  tensor([9.7551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "45\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "46\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "47\n",
      "Different!!!!  tensor([7.8512], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "48\n",
      "Different!!!!  tensor([3.6852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "49\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "50\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "51\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "52\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "53\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "54\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "55\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "56\n",
      "Different!!!!  tensor([7.8606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "57\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "58\n",
      "Different!!!!  tensor([7.7946], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "59\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "60\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "62\n",
      "Different!!!!  tensor([8.3790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "63\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "64\n",
      "Different!!!!  tensor([8.3130], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "65\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "66\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "67\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "68\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "69\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "70\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "71\n",
      "Different!!!!  tensor([9.4912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "72\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "73\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "74\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "75\n",
      "Different!!!!  tensor([9.7739], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "76\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "77\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "78\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "79\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "80\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "81\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "82\n",
      "Different!!!!  tensor([7.5590], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "83\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "84\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "85\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "86\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "87\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "88\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "89\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "90\n",
      "Different!!!!  tensor([4.4393], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "91\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "92\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "93\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "94\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "95\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "96\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "97\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "98\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "99\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "100\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "101\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "102\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "103\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "104\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "105\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "106\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "107\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "108\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "109\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "110\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "111\n",
      "Different!!!!  tensor([8.1434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "112\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "113\n",
      "Different!!!!  tensor([3.7889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "114\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "115\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "116\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "117\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "118\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "119\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "120\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "121\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "123\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "124\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "125\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "126\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "127\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "128\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "129\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "130\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "131\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "132\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "133\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "134\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "135\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "136\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "137\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "138\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "139\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "140\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "141\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "142\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "143\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "144\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "145\n",
      "Different!!!!  tensor([7.6250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "146\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "147\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "148\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "149\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "150\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "151\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "152\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "153\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "154\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "155\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "156\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "157\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "158\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "159\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "160\n",
      "Different!!!!  tensor([7.2197], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "161\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "162\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "163\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "164\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "165\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "166\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "167\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "168\n",
      "Different!!!!  tensor([7.5873], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "169\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "170\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "171\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "172\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "173\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "174\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "175\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "176\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "177\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "178\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "179\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "180\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "181\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "182\n",
      "Different!!!!  tensor([8.1151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "183\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "184\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "186\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "187\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "188\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "189\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "190\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "191\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "192\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "193\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "194\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "195\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "196\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "197\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "198\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "199\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "200\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "201\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "202\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "203\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "204\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "205\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "206\n",
      "Different!!!!  tensor([4.1377], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "207\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "208\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "209\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "210\n",
      "Different!!!!  tensor([8.7277], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "211\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "212\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "213\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "214\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "215\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "216\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "217\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "218\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "219\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "220\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "221\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "222\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "223\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "224\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "225\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "226\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "227\n",
      "Different!!!!  tensor([7.1537], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "228\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "229\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "230\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "231\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "232\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "233\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "234\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "235\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "236\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "237\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "238\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "239\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "240\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "241\n",
      "Different!!!!  tensor([7.7852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "242\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "243\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "244\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "245\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "246\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "247\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "248\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "250\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "251\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "252\n",
      "Different!!!!  tensor([8.3224], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "253\n",
      "Different!!!!  tensor([9.2367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "254\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "255\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "256\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "257\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "258\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "259\n",
      "Different!!!!  tensor([7.8889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "260\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "261\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "262\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "263\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "264\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "265\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "266\n",
      "Different!!!!  tensor([8.0208], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "267\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "268\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "269\n",
      "Different!!!!  tensor([3.4402], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "270\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "271\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "272\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "273\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "274\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "275\n",
      "Different!!!!  tensor([3.7229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "276\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "277\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "278\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "279\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "280\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "281\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "282\n",
      "Different!!!!  tensor([7.7946], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "283\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "284\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "285\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "286\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "287\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "288\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "289\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "290\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "291\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "292\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "293\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "294\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "295\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "296\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "297\n",
      "Different!!!!  tensor([8.7843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "298\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "299\n",
      "Different!!!!  tensor([4.1565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "300\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "301\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "302\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "303\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "304\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "305\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "306\n",
      "Different!!!!  tensor([7.4176], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "307\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "308\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "309\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "310\n",
      "Different!!!!  tensor([8.3790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "311\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "313\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "314\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "315\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "316\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "317\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "318\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "319\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "320\n",
      "Different!!!!  tensor([7.1726], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "321\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "322\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "323\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "324\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "325\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "326\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "327\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "328\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "329\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "330\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "331\n",
      "Different!!!!  tensor([8.2753], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "332\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "333\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "334\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "335\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "336\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "337\n",
      "Different!!!!  tensor([7.5213], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "338\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "339\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "340\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "341\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "342\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "343\n",
      "Different!!!!  tensor([3.7606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "344\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "345\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "346\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "347\n",
      "Different!!!!  tensor([9.1896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "348\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "349\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "350\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "351\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "352\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "353\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "354\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "355\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "356\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "357\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "358\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "359\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "360\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "361\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "362\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "363\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "364\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "365\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "366\n",
      "Different!!!!  tensor([8.5109], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "367\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "368\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "369\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "370\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "371\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "372\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "373\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "375\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "376\n",
      "Different!!!!  tensor([7.1537], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "377\n",
      "Different!!!!  tensor([3.6475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "378\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "379\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "380\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "381\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "382\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "383\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "384\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "385\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "386\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "387\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "388\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "389\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "390\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "391\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "392\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "393\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "394\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "395\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "396\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "397\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "398\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "399\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "400\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "401\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "402\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "403\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "404\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "405\n",
      "Different!!!!  tensor([8.3507], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "406\n",
      "Different!!!!  tensor([7.7098], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "407\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "408\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "409\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "410\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "411\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "412\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "413\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "414\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "415\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "416\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "417\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "418\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "419\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "420\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "421\n",
      "Different!!!!  tensor([8.9728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "422\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "423\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "424\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "425\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "426\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "427\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "428\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "429\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "430\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "431\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "432\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "433\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "434\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "436\n",
      "Different!!!!  tensor([7.4742], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "437\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "438\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "439\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "440\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "441\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "442\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "443\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "444\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "445\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "446\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "447\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "448\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "449\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "450\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "451\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "452\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "453\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "454\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "455\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "456\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "457\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "458\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "459\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "460\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "461\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "462\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "463\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "464\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "465\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "466\n",
      "Different!!!!  tensor([7.5590], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "467\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "468\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "469\n",
      "Different!!!!  tensor([8.3790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "470\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "471\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "472\n",
      "Different!!!!  tensor([3.4213], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "473\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "474\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "475\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "476\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "477\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "478\n",
      "Different!!!!  tensor([4.2602], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "479\n",
      "Different!!!!  tensor([9.1424], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "480\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "481\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "482\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "483\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "484\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "485\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "486\n",
      "Different!!!!  tensor([8.3978], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "487\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "488\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "489\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "490\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "491\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "492\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "493\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "494\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "495\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "496\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "497\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "498\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "499\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "500\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "501\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "502\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "503\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "505\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "506\n",
      "Different!!!!  tensor([7.4176], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "507\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "508\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "509\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "510\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "511\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "512\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "513\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "514\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "515\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "516\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "517\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "518\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "519\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "520\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "521\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "522\n",
      "Different!!!!  tensor([7.0595], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "523\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "524\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "525\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "526\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "527\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "528\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "529\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "530\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "531\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "532\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "533\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "534\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "535\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "536\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "537\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "538\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "539\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "540\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "541\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "542\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "543\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "544\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "545\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "546\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "547\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "548\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "549\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "550\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "551\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "552\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "553\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "554\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "555\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "556\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "557\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "558\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "559\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "560\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "561\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "562\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "563\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "564\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "565\n",
      "Different!!!!  tensor([7.9643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "566\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "568\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "569\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "570\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "571\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "572\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "573\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "574\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "575\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "576\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "577\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "578\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "579\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "580\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "581\n",
      "Different!!!!  tensor([4.0340], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "582\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "583\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "584\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "585\n",
      "Different!!!!  tensor([7.5873], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "586\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "587\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "588\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "589\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "590\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "591\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "592\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "593\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "594\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "595\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "596\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "597\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "598\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "599\n",
      "Different!!!!  tensor([3.6193], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "600\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "601\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "602\n",
      "Different!!!!  tensor([3.3836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "603\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "604\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "605\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "606\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "607\n",
      "Different!!!!  tensor([8.6335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "608\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "609\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "610\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "611\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "612\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "613\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "614\n",
      "Different!!!!  tensor([3.4779], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "615\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "616\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "617\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "618\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "619\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "620\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "621\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "622\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "623\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "624\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "625\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "626\n",
      "Different!!!!  tensor([8.6994], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "627\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "628\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "629\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "631\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "632\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "633\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "634\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "635\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "636\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "637\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "638\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "639\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "640\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "641\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "642\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "643\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "644\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "645\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "646\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "647\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "648\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "649\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "650\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "651\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "652\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "653\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "654\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "655\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "656\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "657\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "658\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "659\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "660\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "661\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "662\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "663\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "664\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "665\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "666\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "667\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "668\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "669\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "670\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "671\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "672\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "673\n",
      "Different!!!!  tensor([9.0576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "674\n",
      "Different!!!!  tensor([9.1801], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "675\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "676\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "677\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "678\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "679\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "680\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "681\n",
      "Different!!!!  tensor([4.3167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "682\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "683\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "684\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "685\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "686\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "687\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "688\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "689\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "690\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "691\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "692\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "694\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "695\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "696\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "697\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "698\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "699\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "700\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "701\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "702\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "703\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "704\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "705\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "706\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "707\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "708\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "709\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "710\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "711\n",
      "Different!!!!  tensor([8.6523], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "712\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "713\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "714\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "715\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "716\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "717\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "718\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "719\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "720\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "721\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "722\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "723\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "724\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "725\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "726\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "727\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "728\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "729\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "730\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "731\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "732\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "733\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "734\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "735\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "736\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "737\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "738\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "739\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "740\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "741\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "742\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "743\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "744\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "745\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "746\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "747\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "748\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "749\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "750\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "751\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "752\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "753\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "754\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "755\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "757\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "758\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "759\n",
      "Different!!!!  tensor([5.2969], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "760\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "761\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "762\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "763\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "764\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "765\n",
      "Different!!!!  tensor([4.2319], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "766\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "767\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "768\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "769\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "770\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "771\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "772\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "773\n",
      "Different!!!!  tensor([9.2932], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "774\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "775\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "776\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "777\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "778\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "779\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "780\n",
      "Different!!!!  tensor([7.1537], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "781\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "782\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "783\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "784\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "785\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "786\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "787\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "788\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "789\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "790\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "791\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "792\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "793\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "794\n",
      "Different!!!!  tensor([4.5241], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "795\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "796\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "797\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "798\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "799\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "800\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "801\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "802\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "803\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "804\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "805\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "806\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "807\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "808\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "809\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "810\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "811\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "812\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "813\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "814\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "815\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "816\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "817\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "819\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "820\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "821\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "822\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "823\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "824\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "825\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "826\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "827\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "828\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "829\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "830\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "831\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "832\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "833\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "834\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "835\n",
      "Different!!!!  tensor([8.9916], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "836\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "837\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "838\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "839\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "840\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "841\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "842\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "843\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "844\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "845\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "846\n",
      "Different!!!!  tensor([7.8889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "847\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "848\n",
      "Different!!!!  tensor([3.4025], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "849\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "850\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "851\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "852\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "853\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "854\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "855\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "856\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "857\n",
      "Different!!!!  tensor([8.8125], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "858\n",
      "Different!!!!  tensor([9.5854], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "859\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "860\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "861\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "862\n",
      "Different!!!!  tensor([8.7183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "863\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "864\n",
      "Different!!!!  tensor([8.6335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "865\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "866\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "867\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "868\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "869\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "870\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "871\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "872\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "873\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "874\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "875\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "876\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "877\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "878\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "880\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "881\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "882\n",
      "Different!!!!  tensor([8.5863], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "883\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "884\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "885\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "886\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "887\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "888\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "889\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "890\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "891\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "892\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "893\n",
      "Different!!!!  tensor([3.4779], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "894\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "895\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "896\n",
      "Different!!!!  tensor([3.2328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "897\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "898\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "899\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "900\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "901\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "902\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "903\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "904\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "905\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "906\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "907\n",
      "Different!!!!  tensor([7.9737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "908\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "909\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "910\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "911\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "912\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "913\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "914\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "915\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "916\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "917\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "918\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "919\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "920\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "921\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "922\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "923\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "924\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "925\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "926\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "927\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "928\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "929\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "930\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "931\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "932\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "933\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "934\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "935\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "936\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "937\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "938\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "939\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "940\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "941\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "943\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "944\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "945\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "946\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "947\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "948\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "949\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "950\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "951\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "952\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "953\n",
      "Different!!!!  tensor([3.4685], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "954\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "955\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "956\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "957\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "958\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "959\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "960\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "961\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "962\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "963\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "964\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "965\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "966\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "967\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "968\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "969\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "970\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "971\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "972\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "973\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "974\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "975\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "976\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "977\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "978\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "979\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "980\n",
      "Different!!!!  tensor([3.7795], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "981\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "982\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "983\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "984\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "985\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "986\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "987\n",
      "Different!!!!  tensor([3.8455], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "988\n",
      "Different!!!!  tensor([4.6749], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "989\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "990\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "991\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "992\n",
      "Different!!!!  tensor([8.5581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "993\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "994\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "995\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "996\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "997\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "998\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "999\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1000\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1001\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1002\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1003\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1004\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1006\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1007\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1008\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1009\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1010\n",
      "Different!!!!  tensor([3.5533], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1011\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1012\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1013\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1014\n",
      "Different!!!!  tensor([3.3836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1015\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1016\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1017\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1018\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1019\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1020\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1021\n",
      "Different!!!!  tensor([4.0623], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1022\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1023\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1024\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1025\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1026\n",
      "Different!!!!  tensor([8.5675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1027\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1028\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1029\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1030\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1031\n",
      "Different!!!!  tensor([3.5816], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1032\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1033\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1034\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1035\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1036\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1037\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1038\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1039\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1040\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1041\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1042\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1043\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1044\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1045\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1046\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1047\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1048\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1049\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1050\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1051\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1052\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1053\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1054\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1055\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1056\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1057\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1058\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1059\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1060\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1061\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1062\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1063\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1064\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1065\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1066\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1067\n",
      "Different!!!!  tensor([3.5250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1069\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1070\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1071\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1072\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1073\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1074\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1075\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1076\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1077\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1078\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1079\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1080\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1081\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1082\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1083\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1084\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1085\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1086\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1087\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1088\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1089\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1090\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1091\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1092\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1093\n",
      "Different!!!!  tensor([4.2602], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1094\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1095\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1096\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1097\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1098\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1099\n",
      "Different!!!!  tensor([3.8172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1100\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1101\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1102\n",
      "Different!!!!  tensor([8.2942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1103\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1104\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1105\n",
      "Different!!!!  tensor([8.5109], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1106\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1107\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1108\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1109\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1110\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1111\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1112\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1113\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1114\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1115\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1116\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1117\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1118\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1119\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1120\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1121\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1122\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1123\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1124\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1125\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1126\n",
      "Different!!!!  tensor([3.6664], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1127\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1128\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1129\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1130\n",
      "Different!!!!  tensor([3.8549], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1132\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1133\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1134\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1135\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1136\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1137\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1138\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1139\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1140\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1141\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1142\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1143\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1144\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1145\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1146\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1147\n",
      "Different!!!!  tensor([7.4176], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1148\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1149\n",
      "Different!!!!  tensor([7.6815], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1150\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1151\n",
      "Different!!!!  tensor([3.7041], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1152\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1153\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1154\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1155\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1156\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1157\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1158\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1159\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1160\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1161\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1162\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1163\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1164\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1165\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1166\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1167\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1168\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1169\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1170\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1171\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1172\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1173\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1174\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1175\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1176\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1177\n",
      "Different!!!!  tensor([8.4921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1178\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1179\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1180\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1181\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1182\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1183\n",
      "Different!!!!  tensor([7.7852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1184\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1185\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1186\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1187\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1188\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1189\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1190\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1191\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1192\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1193\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.8992], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1195\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1196\n",
      "Different!!!!  tensor([3.8360], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1197\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1198\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1199\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1200\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1201\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1202\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1203\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1204\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1205\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1206\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1207\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1208\n",
      "Different!!!!  tensor([5.2593], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1209\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1210\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1211\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1212\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1213\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1214\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1215\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1216\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1217\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1218\n",
      "Different!!!!  tensor([8.2282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1219\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1220\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1221\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1222\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1223\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1224\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1225\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1226\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1227\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1228\n",
      "Different!!!!  tensor([3.8360], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1229\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1230\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1231\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1232\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1233\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1234\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1235\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1236\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1237\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1238\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1239\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1240\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1241\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1242\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1243\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1244\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1245\n",
      "Different!!!!  tensor([8.1434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1246\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1247\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1248\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1249\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1250\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1251\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1252\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1253\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1254\n",
      "Different!!!!  tensor([4.2602], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1255\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1256\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1258\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1259\n",
      "Different!!!!  tensor([3.9397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1260\n",
      "Different!!!!  tensor([4.1754], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1261\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1262\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1263\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1264\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1265\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1266\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1267\n",
      "Different!!!!  tensor([9.1518], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1268\n",
      "Different!!!!  tensor([8.8502], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1269\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1270\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1271\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1272\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1273\n",
      "Different!!!!  tensor([8.0868], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1274\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1275\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1276\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1277\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1278\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1279\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1280\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1281\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1282\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1283\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1284\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1285\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1286\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1287\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1288\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1289\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1290\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1291\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1292\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1293\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1294\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1295\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1296\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1297\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1298\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1299\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1300\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1301\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1302\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1303\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1304\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1305\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1306\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1307\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1308\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1309\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1310\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1311\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1312\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1313\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1314\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1315\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1316\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1317\n",
      "Different!!!!  tensor([8.7277], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1318\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1319\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1321\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1322\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1323\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1324\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1325\n",
      "Different!!!!  tensor([8.0774], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1326\n",
      "Different!!!!  tensor([7.8135], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1327\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1328\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1329\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1330\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1331\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1332\n",
      "Different!!!!  tensor([4.5241], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1333\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1334\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1335\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1336\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1337\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1338\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1339\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1340\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1341\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1342\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1343\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1344\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1345\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1346\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1347\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1348\n",
      "Different!!!!  tensor([3.6852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1349\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1350\n",
      "Different!!!!  tensor([8.6900], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1351\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1352\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1353\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1354\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1355\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1356\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1357\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1358\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1359\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1360\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1361\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1362\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1363\n",
      "Different!!!!  tensor([8.3036], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1364\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1365\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1366\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1367\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1368\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1369\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1370\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1371\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1372\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1373\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1374\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1375\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1376\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1377\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1378\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1379\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1380\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1381\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1382\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([3.3554], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1384\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1385\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1386\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1387\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1388\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1389\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1390\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1391\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1392\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1393\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1394\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1395\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1396\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1397\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1398\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1399\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1400\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1401\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1402\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1403\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1404\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1405\n",
      "Different!!!!  tensor([8.6712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1406\n",
      "Different!!!!  tensor([9.8964], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1407\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1408\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1409\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1410\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1411\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1412\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1413\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1414\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1415\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1416\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1417\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1418\n",
      "Different!!!!  tensor([3.8832], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1419\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1420\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1421\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1422\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1423\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1424\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1425\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1426\n",
      "Different!!!!  tensor([8.0962], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1427\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1428\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1429\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1430\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1431\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1432\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1433\n",
      "Different!!!!  tensor([7.7192], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1434\n",
      "Different!!!!  tensor([7.9737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1435\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1436\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1437\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1438\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1439\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1440\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1441\n",
      "Different!!!!  tensor([8.3978], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1442\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1443\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1444\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1445\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1447\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1448\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1449\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1450\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1451\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1452\n",
      "Different!!!!  tensor([3.9209], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1453\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1454\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1455\n",
      "Different!!!!  tensor([3.5250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1456\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1457\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1458\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1459\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1460\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1461\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1462\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1463\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1464\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1465\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1466\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1467\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1468\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1469\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1470\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1471\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1472\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1473\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1474\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1475\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1476\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1477\n",
      "Different!!!!  tensor([3.6852], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1478\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1479\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1480\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1481\n",
      "Different!!!!  tensor([3.9209], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1482\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1483\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1484\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1485\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1486\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1487\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1488\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1489\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1490\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1491\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1492\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1493\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1494\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1495\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1496\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1497\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1498\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1499\n",
      "Different!!!!  tensor([8.4167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1500\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1501\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1502\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1503\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1504\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1505\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1506\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1507\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1508\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1510\n",
      "Different!!!!  tensor([8.7748], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1511\n",
      "Different!!!!  tensor([7.3705], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1512\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1513\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1514\n",
      "Different!!!!  tensor([3.7795], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1515\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1516\n",
      "Different!!!!  tensor([4.2036], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1517\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1518\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1519\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1520\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1521\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1522\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1523\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1524\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1525\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1526\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1527\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1528\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1529\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1530\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1531\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1532\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1533\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1534\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1535\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1536\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1537\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1538\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1539\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1540\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1541\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1542\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1543\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1544\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1545\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1546\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1547\n",
      "Different!!!!  tensor([8.9445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1548\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1549\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1550\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1551\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1552\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1553\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1554\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1555\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1556\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1557\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1558\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1559\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1560\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1561\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1562\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1563\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1564\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1565\n",
      "Different!!!!  tensor([8.1811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1566\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1567\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1568\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1569\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1570\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1571\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1572\n",
      "Different!!!!  tensor([3.1857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1573\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1574\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1575\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1576\n",
      "Different!!!!  tensor([8.1245], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1578\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1579\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1580\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1581\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1582\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1583\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1584\n",
      "Different!!!!  tensor([7.4365], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1585\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1586\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1587\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1588\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1589\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1590\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1591\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1592\n",
      "Different!!!!  tensor([8.1057], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1593\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1594\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1595\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1596\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1597\n",
      "Different!!!!  tensor([4.3167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1598\n",
      "Different!!!!  tensor([9.4158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1599\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1600\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1601\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1602\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1603\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1604\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1605\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1606\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1607\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1608\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1609\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1610\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1611\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1612\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1613\n",
      "Different!!!!  tensor([9.6985], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1614\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1615\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1616\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1617\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1618\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1619\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1620\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1621\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1622\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1623\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1624\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1625\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1626\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1627\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1628\n",
      "Different!!!!  tensor([7.3799], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1629\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1630\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1631\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1632\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1633\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1634\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1635\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1636\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1637\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1638\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1639\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1641\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1642\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1643\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1644\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1645\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1646\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1647\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1648\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1649\n",
      "Different!!!!  tensor([7.5873], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1650\n",
      "Different!!!!  tensor([4.2319], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1651\n",
      "Different!!!!  tensor([7.8606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1652\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1653\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1654\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1655\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1656\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1657\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1658\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1659\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1660\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1661\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1662\n",
      "Different!!!!  tensor([4.6749], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1663\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1664\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1665\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1666\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1667\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1668\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1669\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1670\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1671\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1672\n",
      "Different!!!!  tensor([7.9077], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1673\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1674\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1675\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1676\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1677\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1678\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1679\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1680\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1681\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1682\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1683\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1684\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1685\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1686\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1687\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1688\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1689\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1690\n",
      "Different!!!!  tensor([8.0397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1691\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1692\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1693\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1694\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1695\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1696\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1697\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1698\n",
      "Different!!!!  tensor([7.0595], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1699\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1700\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1701\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1702\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1704\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1705\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1706\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1707\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1708\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1709\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1710\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1711\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1712\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1713\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1714\n",
      "Different!!!!  tensor([8.1811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1715\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1716\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1717\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1718\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1719\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1720\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1721\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1722\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1723\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1724\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1725\n",
      "Different!!!!  tensor([8.0114], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1726\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1727\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1728\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1729\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1730\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1731\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1732\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1733\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1734\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1735\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1736\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1737\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1738\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1739\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1740\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1741\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1742\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1743\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1744\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1745\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1746\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1747\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1748\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1749\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1750\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1751\n",
      "Different!!!!  tensor([8.5958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1752\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1753\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1754\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1755\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1756\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1757\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1758\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1759\n",
      "Different!!!!  tensor([4.4393], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1760\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1761\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1762\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1763\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1764\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1765\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1767\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1768\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1769\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1770\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1771\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1772\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1773\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1774\n",
      "Different!!!!  tensor([4.1471], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1775\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1776\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1777\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1778\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1779\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1780\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1781\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1782\n",
      "Different!!!!  tensor([4.1565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1783\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1784\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1785\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1786\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1787\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1788\n",
      "Different!!!!  tensor([8.6052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1789\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1790\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1791\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1792\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1793\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1794\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1795\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1796\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1797\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1798\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1799\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1800\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1801\n",
      "Different!!!!  tensor([9.1330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1802\n",
      "Different!!!!  tensor([3.7606], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1803\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1804\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1805\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1806\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1807\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1808\n",
      "Different!!!!  tensor([6.4562], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1809\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1810\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1811\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1812\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1813\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1814\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1815\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1816\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1817\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1818\n",
      "Different!!!!  tensor([4.3450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1819\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1820\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1821\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1822\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1823\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1824\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1825\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1826\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1827\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1828\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1830\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1831\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1832\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1833\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1834\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1835\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1836\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1837\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1838\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1839\n",
      "Different!!!!  tensor([8.3130], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1840\n",
      "Different!!!!  tensor([7.8512], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1841\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1842\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1843\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1844\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1845\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1846\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1847\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1848\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1849\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1850\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1851\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1852\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1853\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1854\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1855\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1856\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1857\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1858\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1859\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1860\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1861\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1862\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1863\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1864\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1865\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1866\n",
      "Different!!!!  tensor([8.2282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1867\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1868\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1869\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1870\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1871\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1872\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1873\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1874\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1875\n",
      "Different!!!!  tensor([4.0623], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1876\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1877\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1878\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1879\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1880\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1881\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1882\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1883\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1884\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1885\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1886\n",
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1887\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1888\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1889\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1890\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1891\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([7.2951], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1893\n",
      "Different!!!!  tensor([7.7381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1894\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1895\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1896\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1897\n",
      "Different!!!!  tensor([8.4073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1898\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1899\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1900\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1901\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1902\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1903\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1904\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1905\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1906\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1907\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1908\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1909\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1910\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1911\n",
      "Different!!!!  tensor([4.0717], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1912\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1913\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1914\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1915\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1916\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1917\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1918\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1919\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1920\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1921\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1922\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1923\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1924\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1925\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1926\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1927\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1928\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1929\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1930\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1931\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1932\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1933\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1934\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1935\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1936\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1937\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1938\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1939\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1940\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1941\n",
      "Different!!!!  tensor([7.6627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1942\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1943\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1944\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1945\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1946\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1947\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1948\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1949\n",
      "Different!!!!  tensor([8.6712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1950\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1951\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1952\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1953\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1954\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.1188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1956\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1957\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1958\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1959\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1960\n",
      "Different!!!!  tensor([3.7229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1961\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1962\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1963\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1964\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1965\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1966\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1967\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1968\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1969\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1970\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1971\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1972\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1973\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1974\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1975\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1976\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1977\n",
      "Different!!!!  tensor([8.5769], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1978\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1979\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1980\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1981\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1982\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1983\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1984\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1985\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1986\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1987\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1988\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1989\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1990\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1991\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1992\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1993\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1994\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1995\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1996\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1997\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1998\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "1999\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2000\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2001\n",
      "Different!!!!  tensor([4.0340], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2002\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2003\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2004\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2005\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2006\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2007\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2008\n",
      "Different!!!!  tensor([3.7229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2009\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2010\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2011\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2012\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2013\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2014\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2015\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2016\n",
      "Different!!!!  tensor([4.0340], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2017\n",
      "Different!!!!  tensor([3.7324], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([3.7795], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2019\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2020\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2021\n",
      "Different!!!!  tensor([7.6250], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2022\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2023\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2024\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2025\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2026\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2027\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2028\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2029\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2030\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2031\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2032\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2033\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2034\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2035\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2036\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2037\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2038\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2039\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2040\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2041\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2042\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2043\n",
      "Different!!!!  tensor([7.2008], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2044\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2045\n",
      "Different!!!!  tensor([7.5119], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2046\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2047\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2048\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2049\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2050\n",
      "Different!!!!  tensor([4.2508], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2051\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2052\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2053\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2054\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2055\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2056\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2057\n",
      "Different!!!!  tensor([9.5100], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2058\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2059\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2060\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2061\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2062\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2063\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2064\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2065\n",
      "Different!!!!  tensor([3.5062], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2066\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2067\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2068\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2069\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2070\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2071\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2072\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2073\n",
      "Different!!!!  tensor([3.6664], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2074\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2075\n",
      "Different!!!!  tensor([8.4638], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2076\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2077\n",
      "Different!!!!  tensor([3.3931], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2078\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2079\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2080\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2082\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2083\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2084\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2085\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2086\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2087\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2088\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2089\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2090\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2091\n",
      "Different!!!!  tensor([7.2197], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2092\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2093\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2094\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2095\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2096\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2097\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2098\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2099\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2100\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2101\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2102\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2103\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2104\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2105\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2106\n",
      "Different!!!!  tensor([9.0670], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2107\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2108\n",
      "Different!!!!  tensor([7.4647], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2109\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2110\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2111\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2112\n",
      "Different!!!!  tensor([8.2188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2113\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2114\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2115\n",
      "Different!!!!  tensor([8.0962], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2116\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2117\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2118\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2119\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2120\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2121\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2122\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2123\n",
      "Different!!!!  tensor([4.0717], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2124\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2125\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2126\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2127\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2128\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2129\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2130\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2131\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2132\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2133\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2134\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2135\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2136\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2137\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2138\n",
      "Different!!!!  tensor([3.5627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2139\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2140\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2141\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2142\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2143\n",
      "Different!!!!  tensor([7.5496], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2145\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2146\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2147\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2148\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2149\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2150\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2151\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2152\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2153\n",
      "Different!!!!  tensor([3.8455], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2154\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2155\n",
      "Different!!!!  tensor([7.8040], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2156\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2157\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2158\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2159\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2160\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2161\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2162\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2163\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2164\n",
      "Different!!!!  tensor([6.5599], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2165\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2166\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2167\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2168\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2169\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2170\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2171\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2172\n",
      "Different!!!!  tensor([8.0303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2173\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2174\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2175\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2176\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2177\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2178\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2179\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2180\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2181\n",
      "Different!!!!  tensor([3.7135], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2182\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2183\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2184\n",
      "Different!!!!  tensor([8.8220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2185\n",
      "Different!!!!  tensor([7.1820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2186\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2187\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2188\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2189\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2190\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2191\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2192\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2193\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2194\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2195\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2196\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2197\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2198\n",
      "Different!!!!  tensor([4.1377], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2199\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2200\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2201\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2202\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2203\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2204\n",
      "Different!!!!  tensor([3.7983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2205\n",
      "Different!!!!  tensor([3.8266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2206\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2208\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2209\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2210\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2211\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2212\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2213\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2214\n",
      "Different!!!!  tensor([3.1574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2215\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2216\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2217\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2218\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2219\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2220\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2221\n",
      "Different!!!!  tensor([8.5863], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2222\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2223\n",
      "Different!!!!  tensor([3.5627], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2224\n",
      "Different!!!!  tensor([6.6636], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2225\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2226\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2227\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2228\n",
      "Different!!!!  tensor([8.5486], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2229\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2230\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2231\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2232\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2233\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2234\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2235\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2236\n",
      "Different!!!!  tensor([9.0199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2237\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2238\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2239\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2240\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2241\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2242\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2243\n",
      "Different!!!!  tensor([4.8917], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2244\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2245\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2246\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2247\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2248\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2249\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2250\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2251\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2252\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2253\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2254\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2255\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2256\n",
      "Different!!!!  tensor([3.8455], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2257\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2258\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2259\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2260\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2261\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2262\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2263\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2264\n",
      "Different!!!!  tensor([8.0585], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2265\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2266\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2267\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2268\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2269\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2270\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2271\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2272\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2273\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2274\n",
      "Different!!!!  tensor([9.1895], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2276\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2277\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2278\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2279\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2280\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2281\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2282\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2283\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2284\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2285\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2286\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2287\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2288\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2289\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2290\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2291\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2292\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2293\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2294\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2295\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2296\n",
      "Different!!!!  tensor([7.9643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2297\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2298\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2299\n",
      "Different!!!!  tensor([6.8992], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2300\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2301\n",
      "Different!!!!  tensor([4.1848], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2302\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2303\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2304\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2305\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2306\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2307\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2308\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2309\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2310\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2311\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2312\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2313\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2314\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2315\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2316\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2317\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2318\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2319\n",
      "Different!!!!  tensor([5.0330], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2320\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2321\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2322\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2323\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2324\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2325\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2326\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2327\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2328\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2329\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2330\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2331\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2332\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2333\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2334\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2335\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2336\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2337\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2339\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2340\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2341\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2342\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2343\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2344\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2345\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2346\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2347\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2348\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2349\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2350\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2351\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2352\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2353\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2354\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2355\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2356\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2357\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2358\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2359\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2360\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2361\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2362\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2363\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2364\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2365\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2366\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2367\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2368\n",
      "Different!!!!  tensor([4.1942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2369\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2370\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2371\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2372\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2373\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2374\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2375\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2376\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2377\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2378\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2379\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2380\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2381\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2382\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2383\n",
      "Different!!!!  tensor([6.5694], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2384\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2385\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2386\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2387\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2388\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2389\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2390\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2391\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2392\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2393\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2394\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2395\n",
      "Different!!!!  tensor([9.2178], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2396\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2397\n",
      "Different!!!!  tensor([8.7371], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2398\n",
      "Different!!!!  tensor([4.2225], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2399\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2400\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.1754], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2402\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2403\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2404\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2405\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2406\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2407\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2408\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2409\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2410\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2411\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2412\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2413\n",
      "Different!!!!  tensor([7.7569], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2414\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2415\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2416\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2417\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2418\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2419\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2420\n",
      "Different!!!!  tensor([6.8898], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2421\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2422\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2423\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2424\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2425\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2426\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2427\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2428\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2429\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2430\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2431\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2432\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2433\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2434\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2435\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2436\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2437\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2438\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2439\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2440\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2441\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2442\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2443\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2444\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2445\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2446\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2447\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2448\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2449\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2450\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2451\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2452\n",
      "Different!!!!  tensor([8.4355], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2453\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2454\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2455\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2456\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2457\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2458\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2459\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2460\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2461\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2462\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2463\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2465\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2466\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2467\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2468\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2469\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2470\n",
      "Different!!!!  tensor([4.1942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2471\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2472\n",
      "Different!!!!  tensor([7.7946], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2473\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2474\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2475\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2476\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2477\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2478\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2479\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2480\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2481\n",
      "Different!!!!  tensor([8.3507], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2482\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2483\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2484\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2485\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2486\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2487\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2488\n",
      "Different!!!!  tensor([6.9275], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2489\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2490\n",
      "Different!!!!  tensor([8.7089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2491\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2492\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2493\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2494\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2495\n",
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2496\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2497\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2498\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2499\n",
      "Different!!!!  tensor([7.5590], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2500\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2501\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2502\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2503\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2504\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2505\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2506\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2507\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2508\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2509\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2510\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2511\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2512\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2513\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2514\n",
      "Different!!!!  tensor([7.5119], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2515\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2516\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2517\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2518\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2519\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2520\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2521\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2522\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2523\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2524\n",
      "Different!!!!  tensor([7.0689], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2525\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2526\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2528\n",
      "Different!!!!  tensor([8.9916], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2529\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2530\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2531\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2532\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2533\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2534\n",
      "Different!!!!  tensor([7.9454], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2535\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2536\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2537\n",
      "Different!!!!  tensor([4.5335], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2538\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2539\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2540\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2541\n",
      "Different!!!!  tensor([7.6909], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2542\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2543\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2544\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2545\n",
      "Different!!!!  tensor([8.7089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2546\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2547\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2548\n",
      "Different!!!!  tensor([9.2084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2549\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2550\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2551\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2552\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2553\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2554\n",
      "Different!!!!  tensor([2.9218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2555\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2556\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2557\n",
      "Different!!!!  tensor([6.3526], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2558\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2559\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2560\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2561\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2562\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2563\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2564\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2565\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2566\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2567\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2568\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2569\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2570\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2571\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2572\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2573\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2574\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2575\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2576\n",
      "Different!!!!  tensor([6.9841], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2577\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2578\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2579\n",
      "Different!!!!  tensor([3.5533], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2580\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2581\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2582\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2583\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2584\n",
      "Different!!!!  tensor([7.2008], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2585\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2586\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2587\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2588\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2589\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2591\n",
      "Different!!!!  tensor([3.9586], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2592\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2593\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2594\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2595\n",
      "Different!!!!  tensor([4.4864], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2596\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2597\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2598\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2599\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2600\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2601\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2602\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2603\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2604\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2605\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2606\n",
      "Different!!!!  tensor([7.4647], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2607\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2608\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2609\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2610\n",
      "Different!!!!  tensor([7.0595], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2611\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2612\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2613\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2614\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2615\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2616\n",
      "Different!!!!  tensor([10.0284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2617\n",
      "Different!!!!  tensor([6.3620], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2618\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2619\n",
      "Different!!!!  tensor([3.9963], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2620\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2621\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2622\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2623\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2624\n",
      "Different!!!!  tensor([7.0312], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2625\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2626\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2627\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2628\n",
      "Different!!!!  tensor([8.1057], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2629\n",
      "Different!!!!  tensor([7.5119], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2630\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2631\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2632\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2633\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2634\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2635\n",
      "Different!!!!  tensor([4.0151], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2636\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2637\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2638\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2639\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2640\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2641\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2642\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2643\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2644\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2645\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2646\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2647\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2648\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2649\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2650\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2651\n",
      "Different!!!!  tensor([5.3252], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2652\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2654\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2655\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2656\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2657\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2658\n",
      "Different!!!!  tensor([7.7004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2659\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2660\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2661\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2662\n",
      "Different!!!!  tensor([6.1735], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2663\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2664\n",
      "Different!!!!  tensor([5.2310], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2665\n",
      "Different!!!!  tensor([3.7324], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2666\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2667\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2668\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2669\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2670\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2671\n",
      "Different!!!!  tensor([4.3450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2672\n",
      "Different!!!!  tensor([9.2555], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2673\n",
      "Different!!!!  tensor([8.2753], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2674\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2675\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2676\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2677\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2678\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2679\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2680\n",
      "Different!!!!  tensor([7.3611], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2681\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2682\n",
      "Different!!!!  tensor([6.8144], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2683\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2684\n",
      "Different!!!!  tensor([4.2413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2685\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2686\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2687\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2688\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2689\n",
      "Different!!!!  tensor([7.8229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2690\n",
      "Different!!!!  tensor([6.4280], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2691\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2692\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2693\n",
      "Different!!!!  tensor([3.8643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2694\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2695\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2696\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2697\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2698\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2699\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2700\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2701\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2702\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2703\n",
      "Different!!!!  tensor([8.3413], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2704\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2705\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2706\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2707\n",
      "Different!!!!  tensor([3.8643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2708\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2709\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2710\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2711\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2712\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2713\n",
      "Different!!!!  tensor([8.2093], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2714\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2715\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2717\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2718\n",
      "Different!!!!  tensor([6.7673], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2719\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2720\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2721\n",
      "Different!!!!  tensor([4.6560], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2722\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2723\n",
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2724\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2725\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2726\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2727\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2728\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2729\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2730\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2731\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2732\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2733\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2734\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2735\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2736\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2737\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2738\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2739\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2740\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2741\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2742\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2743\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2744\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2745\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2746\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2747\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2748\n",
      "Different!!!!  tensor([8.2188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2749\n",
      "Different!!!!  tensor([7.2291], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2750\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2751\n",
      "Different!!!!  tensor([7.8983], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2752\n",
      "Different!!!!  tensor([6.0415], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2753\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2754\n",
      "Different!!!!  tensor([7.6532], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2755\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2756\n",
      "Different!!!!  tensor([7.0218], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2757\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2758\n",
      "Different!!!!  tensor([9.6608], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2759\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2760\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2761\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2762\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2763\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2764\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2765\n",
      "Different!!!!  tensor([3.9869], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2766\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2767\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2768\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2769\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2770\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2771\n",
      "Different!!!!  tensor([8.8314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2772\n",
      "Different!!!!  tensor([3.8643], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2773\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2774\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2775\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2776\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2777\n",
      "Different!!!!  tensor([6.9746], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2778\n",
      "Different!!!!  tensor([6.5693], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2780\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2781\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2782\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2783\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2784\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2785\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2786\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2787\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2788\n",
      "Different!!!!  tensor([4.9482], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2789\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2790\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2791\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2792\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2793\n",
      "Different!!!!  tensor([7.7192], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2794\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2795\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2796\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2797\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2798\n",
      "Different!!!!  tensor([7.3328], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2799\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2800\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2801\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2802\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2803\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2804\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2805\n",
      "Different!!!!  tensor([4.3073], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2806\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2807\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2808\n",
      "Different!!!!  tensor([5.4666], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2809\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2810\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2811\n",
      "Different!!!!  tensor([8.2470], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2812\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2813\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2814\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2815\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2816\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2817\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2818\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2819\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2820\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2821\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2822\n",
      "Different!!!!  tensor([7.1254], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2823\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2824\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2825\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2826\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2827\n",
      "Different!!!!  tensor([4.8540], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2828\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2829\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2830\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2831\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2832\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2833\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2834\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2835\n",
      "Different!!!!  tensor([7.8323], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2836\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2837\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2838\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2839\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2840\n",
      "Different!!!!  tensor([9.2744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2841\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2842\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2843\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2845\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2846\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2847\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2848\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2849\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2850\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2851\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2852\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2853\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2854\n",
      "Different!!!!  tensor([8.2753], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2855\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2856\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2857\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2858\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2859\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2860\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2861\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2862\n",
      "Different!!!!  tensor([7.4836], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2863\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2864\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2865\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2866\n",
      "Different!!!!  tensor([4.3733], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2867\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2868\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2869\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2870\n",
      "Different!!!!  tensor([6.2112], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2871\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2872\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2873\n",
      "Different!!!!  tensor([7.4647], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2874\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2875\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2876\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2877\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2878\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2879\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2880\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2881\n",
      "Different!!!!  tensor([4.5712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2882\n",
      "Different!!!!  tensor([7.3234], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2883\n",
      "Different!!!!  tensor([8.4732], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2884\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2885\n",
      "Different!!!!  tensor([7.8229], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2886\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2887\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2888\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2889\n",
      "Different!!!!  tensor([8.3036], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2890\n",
      "Different!!!!  tensor([8.2470], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2891\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2892\n",
      "Different!!!!  tensor([5.9661], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2893\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2894\n",
      "Different!!!!  tensor([5.0236], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2895\n",
      "Different!!!!  tensor([8.6712], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2896\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2897\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2898\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2899\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2900\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2901\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2902\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2903\n",
      "Different!!!!  tensor([7.7098], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2904\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2905\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2906\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2908\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2909\n",
      "Different!!!!  tensor([7.8417], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2910\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2911\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2912\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2913\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2914\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2915\n",
      "Different!!!!  tensor([8.0208], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2916\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2917\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2918\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2919\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2920\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2921\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2922\n",
      "Different!!!!  tensor([5.6645], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2923\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2924\n",
      "Different!!!!  tensor([3.6381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2925\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2926\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2927\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2928\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2929\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2930\n",
      "Different!!!!  tensor([4.4581], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2931\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2932\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2933\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2934\n",
      "Different!!!!  tensor([4.8351], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2935\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2936\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2937\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2938\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2939\n",
      "Different!!!!  tensor([4.3356], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2940\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2941\n",
      "Different!!!!  tensor([6.4468], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2942\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2943\n",
      "Different!!!!  tensor([7.2668], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2944\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2945\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2946\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2947\n",
      "Different!!!!  tensor([3.8926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2948\n",
      "Different!!!!  tensor([7.3893], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2949\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2950\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2951\n",
      "Different!!!!  tensor([6.2206], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2952\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2953\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2954\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2955\n",
      "Different!!!!  tensor([7.2103], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2956\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2957\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2958\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2959\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2960\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2961\n",
      "Different!!!!  tensor([7.6721], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2962\n",
      "Different!!!!  tensor([6.0792], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2963\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2964\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2965\n",
      "Different!!!!  tensor([7.6344], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2966\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2967\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2968\n",
      "Different!!!!  tensor([7.4742], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2970\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2971\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2972\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2973\n",
      "Different!!!!  tensor([4.7597], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2974\n",
      "Different!!!!  tensor([3.4025], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2975\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2976\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2977\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2978\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2979\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2980\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2981\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2982\n",
      "Different!!!!  tensor([4.0717], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2983\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2984\n",
      "Different!!!!  tensor([5.6551], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2985\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2986\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2987\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2988\n",
      "Different!!!!  tensor([3.9774], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2989\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2990\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2991\n",
      "Different!!!!  tensor([7.7475], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2992\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2993\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2994\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2995\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2996\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2997\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2998\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "2999\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3000\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3001\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3002\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3003\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3004\n",
      "Different!!!!  tensor([6.9181], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3005\n",
      "Different!!!!  tensor([3.8078], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3006\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3007\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3008\n",
      "Different!!!!  tensor([7.4270], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3009\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3010\n",
      "Different!!!!  tensor([5.5514], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3011\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3012\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3013\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3014\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3015\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3016\n",
      "Different!!!!  tensor([6.7107], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3017\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3018\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3019\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3020\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3021\n",
      "Different!!!!  tensor([5.8719], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3022\n",
      "Different!!!!  tensor([8.2847], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3023\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3024\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3025\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3026\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3027\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3028\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3029\n",
      "Different!!!!  tensor([7.1914], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3030\n",
      "Different!!!!  tensor([6.5976], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3031\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3033\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3034\n",
      "Different!!!!  tensor([4.6937], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3035\n",
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3036\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3037\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3038\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3039\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3040\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3041\n",
      "Different!!!!  tensor([4.4770], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3042\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3043\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3044\n",
      "Different!!!!  tensor([7.1726], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3045\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3046\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3047\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3048\n",
      "Different!!!!  tensor([5.4101], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3049\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3050\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3051\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3052\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3053\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3054\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3055\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3056\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3057\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3058\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3059\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3060\n",
      "Different!!!!  tensor([4.4298], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3061\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3062\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3063\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3064\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3065\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3066\n",
      "Different!!!!  tensor([6.3149], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3067\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3068\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3069\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3070\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3071\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3072\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3073\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3074\n",
      "Different!!!!  tensor([5.4478], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3075\n",
      "Different!!!!  tensor([4.4393], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3076\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3077\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3078\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3079\n",
      "Different!!!!  tensor([4.7126], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3080\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3081\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3082\n",
      "Different!!!!  tensor([8.2659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3083\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3084\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3085\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3086\n",
      "Different!!!!  tensor([8.4355], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3087\n",
      "Different!!!!  tensor([6.7390], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3088\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3089\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3090\n",
      "Different!!!!  tensor([4.4204], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3091\n",
      "Different!!!!  tensor([4.7032], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3092\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3093\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3094\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([7.0029], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3096\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3097\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3098\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3099\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3100\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3101\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3102\n",
      "Different!!!!  tensor([6.8615], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3103\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3104\n",
      "Different!!!!  tensor([5.9190], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3105\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3106\n",
      "Different!!!!  tensor([5.7494], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3107\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3108\n",
      "Different!!!!  tensor([5.8436], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3109\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3110\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3111\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3112\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3113\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3114\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3115\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3116\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3117\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3118\n",
      "Different!!!!  tensor([3.6570], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3119\n",
      "Different!!!!  tensor([8.2188], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3120\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3121\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3122\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3123\n",
      "Different!!!!  tensor([6.1923], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3124\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3125\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3126\n",
      "Different!!!!  tensor([8.5675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3127\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3128\n",
      "Different!!!!  tensor([8.2470], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3129\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3130\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3131\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3132\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3133\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3134\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3135\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3136\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3137\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3138\n",
      "Different!!!!  tensor([7.8700], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3139\n",
      "Different!!!!  tensor([4.6089], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3140\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3141\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3142\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3143\n",
      "Different!!!!  tensor([5.0896], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3144\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3145\n",
      "Different!!!!  tensor([6.8050], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3146\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3147\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3148\n",
      "Different!!!!  tensor([5.9850], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3149\n",
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3150\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3151\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3152\n",
      "Different!!!!  tensor([7.5024], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3153\n",
      "Different!!!!  tensor([6.8710], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3154\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3155\n",
      "Different!!!!  tensor([6.5788], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3156\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3157\n",
      "Different!!!!  tensor([7.0500], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.2960], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3159\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3160\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3161\n",
      "Different!!!!  tensor([5.7022], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3162\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3163\n",
      "Different!!!!  tensor([6.8333], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3164\n",
      "Different!!!!  tensor([5.8153], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3165\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3166\n",
      "Different!!!!  tensor([9.0387], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3167\n",
      "Different!!!!  tensor([4.0246], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3168\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3169\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3170\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3171\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3172\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3173\n",
      "Different!!!!  tensor([7.3139], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3174\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3175\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3176\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3177\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3178\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3179\n",
      "Different!!!!  tensor([7.6815], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3180\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3181\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3182\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3183\n",
      "Different!!!!  tensor([4.8445], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3184\n",
      "Different!!!!  tensor([3.6381], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3185\n",
      "Different!!!!  tensor([5.2875], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3186\n",
      "Different!!!!  tensor([6.0038], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3187\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3188\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3189\n",
      "Different!!!!  tensor([4.5052], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3190\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3191\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3192\n",
      "Different!!!!  tensor([6.8238], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3193\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3194\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3195\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3196\n",
      "Different!!!!  tensor([8.0397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3197\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3198\n",
      "Different!!!!  tensor([5.2121], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3199\n",
      "Different!!!!  tensor([7.0783], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3200\n",
      "Different!!!!  tensor([6.8050], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3201\n",
      "Different!!!!  tensor([6.5128], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3202\n",
      "Different!!!!  tensor([6.1075], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3203\n",
      "Different!!!!  tensor([7.2480], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3204\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3205\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3206\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3207\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3208\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3209\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3210\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3211\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3212\n",
      "Different!!!!  tensor([5.5986], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3213\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3214\n",
      "Different!!!!  tensor([3.6664], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3215\n",
      "Different!!!!  tensor([3.6287], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3216\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3217\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3218\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3219\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3220\n",
      "Different!!!!  tensor([4.7974], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3222\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3223\n",
      "Different!!!!  tensor([7.3516], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3224\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3225\n",
      "Different!!!!  tensor([3.6287], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3226\n",
      "Different!!!!  tensor([5.8059], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3227\n",
      "Different!!!!  tensor([7.2008], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3228\n",
      "Different!!!!  tensor([6.8521], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3229\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3230\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3231\n",
      "Different!!!!  tensor([4.3544], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3232\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3233\n",
      "Different!!!!  tensor([4.2979], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3234\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3235\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3236\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3237\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3238\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3239\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3240\n",
      "Different!!!!  tensor([4.5147], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3241\n",
      "Different!!!!  tensor([3.9869], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3242\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3243\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3244\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3245\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3246\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3247\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3248\n",
      "Different!!!!  tensor([4.0246], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3249\n",
      "Different!!!!  tensor([6.2677], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3250\n",
      "Different!!!!  tensor([8.3507], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3251\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3252\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3253\n",
      "Different!!!!  tensor([5.0613], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3254\n",
      "Different!!!!  tensor([4.9388], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3255\n",
      "Different!!!!  tensor([3.7135], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3256\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3257\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3258\n",
      "Different!!!!  tensor([7.5307], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3259\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3260\n",
      "Different!!!!  tensor([3.7889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3261\n",
      "Different!!!!  tensor([7.9926], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3262\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3263\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3264\n",
      "Different!!!!  tensor([8.3601], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3265\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3266\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3267\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3268\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3269\n",
      "Different!!!!  tensor([9.7174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3270\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3271\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3272\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3273\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3274\n",
      "Different!!!!  tensor([6.9369], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3275\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3276\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3277\n",
      "Different!!!!  tensor([7.1066], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3278\n",
      "Different!!!!  tensor([6.0321], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3279\n",
      "Different!!!!  tensor([4.7503], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3280\n",
      "Different!!!!  tensor([4.7409], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3281\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3282\n",
      "Different!!!!  tensor([4.9671], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3283\n",
      "Different!!!!  tensor([4.1377], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.0698], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3285\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3286\n",
      "Different!!!!  tensor([7.8889], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3287\n",
      "Different!!!!  tensor([4.9199], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3288\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3289\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3290\n",
      "Different!!!!  tensor([5.4572], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3291\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3292\n",
      "Different!!!!  tensor([5.1933], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3293\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3294\n",
      "Different!!!!  tensor([6.5882], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3295\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3296\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3297\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3298\n",
      "Different!!!!  tensor([5.6834], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3299\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3300\n",
      "Different!!!!  tensor([4.3921], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3301\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3302\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3303\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3304\n",
      "Different!!!!  tensor([4.0811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3305\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3306\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3307\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3308\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3309\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3310\n",
      "Different!!!!  tensor([4.5241], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3311\n",
      "Different!!!!  tensor([5.4383], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3312\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3313\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3314\n",
      "Different!!!!  tensor([7.0972], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3315\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3316\n",
      "Different!!!!  tensor([6.1829], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3317\n",
      "Different!!!!  tensor([5.7871], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3318\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3319\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3320\n",
      "Different!!!!  tensor([5.6174], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3321\n",
      "Different!!!!  tensor([4.3639], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3322\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3323\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3324\n",
      "Different!!!!  tensor([9.7928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3325\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3326\n",
      "Different!!!!  tensor([8.0397], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3327\n",
      "Different!!!!  tensor([8.2659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3328\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3329\n",
      "Different!!!!  tensor([5.2687], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3330\n",
      "Different!!!!  tensor([5.9096], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3331\n",
      "Different!!!!  tensor([8.3884], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3332\n",
      "Different!!!!  tensor([6.6259], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3333\n",
      "Different!!!!  tensor([5.5797], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3334\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3335\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3336\n",
      "Different!!!!  tensor([7.6438], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3337\n",
      "Different!!!!  tensor([4.1659], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3338\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3339\n",
      "Different!!!!  tensor([6.7202], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3340\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3341\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3342\n",
      "Different!!!!  tensor([7.2385], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3343\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3345\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3346\n",
      "Different!!!!  tensor([6.7861], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3347\n",
      "Different!!!!  tensor([7.1349], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3348\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3349\n",
      "Different!!!!  tensor([7.4082], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3350\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3351\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3352\n",
      "Different!!!!  tensor([5.8248], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3353\n",
      "Different!!!!  tensor([4.8257], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3354\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3355\n",
      "Different!!!!  tensor([5.5232], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3356\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3357\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3358\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3359\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3360\n",
      "Different!!!!  tensor([4.2885], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3361\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3362\n",
      "Different!!!!  tensor([7.6061], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3363\n",
      "Different!!!!  tensor([5.7965], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3364\n",
      "Different!!!!  tensor([5.9473], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3365\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3366\n",
      "Different!!!!  tensor([4.2131], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3367\n",
      "Different!!!!  tensor([6.0604], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3368\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3369\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3370\n",
      "Different!!!!  tensor([8.1622], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3371\n",
      "Different!!!!  tensor([4.5806], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3372\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3373\n",
      "Different!!!!  tensor([4.3262], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3374\n",
      "Different!!!!  tensor([6.2300], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3375\n",
      "Different!!!!  tensor([6.6919], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3376\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3377\n",
      "Different!!!!  tensor([4.4675], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3378\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3379\n",
      "Different!!!!  tensor([4.1282], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3380\n",
      "Different!!!!  tensor([7.9737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3381\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3382\n",
      "Different!!!!  tensor([5.3064], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3383\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3384\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3385\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3386\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3387\n",
      "Different!!!!  tensor([6.5034], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3388\n",
      "Different!!!!  tensor([4.0623], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3389\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3390\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3391\n",
      "Different!!!!  tensor([7.0406], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3392\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3393\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3394\n",
      "Different!!!!  tensor([5.8530], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3395\n",
      "Different!!!!  tensor([5.9756], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3396\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3397\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3398\n",
      "Different!!!!  tensor([5.1461], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3399\n",
      "Different!!!!  tensor([5.6268], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3400\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3401\n",
      "Different!!!!  tensor([4.4110], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3402\n",
      "Different!!!!  tensor([7.4459], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3403\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3404\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3405\n",
      "Different!!!!  tensor([9.6043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3406\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.3054], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3408\n",
      "Different!!!!  tensor([7.9266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3409\n",
      "Different!!!!  tensor([7.1631], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3410\n",
      "Different!!!!  tensor([5.3347], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3411\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3412\n",
      "Different!!!!  tensor([5.1650], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3413\n",
      "Different!!!!  tensor([6.4751], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3414\n",
      "Different!!!!  tensor([5.7305], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3415\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3416\n",
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3417\n",
      "Different!!!!  tensor([6.6071], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3418\n",
      "Different!!!!  tensor([5.2592], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3419\n",
      "Different!!!!  tensor([6.4939], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3420\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3421\n",
      "Different!!!!  tensor([5.9567], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3422\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3423\n",
      "Different!!!!  tensor([6.8804], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3424\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3425\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3426\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3427\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3428\n",
      "Different!!!!  tensor([5.1273], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3429\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3430\n",
      "Different!!!!  tensor([6.4845], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3431\n",
      "Different!!!!  tensor([5.7211], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3432\n",
      "Different!!!!  tensor([7.1160], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3433\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3434\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3435\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3436\n",
      "Different!!!!  tensor([6.6353], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3437\n",
      "Different!!!!  tensor([5.4949], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3438\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3439\n",
      "Different!!!!  tensor([6.1169], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3440\n",
      "Different!!!!  tensor([7.0123], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3441\n",
      "Different!!!!  tensor([5.6080], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3442\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3443\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3444\n",
      "Different!!!!  tensor([7.7758], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3445\n",
      "Different!!!!  tensor([6.3337], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3446\n",
      "Different!!!!  tensor([4.2319], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3447\n",
      "Different!!!!  tensor([7.9549], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3448\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3449\n",
      "Different!!!!  tensor([6.7767], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3450\n",
      "Different!!!!  tensor([5.8813], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3451\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3452\n",
      "Different!!!!  tensor([6.5316], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3453\n",
      "Different!!!!  tensor([5.9002], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3454\n",
      "Different!!!!  tensor([4.7691], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3455\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3456\n",
      "Different!!!!  tensor([6.0227], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3457\n",
      "Different!!!!  tensor([5.6740], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3458\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3459\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3460\n",
      "Different!!!!  tensor([3.9774], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3461\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3462\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3463\n",
      "Different!!!!  tensor([8.7183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3464\n",
      "Different!!!!  tensor([6.5411], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3465\n",
      "Different!!!!  tensor([6.9935], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3466\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3467\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3468\n",
      "Different!!!!  tensor([4.2696], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3469\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([7.3705], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3471\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3472\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3473\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3474\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3475\n",
      "Different!!!!  tensor([6.8427], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3476\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3477\n",
      "Different!!!!  tensor([6.9087], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3478\n",
      "Different!!!!  tensor([8.2565], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3479\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3480\n",
      "Different!!!!  tensor([5.3724], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3481\n",
      "Different!!!!  tensor([8.3978], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3482\n",
      "Different!!!!  tensor([4.9765], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3483\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3484\n",
      "Different!!!!  tensor([8.9539], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3485\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3486\n",
      "Different!!!!  tensor([7.3422], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3487\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3488\n",
      "Different!!!!  tensor([7.6815], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3489\n",
      "Different!!!!  tensor([4.5429], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3490\n",
      "Different!!!!  tensor([6.4185], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3491\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3492\n",
      "Different!!!!  tensor([4.4958], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3493\n",
      "Different!!!!  tensor([6.5222], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3494\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3495\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3496\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3497\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3498\n",
      "Different!!!!  tensor([7.7286], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3499\n",
      "Different!!!!  tensor([5.1744], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3500\n",
      "Different!!!!  tensor([4.6843], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3501\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3502\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3503\n",
      "Different!!!!  tensor([6.6825], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3504\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3505\n",
      "Different!!!!  tensor([5.0048], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3506\n",
      "Different!!!!  tensor([8.0020], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3507\n",
      "Different!!!!  tensor([8.7654], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3508\n",
      "Different!!!!  tensor([5.5420], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3509\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3510\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3511\n",
      "Different!!!!  tensor([3.7512], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3512\n",
      "Different!!!!  tensor([6.6730], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3513\n",
      "Different!!!!  tensor([6.0133], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3514\n",
      "Different!!!!  tensor([4.8634], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3515\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3516\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3517\n",
      "Different!!!!  tensor([3.0820], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3518\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3519\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3520\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3521\n",
      "Different!!!!  tensor([7.7663], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3522\n",
      "Different!!!!  tensor([5.9944], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3523\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3524\n",
      "Different!!!!  tensor([4.5901], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3525\n",
      "Different!!!!  tensor([4.9294], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3526\n",
      "Different!!!!  tensor([6.4091], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3527\n",
      "Different!!!!  tensor([5.2498], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3528\n",
      "Different!!!!  tensor([7.9360], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3529\n",
      "Different!!!!  tensor([6.5505], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3530\n",
      "Different!!!!  tensor([5.6928], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3531\n",
      "Different!!!!  tensor([6.3903], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3532\n",
      "Different!!!!  tensor([6.0887], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3534\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3535\n",
      "Different!!!!  tensor([5.4289], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3536\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3537\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3538\n",
      "Different!!!!  tensor([7.9172], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3539\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3540\n",
      "Different!!!!  tensor([5.0519], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3541\n",
      "Different!!!!  tensor([7.2857], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3542\n",
      "Different!!!!  tensor([7.8323], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3543\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3544\n",
      "Different!!!!  tensor([4.1000], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3545\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3546\n",
      "Different!!!!  tensor([4.5995], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3547\n",
      "Different!!!!  tensor([3.2517], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3548\n",
      "Different!!!!  tensor([4.0528], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3549\n",
      "Different!!!!  tensor([5.8342], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3550\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3551\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3552\n",
      "Different!!!!  tensor([8.4167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3553\n",
      "Different!!!!  tensor([5.3535], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3554\n",
      "Different!!!!  tensor([6.0981], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3555\n",
      "Different!!!!  tensor([3.8266], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3556\n",
      "Different!!!!  tensor([7.7098], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3557\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3558\n",
      "Different!!!!  tensor([4.6278], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3559\n",
      "Different!!!!  tensor([6.2395], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3560\n",
      "Different!!!!  tensor([6.7484], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3561\n",
      "Different!!!!  tensor([7.5213], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3562\n",
      "Different!!!!  tensor([7.5778], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3563\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3564\n",
      "Different!!!!  tensor([8.1339], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3565\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3566\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3567\n",
      "Different!!!!  tensor([5.7776], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3568\n",
      "Different!!!!  tensor([4.9576], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3569\n",
      "Different!!!!  tensor([7.4553], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3570\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3571\n",
      "Different!!!!  tensor([5.7682], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3572\n",
      "Different!!!!  tensor([5.5043], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3573\n",
      "Different!!!!  tensor([6.3431], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3574\n",
      "Different!!!!  tensor([5.0425], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3575\n",
      "Different!!!!  tensor([5.1838], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3576\n",
      "Different!!!!  tensor([5.7399], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3577\n",
      "Different!!!!  tensor([4.7314], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3578\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3579\n",
      "Different!!!!  tensor([4.4487], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3580\n",
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3581\n",
      "Different!!!!  tensor([6.3243], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3582\n",
      "Different!!!!  tensor([4.8822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3583\n",
      "Different!!!!  tensor([5.2215], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3584\n",
      "Different!!!!  tensor([7.6155], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3585\n",
      "Different!!!!  tensor([5.8625], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3586\n",
      "Different!!!!  tensor([4.7786], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3587\n",
      "Different!!!!  tensor([7.1443], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3588\n",
      "Different!!!!  tensor([5.5609], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3589\n",
      "Different!!!!  tensor([7.4459], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3590\n",
      "Different!!!!  tensor([5.3158], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3591\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3592\n",
      "Different!!!!  tensor([6.6070], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3593\n",
      "Different!!!!  tensor([6.3808], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3594\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3595\n",
      "Different!!!!  tensor([5.4760], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.9652], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3597\n",
      "Different!!!!  tensor([6.1452], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3598\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3599\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3600\n",
      "Different!!!!  tensor([5.1367], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3601\n",
      "Different!!!!  tensor([5.8907], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3602\n",
      "Different!!!!  tensor([5.0802], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3603\n",
      "Different!!!!  tensor([7.3045], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3604\n",
      "Different!!!!  tensor([6.9558], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3605\n",
      "Different!!!!  tensor([4.4016], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3606\n",
      "Different!!!!  tensor([4.9859], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3607\n",
      "Different!!!!  tensor([8.5392], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3608\n",
      "Different!!!!  tensor([6.8050], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3609\n",
      "Different!!!!  tensor([5.4855], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3610\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3611\n",
      "Different!!!!  tensor([5.0707], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3612\n",
      "Different!!!!  tensor([7.3988], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3613\n",
      "Different!!!!  tensor([6.9464], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3614\n",
      "Different!!!!  tensor([5.2404], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3615\n",
      "Different!!!!  tensor([6.2489], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3616\n",
      "Different!!!!  tensor([4.5618], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3617\n",
      "Different!!!!  tensor([6.4657], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3618\n",
      "Different!!!!  tensor([8.1811], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3619\n",
      "Different!!!!  tensor([6.7013], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3620\n",
      "Different!!!!  tensor([5.1556], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3621\n",
      "Different!!!!  tensor([4.3827], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3622\n",
      "Different!!!!  tensor([7.2951], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3623\n",
      "Different!!!!  tensor([7.5684], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3624\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3625\n",
      "Different!!!!  tensor([5.7588], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3626\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3627\n",
      "Different!!!!  tensor([5.9379], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3628\n",
      "Different!!!!  tensor([4.1942], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3629\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3630\n",
      "Different!!!!  tensor([5.2027], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3631\n",
      "Different!!!!  tensor([4.9105], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3632\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3633\n",
      "Different!!!!  tensor([8.4167], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3634\n",
      "Different!!!!  tensor([6.7956], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3635\n",
      "Different!!!!  tensor([5.0990], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3636\n",
      "Different!!!!  tensor([6.7296], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3637\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3638\n",
      "Different!!!!  tensor([5.5703], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3639\n",
      "Different!!!!  tensor([6.2583], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3640\n",
      "Different!!!!  tensor([5.9284], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3641\n",
      "Different!!!!  tensor([9.1047], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3642\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3643\n",
      "Different!!!!  tensor([4.8068], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3644\n",
      "Different!!!!  tensor([4.3450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3645\n",
      "Different!!!!  tensor([6.3714], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3646\n",
      "Different!!!!  tensor([5.2593], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3647\n",
      "Different!!!!  tensor([4.7220], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3648\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3649\n",
      "Different!!!!  tensor([7.2574], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3650\n",
      "Different!!!!  tensor([5.1179], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3651\n",
      "Different!!!!  tensor([7.9549], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3652\n",
      "Different!!!!  tensor([5.3629], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3653\n",
      "Different!!!!  tensor([4.0434], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3654\n",
      "Different!!!!  tensor([8.4450], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3655\n",
      "Different!!!!  tensor([7.3705], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3656\n",
      "Different!!!!  tensor([6.6448], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3657\n",
      "Different!!!!  tensor([3.8737], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different!!!!  tensor([6.2772], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3659\n",
      "Different!!!!  tensor([5.4006], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3660\n",
      "Different!!!!  tensor([6.6165], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3661\n",
      "Different!!!!  tensor([5.3818], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3662\n",
      "Different!!!!  tensor([4.5524], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3663\n",
      "Different!!!!  tensor([5.5326], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3664\n",
      "Different!!!!  tensor([6.7579], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3665\n",
      "Different!!!!  tensor([3.7418], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3666\n",
      "Different!!!!  tensor([6.2018], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3667\n",
      "Different!!!!  tensor([4.8163], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3668\n",
      "Different!!!!  tensor([6.6542], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3669\n",
      "Different!!!!  tensor([4.6183], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3670\n",
      "Different!!!!  tensor([8.7371], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3671\n",
      "Different!!!!  tensor([7.4459], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3672\n",
      "Different!!!!  tensor([6.1641], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3673\n",
      "Different!!!!  tensor([5.7117], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3674\n",
      "Different!!!!  tensor([3.9303], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3675\n",
      "Different!!!!  tensor([7.2762], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3676\n",
      "Different!!!!  tensor([5.6363], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3677\n",
      "Different!!!!  tensor([3.6004], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3678\n",
      "Different!!!!  tensor([4.6372], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3679\n",
      "Different!!!!  tensor([8.1057], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3680\n",
      "Different!!!!  tensor([7.0877], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3681\n",
      "Different!!!!  tensor([3.9491], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3682\n",
      "Different!!!!  tensor([5.5891], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3683\n",
      "Different!!!!  tensor([6.1358], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3684\n",
      "Different!!!!  tensor([6.4374], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3685\n",
      "Different!!!!  tensor([6.3997], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3686\n",
      "Different!!!!  tensor([3.9114], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3687\n",
      "Different!!!!  tensor([5.0142], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3688\n",
      "Different!!!!  tensor([4.9011], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3689\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3690\n",
      "Different!!!!  tensor([6.0510], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3691\n",
      "Different!!!!  tensor([8.0868], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3692\n",
      "Different!!!!  tensor([5.1084], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3693\n",
      "Different!!!!  tensor([5.4195], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3694\n",
      "Different!!!!  tensor([5.3912], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3695\n",
      "Different!!!!  tensor([4.2790], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3696\n",
      "Different!!!!  tensor([4.9953], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3697\n",
      "Different!!!!  tensor([5.6457], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3698\n",
      "Different!!!!  tensor([6.1264], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3699\n",
      "Different!!!!  tensor([6.5694], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3700\n",
      "Different!!!!  tensor([4.6655], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3701\n",
      "Different!!!!  tensor([7.9831], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3702\n",
      "Different!!!!  tensor([4.7880], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3703\n",
      "Different!!!!  tensor([5.2781], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3704\n",
      "Different!!!!  tensor([4.6466], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3705\n",
      "Different!!!!  tensor([8.4638], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3706\n",
      "Different!!!!  tensor([5.5137], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3707\n",
      "Different!!!!  tensor([7.5401], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3708\n",
      "Different!!!!  tensor([7.9454], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3709\n",
      "Different!!!!  tensor([5.3441], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3710\n",
      "Different!!!!  tensor([5.2970], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3711\n",
      "Different!!!!  tensor([6.2866], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3712\n",
      "Different!!!!  tensor([8.0868], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3713\n",
      "Different!!!!  tensor([4.8728], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3714\n",
      "Different!!!!  tensor([8.9822], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3715\n",
      "Different!!!!  tensor([4.1094], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3716\n",
      "Different!!!!  tensor([6.1546], device='cuda:0', grad_fn=<MaxBackward0>) tensor([35.7500], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "3717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1116c6f90797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_real_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalues_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvalues_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/models/cifar10/mobilenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compare fake-quantized validation and inference\n",
    "N = 10000\n",
    "error = 0\n",
    "for i in range(N):\n",
    "    #i, (inputs, target) = next(enumerate(val_loader))\n",
    "    inputs = torch.randn(1,3,32,32)\n",
    "    input_var = Variable(inputs.type(ttype))\n",
    "    quantizer.store_and_quantize(training=training)\n",
    "    output_1 = model(input_var)\n",
    "    quantizer.restore_real_value()            \n",
    "    output_2 = quantizer.deployment_model(input_var)\n",
    "    values_1, indices_1 = output_1.max(1)\n",
    "    values_2, indices_2 = output_2.max(1)\n",
    "    if indices_1[0].item() == indices_2[0].item():\n",
    "        pass\n",
    "    else:\n",
    "        print('Different!!!! ', values_1, values_2)\n",
    "        print(i)\n",
    "        error += 1\n",
    "\n",
    "if error == 0:\n",
    "    print('check OKKKK!!!!')\n",
    "else:\n",
    "    print('number of errors', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 1],\n",
       "        [1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x > 0)| (x > -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -2.1960],\n",
       "        [ 0.4333,  0.0000],\n",
       "        [ 0.3283,  0.3488]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.masked_fill_(x>0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -2.1960],\n",
       "        [ 0.4333,  0.0000],\n",
       "        [ 0.3283,  0.3488]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2], device='cuda:0') tensor([5], device='cuda:0')\n",
      "tensor([[ 1.0216,  1.0125,  2.6960,  1.9408, -1.0364,  2.6033, -0.5207, -0.8645,\n",
      "         -5.0692, -1.7818]], device='cuda:0', grad_fn=<ThAddmmBackward>)\n",
      "tensor([[  7290.0000,   7771.5000,  20460.5000,  15013.5000,  -7461.7500,\n",
      "          20608.2500,  -4499.2500,  -6394.0000, -39368.0000, -13401.0000]],\n",
      "       device='cuda:0', grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(indices_1, indices_2)\n",
    "print(output_1)\n",
    "print(output_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize(training=training)\n",
    "bias_rr = model.module.fc.bias.data.clone()\n",
    "oo1 = model(input_var)- bias_rr\n",
    "quantizer.restore_real_value()\n",
    "oo2 = quantizer.deployment_model(input_var)  - quantizer.deployment_model.fc[0].bias.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([[ 1.6300,  3.8400,  2.8988, -1.6477, -1.8950, -0.0602,  2.2296, -1.1928,\n",
      "         -3.9970, -1.8142]], device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([[ 12508.5000,  29883.5000,  21955.5000, -12784.5000, -13882.0000,\n",
      "           -348.2500,  16404.2500,  -8747.5000, -31339.2500, -13704.2500]],\n",
      "       device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([ 0.0271, -0.6058,  0.3413,  0.7958,  0.3924, -0.1132, -0.1445,  0.3799,\n",
      "        -0.5359, -0.5703], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "oo_values_1, oo_indices_1 = oo1.max(1)\n",
    "oo_values_2, oo_indices_2 = oo2.max(1)\n",
    "print(oo_indices_1, oo_indices_2)\n",
    "print(oo1)\n",
    "print(oo2)\n",
    "print(bias_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0') tensor([1], device='cuda:0')\n",
      "tensor([[ 1.6300,  3.8400,  2.8988, -1.6477, -1.8950, -0.0602,  2.2296, -1.1928,\n",
      "         -3.9970, -1.8142]], device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([[ 12508.5000,  29883.5000,  21955.5000, -12784.5000, -13882.0000,\n",
      "           -348.2500,  16404.2500,  -8747.5000, -31339.2500, -13704.2500]],\n",
      "       device='cuda:0', grad_fn=<ThSubBackward>)\n",
      "tensor([ 0.0271, -0.6058,  0.3413,  0.7958,  0.3924, -0.1132, -0.1445,  0.3799,\n",
      "        -0.5359, -0.5703], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "oo_values_1, oo_indices_1 = oo1.max(1)\n",
    "oo_values_2, oo_indices_2 = oo2.max(1)\n",
    "print(oo_indices_1, oo_indices_2)\n",
    "print(oo1)\n",
    "print(oo2)\n",
    "print(bias_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  207., -4646.,  2615.,  6101.,  3010.,  -870., -1108.,  2911., -4107.,\n",
       "        -4371.], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.fc[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7513, -1.5044,  2.8504,  2.2066, -1.1107, -0.1140,  2.7132, -0.2327,\n",
       "         -3.6545, -2.9618]], device='cuda:0', grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12914.2500, -10423.2500,  21100.5000,  16114.0000,  -8682.7500,\n",
       "           -955.2500,  21532.7500,  -1240.7500, -28277.2500, -22523.2500]],\n",
       "       device='cuda:0', grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0269, -0.6061,  0.3412,  0.7959,  0.3926, -0.1135, -0.1445,  0.3798,\n",
       "        -0.5357, -0.5702], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.fc.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  207., -4646.,  2615.,  6101.,  3010.,  -870., -1108.,  2911., -4107.,\n",
       "        -4371.], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.fc[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i_l,layer in enumerate(quantizer.param_to_quantize):\n",
    "    print(layer['act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0269, -0.6061,  0.3412,  0.7959,  0.3926, -0.1135, -0.1445,  0.3798,\n",
       "        -0.5357, -0.5702], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ScaledClippedLinearQuantization(M=0.1803930699825287, clip_val=255)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): ScaledClippedLinearQuantization(M=2.054954767227173, clip_val=255)\n",
       "    (4): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ScaledClippedLinearQuantization(M=0.008188934065401554, clip_val=255)\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): ScaledClippedLinearQuantization(M=0.7202885746955872, clip_val=255)\n",
       "    (8): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): ScaledClippedLinearQuantization(M=0.004449482541531324, clip_val=255)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): ScaledClippedLinearQuantization(M=0.5195009112358093, clip_val=255)\n",
       "    (12): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): ScaledClippedLinearQuantization(M=0.0037819177377969027, clip_val=255)\n",
       "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): ScaledClippedLinearQuantization(M=0.38205116987228394, clip_val=255)\n",
       "    (16): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): ScaledClippedLinearQuantization(M=0.0029082787223160267, clip_val=255)\n",
       "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): ScaledClippedLinearQuantization(M=0.538362443447113, clip_val=255)\n",
       "    (20): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): ScaledClippedLinearQuantization(M=0.0026687942445278168, clip_val=255)\n",
       "    (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): ScaledClippedLinearQuantization(M=0.465130478143692, clip_val=255)\n",
       "    (24): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): ScaledClippedLinearQuantization(M=0.002053817268460989, clip_val=255)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): ScaledClippedLinearQuantization(M=0.4517885148525238, clip_val=255)\n",
       "    (28): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): ScaledClippedLinearQuantization(M=0.002201623748987913, clip_val=255)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): ScaledClippedLinearQuantization(M=0.41972294449806213, clip_val=255)\n",
       "    (32): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): ScaledClippedLinearQuantization(M=0.0022723807487636805, clip_val=255)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): ScaledClippedLinearQuantization(M=0.29105091094970703, clip_val=255)\n",
       "    (36): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ScaledClippedLinearQuantization(M=0.0024077112320810556, clip_val=255)\n",
       "    (38): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): ScaledClippedLinearQuantization(M=0.11353554576635361, clip_val=255)\n",
       "    (40): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ScaledClippedLinearQuantization(M=0.0020281749311834574, clip_val=255)\n",
       "    (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): ScaledClippedLinearQuantization(M=0.07262083888053894, clip_val=255)\n",
       "    (44): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): ScaledClippedLinearQuantization(M=0.002086713444441557, clip_val=255)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): ScaledClippedLinearQuantization(M=0.0812416598200798, clip_val=255)\n",
       "    (48): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): ScaledClippedLinearQuantization(M=0.0020846007391810417, clip_val=255)\n",
       "    (50): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): ScaledClippedLinearQuantization(M=0.06995085626840591, clip_val=255)\n",
       "    (52): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): ScaledClippedLinearQuantization(M=0.002058308804407716, clip_val=255)\n",
       "    (54): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.6592e+00, -9.9213e-01, -1.8538e+00,  ..., -1.6261e+00,\n",
       "            1.5950e+00, -1.5785e+00],\n",
       "          [ 9.5142e-01,  1.1928e+00,  2.8836e+00,  ..., -7.2053e-01,\n",
       "           -7.4531e-02,  3.7682e-01],\n",
       "          [-3.1280e-01,  2.1150e+00, -1.7433e-01,  ...,  7.0935e-01,\n",
       "           -1.6594e+00,  2.1062e-01],\n",
       "          ...,\n",
       "          [ 3.3497e-02,  6.2067e-01,  6.3820e-01,  ...,  4.8994e-01,\n",
       "            2.4570e-01,  6.8401e-01],\n",
       "          [ 1.2982e-01, -1.5967e+00, -3.2398e-01,  ...,  6.2351e-01,\n",
       "            4.4640e-01, -7.6585e-01],\n",
       "          [ 3.9689e-01, -7.4495e-01, -1.2975e+00,  ...,  2.0225e+00,\n",
       "            2.7354e-01, -6.5239e-01]],\n",
       "\n",
       "         [[-3.9841e-02,  7.7911e-01,  1.1855e+00,  ..., -6.1800e-01,\n",
       "            5.0772e-01,  4.5049e-01],\n",
       "          [ 1.1449e+00,  1.2373e+00, -3.3193e-01,  ...,  8.9476e-01,\n",
       "            7.7673e-01,  2.1901e+00],\n",
       "          [-5.0232e-01, -3.0888e-01,  5.3224e-01,  ...,  7.4330e-01,\n",
       "            8.5863e-02, -9.9452e-01],\n",
       "          ...,\n",
       "          [ 3.3065e-01, -7.7455e-01, -1.1916e+00,  ..., -1.4206e+00,\n",
       "           -3.9144e-01,  2.2541e-01],\n",
       "          [-5.2741e-02, -2.9506e-01, -1.5071e+00,  ..., -2.8633e-01,\n",
       "           -1.4177e+00, -2.4069e-01],\n",
       "          [-1.0461e+00,  1.8122e+00, -6.0457e-01,  ..., -1.2118e-01,\n",
       "           -1.0473e+00, -2.4402e-01]],\n",
       "\n",
       "         [[ 4.6992e-01,  3.1880e-01, -1.8890e-01,  ...,  7.8889e-01,\n",
       "            6.2453e-01, -1.4221e+00],\n",
       "          [ 5.2393e-01, -1.5899e+00, -5.0964e-01,  ..., -1.5338e+00,\n",
       "            1.4490e+00,  3.4530e-01],\n",
       "          [ 1.5339e+00, -5.5805e-01,  2.7071e-01,  ..., -2.9869e-01,\n",
       "           -1.1464e+00, -8.9437e-02],\n",
       "          ...,\n",
       "          [ 6.8767e-01, -1.0157e-01,  2.2122e-01,  ..., -8.2075e-01,\n",
       "            2.2601e+00,  1.3912e+00],\n",
       "          [ 4.4290e-01,  8.0562e-01,  2.8213e-02,  ...,  1.0487e+00,\n",
       "            1.5510e+00, -2.6768e+00],\n",
       "          [ 6.9047e-01, -1.7992e+00,  9.2913e-01,  ..., -1.2556e+00,\n",
       "            2.3107e+00,  1.0157e+00]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innn = torch.randn(1,3,224,224)\n",
    "innn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f1a90929208>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 181, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 152, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "store_and_quantize() got an unexpected keyword argument 'update_float_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-50cbb7863535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_and_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_quant_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_float_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: store_and_quantize() got an unexpected keyword argument 'update_float_params'"
     ]
    }
   ],
   "source": [
    "# forward and backward\n",
    "for _ in range(100):\n",
    "    i, (inputs, target) = next(enumerate(val_loader))\n",
    "    if gpus is not None:\n",
    "        target = target.cuda(async=True)\n",
    "    input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "    target_var = Variable(target)\n",
    "    quantizer.store_and_quantize(get_quant_params=not training)\n",
    "    output = model(input_var)\n",
    "    loss = criterion(output, target_var)\n",
    "    print(loss.data)\n",
    "    #print(output.data)\n",
    "    #print(model.model[0][1].weight.data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    quantizer.restore_real_value()            \n",
    "    quantizer.backprop_quant_gradients()  \n",
    "    optimizer.step()\n",
    "    #print(model.model[0][1].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model[0][1].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): mobilenet_quant_devel(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (13): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (14): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (15): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (16): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (17): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (18): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (19): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (20): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (21): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (22): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (23): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (24): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (25): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (26): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ClippedLinearQuantization(num_bits=8, clip_val=6)\n",
       "      )\n",
       "      (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-98084f23ec96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# ensure that the worker exits on process exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "i, (inputs, target) = next(enumerate(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "if gpus is not None:\n",
    "    target = target.cuda(async=True)\n",
    "input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "target_var = Variable(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2973, -1.8580, -0.5975,  ...,  1.3263,  1.2916, -0.4290],\n",
       "          [-1.0215,  0.0675, -0.9857,  ...,  1.3524,  0.6812,  1.5958],\n",
       "          [ 0.0089,  0.3174, -0.6525,  ..., -1.4961,  0.6239, -0.9933],\n",
       "          ...,\n",
       "          [ 0.4883, -0.0547, -0.0974,  ...,  0.1020, -1.3684, -0.1422],\n",
       "          [ 0.2412,  0.2797, -0.2057,  ...,  0.1744, -1.4662,  0.7479],\n",
       "          [-1.8779,  1.6263, -1.6444,  ..., -0.3677,  0.8274,  0.8945]],\n",
       "\n",
       "         [[ 1.6880,  1.1349, -0.0469,  ..., -0.2843,  0.0886, -0.8200],\n",
       "          [-0.4533, -1.3211, -1.0611,  ..., -0.2144,  0.1663,  0.4091],\n",
       "          [ 1.4487, -0.3952,  0.4396,  ...,  0.1303, -0.1160,  0.3651],\n",
       "          ...,\n",
       "          [-0.1381,  0.4277, -0.0694,  ...,  0.1845,  0.2456, -1.9692],\n",
       "          [ 0.5448, -0.5923,  0.0232,  ..., -0.0760,  0.0342, -0.0051],\n",
       "          [ 0.8865, -1.2380,  1.3131,  ...,  0.7514,  0.3448,  0.0068]],\n",
       "\n",
       "         [[-1.6050, -0.4920,  0.0739,  ...,  0.7775,  0.2723, -0.1352],\n",
       "          [-0.0201, -0.0703,  0.2337,  ...,  0.6329,  0.6953, -0.0695],\n",
       "          [ 0.5104,  1.1874, -1.0925,  ..., -0.7873,  0.2734, -0.2888],\n",
       "          ...,\n",
       "          [ 1.7035, -0.2425, -0.0880,  ...,  1.0512, -1.6153, -0.0476],\n",
       "          [-0.6426, -0.2239, -0.2007,  ..., -1.3564, -0.8920, -1.0315],\n",
       "          [ 0.5917, -1.2916,  1.4357,  ...,  0.7011, -0.2120,  1.2757]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_var = Variable(torch.randn(1,3,32,32)).type(ttype)\n",
    "input_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "quantizer.store_and_quantize(training=False, get_quant_params=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1488, -2.6569,  5.3355,  1.3420, -1.5785,  0.7806,  4.1762,  1.8382,\n",
       "         -5.5024, -2.6558]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_var)\n",
    "quantizer.restore_real_value()\n",
    "output.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize(training=False, get_quant_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -30.6576,   21.5708,  117.1469,    6.1752, -111.7767,   37.9310,\n",
       "          133.3673,   48.5023, -167.9324,  -54.4490]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.type(ttype)\n",
    "output2 = quantizer.deployment_model(input_var)\n",
    "quantizer.restore_real_value()\n",
    "\n",
    "output2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0227, -0.0039,  0.0264,  0.1494,  0.0201,  0.0206,  0.0225,  0.0312,\n",
       "          0.0266,  0.0338]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.data/output2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(711.0366, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output.data-output2.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.2298e+00, -1.3090e+00, -1.2916e+00,  ..., -1.2027e+00,\n",
       "           -1.1830e+00, -1.1214e+00],\n",
       "          [-1.2267e+00, -1.2917e+00, -1.2363e+00,  ..., -1.1769e+00,\n",
       "           -1.2008e+00, -9.8096e-01],\n",
       "          [-1.1631e+00, -1.1992e+00, -1.2507e+00,  ..., -1.2457e+00,\n",
       "           -1.2062e+00, -1.0203e+00],\n",
       "          ...,\n",
       "          [-1.1890e+00, -1.0999e+00, -1.1926e+00,  ..., -1.2107e+00,\n",
       "           -1.2402e+00, -1.0884e+00],\n",
       "          [-1.1694e+00, -1.0818e+00, -1.2422e+00,  ..., -1.3756e+00,\n",
       "           -1.1469e+00, -1.1982e+00],\n",
       "          [-1.1578e+00, -1.2190e+00, -1.2013e+00,  ..., -1.2658e+00,\n",
       "           -1.2421e+00, -1.2590e+00]],\n",
       "\n",
       "         [[ 1.3645e-02, -9.3586e-02, -8.3263e-02,  ..., -1.5779e-01,\n",
       "            5.7411e-02, -6.6135e-02],\n",
       "          [ 9.7727e-02,  1.1886e-02,  5.1301e-02,  ..., -1.0439e-01,\n",
       "            6.8946e-02, -1.0653e-01],\n",
       "          [-5.2667e-02,  1.0581e-01,  4.1952e-02,  ..., -2.0856e-01,\n",
       "            2.7062e-01, -4.0397e-03],\n",
       "          ...,\n",
       "          [ 4.0488e-02,  1.9030e-01, -1.4606e-01,  ...,  2.5247e-01,\n",
       "           -1.0326e-01, -7.4413e-02],\n",
       "          [-1.8991e-02,  5.6961e-02, -1.3114e-01,  ...,  1.2357e-01,\n",
       "            6.6641e-02, -2.1107e-02],\n",
       "          [-4.9492e-02,  8.1401e-02,  3.4183e-02,  ...,  9.9227e-02,\n",
       "            6.3336e-03, -2.5506e-02]],\n",
       "\n",
       "         [[-1.4930e-01, -2.9414e-01, -2.7980e-01,  ..., -1.7580e-01,\n",
       "           -2.4306e-01, -1.4068e-01],\n",
       "          [-1.9619e-01, -2.0429e-01, -2.0143e-01,  ..., -2.7325e-01,\n",
       "           -1.2954e-01, -1.8017e-02],\n",
       "          [-2.7843e-02,  5.8932e-02, -1.3408e-01,  ..., -2.3310e-01,\n",
       "            4.0310e-02, -3.4693e-03],\n",
       "          ...,\n",
       "          [-8.6351e-02,  5.3724e-03, -9.3770e-02,  ..., -5.4428e-02,\n",
       "           -1.5197e-01, -1.8827e-01],\n",
       "          [-1.4093e-02, -3.6462e-02, -1.0330e-01,  ..., -7.0195e-02,\n",
       "           -4.5040e-02, -1.3007e-01],\n",
       "          [-7.3680e-02, -1.0060e-01, -5.2211e-02,  ..., -1.2701e-01,\n",
       "           -1.0911e-01, -1.7992e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.6009e-01,  2.6931e-01,  2.6136e-01,  ...,  2.5117e-01,\n",
       "            2.6668e-01,  2.5369e-01],\n",
       "          [ 2.4158e-01,  2.6358e-01,  2.7304e-01,  ...,  2.5273e-01,\n",
       "            2.3883e-01,  2.4989e-01],\n",
       "          [ 2.5682e-01,  2.4631e-01,  2.5767e-01,  ...,  2.4888e-01,\n",
       "            2.2210e-01,  2.5733e-01],\n",
       "          ...,\n",
       "          [ 2.4252e-01,  2.5167e-01,  2.5067e-01,  ...,  2.1243e-01,\n",
       "            2.7385e-01,  2.7531e-01],\n",
       "          [ 2.4969e-01,  2.6487e-01,  2.3988e-01,  ...,  2.2648e-01,\n",
       "            2.3806e-01,  2.6165e-01],\n",
       "          [ 2.4563e-01,  2.6747e-01,  2.2758e-01,  ...,  2.3912e-01,\n",
       "            2.5992e-01,  2.4890e-01]],\n",
       "\n",
       "         [[ 7.7269e-01,  1.5988e+00,  2.4018e+00,  ...,  7.8589e-01,\n",
       "            1.0539e+00,  3.3658e-01],\n",
       "          [ 5.5828e-01,  9.5779e-01,  1.8177e+00,  ...,  6.1384e-01,\n",
       "            1.5493e+00,  1.0191e-01],\n",
       "          [-4.1264e-01, -4.7407e-01,  2.6063e-01,  ...,  2.2764e-01,\n",
       "            1.2261e+00, -1.1016e+00],\n",
       "          ...,\n",
       "          [ 2.0218e-01,  6.2887e-01,  5.1546e-01,  ...,  1.3798e+00,\n",
       "            9.1003e-01,  9.9458e-01],\n",
       "          [ 3.0648e-01,  7.4250e-01,  7.8442e-01,  ...,  1.1953e+00,\n",
       "            1.8408e+00,  1.2555e+00],\n",
       "          [ 1.4551e-01,  5.0154e-01,  6.0392e-01,  ...,  5.9098e-01,\n",
       "            4.3648e-01,  4.0840e-01]],\n",
       "\n",
       "         [[-5.9005e-01,  1.3004e-01,  1.0291e-01,  ...,  1.2661e-01,\n",
       "            3.2128e-01,  3.8054e-01],\n",
       "          [-1.0937e+00, -2.3148e-01,  3.3709e-01,  ...,  9.6593e-01,\n",
       "            1.1545e+00,  1.1332e+00],\n",
       "          [-9.3223e-01, -2.4278e-01,  4.5589e-01,  ...,  2.8804e+00,\n",
       "            2.4562e+00,  1.6902e+00],\n",
       "          ...,\n",
       "          [-4.8425e-01, -1.1442e+00, -8.5513e-01,  ..., -1.5815e-01,\n",
       "           -1.0104e+00, -1.3756e-01],\n",
       "          [-1.0418e+00, -1.0104e+00, -4.4722e-01,  ..., -1.1064e-01,\n",
       "           -8.1580e-01, -3.3329e-01],\n",
       "          [-4.0951e-02, -4.4816e-01, -1.2408e-01,  ...,  6.5569e-01,\n",
       "            3.4113e-01,  6.8943e-01]]]],\n",
       "       device='cuda:0', grad_fn=<CudnnBatchNormBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(model.module.model[0][0],model.module.model[0][1] )#,model.module.model[1])\n",
    "output1 = net(input_var)\n",
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_md = quantizer.deployment_model.model\n",
    "net2= nn.Sequential( ptr_md[0])#,ptr_md[1])#,ptr_md[2],ptr_md[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.8973e+02, -3.0840e+02, -3.0429e+02,  ..., -2.8336e+02,\n",
       "           -2.7872e+02, -2.6420e+02],\n",
       "          [-2.8900e+02, -3.0432e+02, -2.9126e+02,  ..., -2.7727e+02,\n",
       "           -2.8290e+02, -2.3111e+02],\n",
       "          [-2.7401e+02, -2.8253e+02, -2.9466e+02,  ..., -2.9349e+02,\n",
       "           -2.8418e+02, -2.4039e+02],\n",
       "          ...,\n",
       "          [-2.8012e+02, -2.5914e+02, -2.8097e+02,  ..., -2.8525e+02,\n",
       "           -2.9218e+02, -2.5641e+02],\n",
       "          [-2.7550e+02, -2.5487e+02, -2.9266e+02,  ..., -3.2409e+02,\n",
       "           -2.7020e+02, -2.8228e+02],\n",
       "          [-2.7278e+02, -2.8719e+02, -2.8301e+02,  ..., -2.9821e+02,\n",
       "           -2.9263e+02, -2.9661e+02]],\n",
       "\n",
       "         [[ 3.2147e+00, -2.2048e+01, -1.9617e+01,  ..., -3.7174e+01,\n",
       "            1.3526e+01, -1.5581e+01],\n",
       "          [ 2.3024e+01,  2.8003e+00,  1.2086e+01,  ..., -2.4594e+01,\n",
       "            1.6244e+01, -2.5098e+01],\n",
       "          [-1.2408e+01,  2.4930e+01,  9.8837e+00,  ..., -4.9137e+01,\n",
       "            6.3758e+01, -9.5175e-01],\n",
       "          ...,\n",
       "          [ 9.5388e+00,  4.4833e+01, -3.4411e+01,  ...,  5.9480e+01,\n",
       "           -2.4328e+01, -1.7531e+01],\n",
       "          [-4.4742e+00,  1.3420e+01, -3.0896e+01,  ...,  2.9112e+01,\n",
       "            1.5700e+01, -4.9728e+00],\n",
       "          [-1.1660e+01,  1.9178e+01,  8.0534e+00,  ...,  2.3377e+01,\n",
       "            1.4922e+00, -6.0091e+00]],\n",
       "\n",
       "         [[-3.5174e+01, -6.9300e+01, -6.5920e+01,  ..., -4.1419e+01,\n",
       "           -5.7264e+01, -3.3143e+01],\n",
       "          [-4.6221e+01, -4.8129e+01, -4.7456e+01,  ..., -6.4376e+01,\n",
       "           -3.0519e+01, -4.2447e+00],\n",
       "          [-6.5596e+00,  1.3884e+01, -3.1588e+01,  ..., -5.4917e+01,\n",
       "            9.4970e+00, -8.1735e-01],\n",
       "          ...,\n",
       "          [-2.0344e+01,  1.2657e+00, -2.2092e+01,  ..., -1.2823e+01,\n",
       "           -3.5803e+01, -4.4356e+01],\n",
       "          [-3.3203e+00, -8.5903e+00, -2.4336e+01,  ..., -1.6538e+01,\n",
       "           -1.0611e+01, -3.0645e+01],\n",
       "          [-1.7359e+01, -2.3700e+01, -1.2301e+01,  ..., -2.9923e+01,\n",
       "           -2.5705e+01, -4.2389e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.1275e+01,  6.3449e+01,  6.1574e+01,  ...,  5.9176e+01,\n",
       "            6.2830e+01,  5.9768e+01],\n",
       "          [ 5.6915e+01,  6.2098e+01,  6.4328e+01,  ...,  5.9543e+01,\n",
       "            5.6267e+01,  5.8874e+01],\n",
       "          [ 6.0505e+01,  5.8030e+01,  6.0706e+01,  ...,  5.8636e+01,\n",
       "            5.2327e+01,  6.0627e+01],\n",
       "          ...,\n",
       "          [ 5.7136e+01,  5.9293e+01,  5.9056e+01,  ...,  5.0048e+01,\n",
       "            6.4518e+01,  6.4863e+01],\n",
       "          [ 5.8827e+01,  6.2403e+01,  5.6514e+01,  ...,  5.3358e+01,\n",
       "            5.6087e+01,  6.1643e+01],\n",
       "          [ 5.7869e+01,  6.3014e+01,  5.3618e+01,  ...,  5.6336e+01,\n",
       "            6.1236e+01,  5.8640e+01]],\n",
       "\n",
       "         [[ 1.8204e+02,  3.7666e+02,  5.6585e+02,  ...,  1.8515e+02,\n",
       "            2.4830e+02,  7.9298e+01],\n",
       "          [ 1.3153e+02,  2.2565e+02,  4.2824e+02,  ...,  1.4462e+02,\n",
       "            3.6502e+02,  2.4009e+01],\n",
       "          [-9.7216e+01, -1.1169e+02,  6.1404e+01,  ...,  5.3632e+01,\n",
       "            2.8886e+02, -2.5953e+02],\n",
       "          ...,\n",
       "          [ 4.7632e+01,  1.4816e+02,  1.2144e+02,  ...,  3.2507e+02,\n",
       "            2.1440e+02,  2.3432e+02],\n",
       "          [ 7.2205e+01,  1.7493e+02,  1.8481e+02,  ...,  2.8160e+02,\n",
       "            4.3369e+02,  2.9578e+02],\n",
       "          [ 3.4282e+01,  1.1816e+02,  1.4228e+02,  ...,  1.3923e+02,\n",
       "            1.0283e+02,  9.6217e+01]],\n",
       "\n",
       "         [[-1.3901e+02,  3.0637e+01,  2.4246e+01,  ...,  2.9828e+01,\n",
       "            7.5693e+01,  8.9654e+01],\n",
       "          [-2.5767e+02, -5.4536e+01,  7.9416e+01,  ...,  2.2757e+02,\n",
       "            2.7200e+02,  2.6698e+02],\n",
       "          [-2.1963e+02, -5.7197e+01,  1.0741e+02,  ...,  6.7862e+02,\n",
       "            5.7868e+02,  3.9822e+02],\n",
       "          ...,\n",
       "          [-1.1409e+02, -2.6957e+02, -2.0147e+02,  ..., -3.7260e+01,\n",
       "           -2.3806e+02, -3.2408e+01],\n",
       "          [-2.4544e+02, -2.3804e+02, -1.0536e+02,  ..., -2.6067e+01,\n",
       "           -1.9220e+02, -7.8522e+01],\n",
       "          [-9.6480e+00, -1.0559e+02, -2.9234e+01,  ...,  1.5448e+02,\n",
       "            8.0370e+01,  1.6243e+02]]]],\n",
       "       device='cuda:0', grad_fn=<CudnnConvolutionBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = net2(input_var)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6668,  0.8130,  0.5452,  ...,  2.5753,  2.7961,  2.4325],\n",
       "        [ 1.5222,  2.2391,  3.4420,  ...,  1.3467,  1.0082,  1.3457],\n",
       "        [ 0.7315, -0.0835, -0.4004,  ...,  1.5123,  1.1432,  0.5446],\n",
       "        ...,\n",
       "        [ 1.2904,  1.1056,  0.8274,  ...,  0.3274,  1.6566,  0.8545],\n",
       "        [ 0.9708,  0.5848,  1.1020,  ...,  0.4696,  1.5043,  1.8956],\n",
       "        [ 1.3714,  1.2828,  0.9303,  ...,  1.4516,  0.2574,  1.8278]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1085, -0.2053, -0.0891],\n",
       "         [ 0.1201,  0.1860,  0.1317],\n",
       "         [ 0.0116,  0.0116,  0.0271]],\n",
       "\n",
       "        [[-0.2867, -0.4184, -0.2053],\n",
       "         [ 0.1627,  0.2944,  0.1976],\n",
       "         [ 0.0349,  0.0465, -0.0155]],\n",
       "\n",
       "        [[-0.0542, -0.1356, -0.0310],\n",
       "         [ 0.0930,  0.1278,  0.0659],\n",
       "         [ 0.0271, -0.0155, -0.0271]]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3596, -0.6925, -0.2930],\n",
       "         [ 0.3995,  0.6259,  0.4395],\n",
       "         [ 0.0400,  0.0400,  0.0932]],\n",
       "\n",
       "        [[-0.9588, -1.3983, -0.6925],\n",
       "         [ 0.5460,  0.9855,  0.6659],\n",
       "         [ 0.1199,  0.1598, -0.0533]],\n",
       "\n",
       "        [[-0.1864, -0.4528, -0.1065],\n",
       "         [ 0.3063,  0.4261,  0.2264],\n",
       "         [ 0.0932, -0.0533, -0.0932]]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model[0][0].weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9430.5840, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output1.data - output2.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2985, -0.3297, -0.2971,  ..., -0.4190, -0.4657, -0.4281],\n",
       "          [-0.4323, -0.4338, -0.4660,  ..., -0.3846, -0.4436, -0.4710],\n",
       "          [-0.3825, -0.3861, -0.3902,  ..., -0.4035, -0.4159, -0.4612],\n",
       "          ...,\n",
       "          [-0.3960, -0.4204, -0.4495,  ..., -0.4364, -0.3823, -0.4304],\n",
       "          [-0.3646, -0.3705, -0.4174,  ..., -0.4267, -0.4123, -0.3685],\n",
       "          [-0.3844, -0.3780, -0.3610,  ..., -0.3673, -0.3633, -0.3704]],\n",
       "\n",
       "         [[-0.3995, -0.3628, -0.2147,  ..., -1.1500, -1.3742, -1.3333],\n",
       "          [-0.6543, -0.8469, -1.1253,  ..., -0.8415, -0.9610, -1.6104],\n",
       "          [-0.7949, -0.8514, -0.9529,  ..., -0.9631, -0.9862, -1.2883],\n",
       "          ...,\n",
       "          [-0.7851, -0.9943, -1.1852,  ..., -1.0356, -1.1369, -1.0480],\n",
       "          [-0.8469, -0.9498, -1.1165,  ..., -1.0756, -1.2138, -1.0943],\n",
       "          [-0.8307, -0.7719, -0.6681,  ..., -0.9480, -0.8255, -0.7907]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2380, -0.2372, -0.3571,  ..., -0.2342, -0.2336, -0.2234],\n",
       "          [-0.2054, -0.2176, -0.3353,  ..., -0.2996, -0.2949, -0.2846],\n",
       "          [-0.1845, -0.2031, -0.3197,  ..., -0.4017, -0.3796, -0.3455],\n",
       "          ...,\n",
       "          [-0.3395, -0.3385, -0.3391,  ..., -0.2992, -0.2882, -0.2872],\n",
       "          [-0.3444, -0.3437, -0.3383,  ..., -0.2679, -0.2623, -0.2629],\n",
       "          [-0.3358, -0.3356, -0.3318,  ..., -0.2523, -0.2533, -0.2518]],\n",
       "\n",
       "         [[-0.3828,  0.0374,  0.0106,  ...,  0.1503,  0.1733,  0.1664],\n",
       "          [-0.2459,  0.3082,  0.1448,  ..., -0.1982, -0.1454, -0.1307],\n",
       "          [-0.1252,  0.4252,  0.2484,  ..., -1.2257, -1.1075, -0.9670],\n",
       "          ...,\n",
       "          [-0.5629, -0.5105, -0.4649,  ..., -0.3677, -0.2916, -0.2247],\n",
       "          [-0.5568, -0.5349, -0.5073,  ..., -0.1558, -0.1605, -0.0816],\n",
       "          [-0.5439, -0.4651, -0.4508,  ..., -0.0514, -0.0843, -0.0840]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2189, -0.2253, -0.1837,  ..., -0.3683, -0.3852, -0.3540],\n",
       "          [-0.1769, -0.2015, -0.1984,  ..., -0.3258, -0.3106, -0.2858],\n",
       "          [-0.2517, -0.2930, -0.1854,  ..., -0.2929, -0.3700, -0.3564],\n",
       "          ...,\n",
       "          [-0.4953, -0.4909, -0.4664,  ..., -0.1944, -0.1949, -0.2029],\n",
       "          [-0.4593, -0.4838, -0.4514,  ..., -0.1942, -0.1934, -0.1959],\n",
       "          [-0.4801, -0.4214, -0.3664,  ..., -0.1895, -0.1928, -0.1942]],\n",
       "\n",
       "         [[-0.1195,  0.1274,  0.2780,  ..., -0.5975, -0.6878, -0.6424],\n",
       "          [-0.0601,  0.5447,  0.2666,  ..., -0.3930, -0.5264, -0.0122],\n",
       "          [-0.2511,  0.2140,  0.4298,  ..., -0.0464, -0.4539, -0.4418],\n",
       "          ...,\n",
       "          [-0.9026, -1.2045, -1.3517,  ...,  0.4517,  0.4260,  0.4005],\n",
       "          [-0.7699, -1.1043, -1.2150,  ...,  0.4663,  0.4430,  0.4430],\n",
       "          [-0.8815, -1.1492, -1.0867,  ...,  0.5064,  0.4726,  0.4484]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3891, -0.2822, -0.2715,  ..., -0.4092, -0.4461, -0.4759],\n",
       "          [-0.3760, -0.2610, -0.2316,  ..., -0.4135, -0.4208, -0.4055],\n",
       "          [-0.3128, -0.2774, -0.2310,  ..., -0.4384, -0.4015, -0.4073],\n",
       "          ...,\n",
       "          [-0.2400, -0.2598, -0.2610,  ..., -0.3478, -0.3701, -0.3571],\n",
       "          [-0.2440, -0.2642, -0.2642,  ..., -0.3143, -0.3378, -0.3634],\n",
       "          [-0.2480, -0.2489, -0.2527,  ..., -0.3248, -0.3240, -0.3325]],\n",
       "\n",
       "         [[-0.5445, -0.5083,  0.1338,  ..., -0.7011, -1.1041, -1.0485],\n",
       "          [-0.7784, -0.2736,  0.2615,  ..., -0.5770, -1.2865, -0.6724],\n",
       "          [-0.4350,  0.0365,  0.3340,  ..., -0.7457, -1.0920, -0.6252],\n",
       "          ...,\n",
       "          [-0.2782,  0.0245, -0.0248,  ..., -0.6087, -0.7991, -0.7929],\n",
       "          [-0.3170, -0.0578, -0.0609,  ..., -0.4377, -0.5591, -0.7303],\n",
       "          [-0.3108,  0.0492,  0.0717,  ..., -0.5224, -0.4622, -0.4739]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.5862, -0.5707, -0.5268,  ..., -0.3219, -0.3154, -0.3117],\n",
       "          [-0.6039, -0.6030, -0.5710,  ..., -0.2930, -0.2937, -0.3163],\n",
       "          [-0.5539, -0.5975, -0.5952,  ..., -0.2721, -0.2629, -0.2868],\n",
       "          ...,\n",
       "          [-0.2466, -0.2643, -0.2819,  ..., -0.3185, -0.3239, -0.2634],\n",
       "          [-0.2322, -0.2719, -0.3040,  ..., -0.2940, -0.2870, -0.2704],\n",
       "          [-0.2988, -0.2982, -0.2772,  ..., -0.2657, -0.2759, -0.2464]],\n",
       "\n",
       "         [[-1.4044, -2.1832, -1.7710,  ..., -0.4852, -0.4524, -0.3524],\n",
       "          [-1.4719, -2.4704, -2.2743,  ..., -0.2724, -0.3692, -0.3233],\n",
       "          [-1.2783, -2.2367, -2.4536,  ..., -0.1583, -0.1073, -0.1712],\n",
       "          ...,\n",
       "          [-0.1642,  0.1229,  0.0438,  ..., -0.0488, -0.3502, -0.2637],\n",
       "          [-0.2106,  0.1288, -0.1659,  ..., -0.2118, -0.1213, -0.0507],\n",
       "          [-0.4123, -0.1540, -0.0578,  ..., -0.1334, -0.0087,  0.0732]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]],\n",
       "\n",
       "\n",
       "        [[[-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          ...,\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
       "          [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045]],\n",
       "\n",
       "         [[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          ...,\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
       "          [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]],\n",
       "\n",
       "         [[-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          ...,\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171],\n",
       "          [-0.0171, -0.0171, -0.0171,  ..., -0.0171, -0.0171, -0.0171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3203, -0.3582, -0.3617,  ..., -0.2470, -0.2518, -0.2551],\n",
       "          [-0.2990, -0.3568, -0.3618,  ..., -0.2503, -0.2438, -0.2445],\n",
       "          [-0.3275, -0.3487, -0.3342,  ..., -0.2506, -0.2360, -0.2356],\n",
       "          ...,\n",
       "          [-0.4716, -0.4534, -0.4413,  ..., -0.5180, -0.5131, -0.5057],\n",
       "          [-0.4706, -0.4602, -0.4467,  ..., -0.5196, -0.5143, -0.5080],\n",
       "          [-0.4636, -0.4561, -0.4438,  ..., -0.5171, -0.5253, -0.5153]],\n",
       "\n",
       "         [[-0.5390, -0.5656, -0.7333,  ...,  0.0511, -0.0168, -0.0277],\n",
       "          [-0.4595, -0.5864, -0.8356,  ...,  0.0127,  0.0278,  0.0778],\n",
       "          [-0.5190, -0.5838, -0.6005,  ..., -0.0273,  0.0642,  0.1318],\n",
       "          ...,\n",
       "          [-1.1563, -1.5301, -1.4282,  ..., -1.9661, -1.9128, -1.8678],\n",
       "          [-1.1563, -1.5664, -1.4725,  ..., -1.9714, -1.9219, -1.8793],\n",
       "          [-1.1298, -1.5281, -1.4504,  ..., -1.9227, -1.9969, -1.9552]],\n",
       "\n",
       "         [[-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          ...,\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
       "          [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031]]]],\n",
       "       device='cuda:0', grad_fn=<CudnnConvolutionBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0](input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bias_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-71e778bd7a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bias_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].bias.data = bias_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.2268e-03, -5.6052e-45, -1.3223e-01,  9.5181e-02,  5.6052e-45,\n",
       "         2.7001e-02,  1.2026e-02,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "        -5.6052e-45, -1.5036e-01, -5.6052e-45, -5.6052e-45, -5.6052e-45,\n",
       "         5.6052e-45, -5.6052e-45,  5.6052e-45, -1.6848e-01,  3.7778e-02,\n",
       "         5.6052e-45,  5.6052e-45,  5.6052e-45, -5.6052e-45, -5.6052e-45,\n",
       "         5.6052e-45,  8.0595e-03, -5.6052e-45, -1.1367e-01, -1.0080e-01,\n",
       "         1.2784e-01,  4.8255e-03], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_tensor = model.module.model[0][1].running_mean.clone()\n",
    "mu_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1460e+00, -1.3018e-05,  9.6313e-01,  1.1933e+00, -3.9374e-04,\n",
       "         1.1778e+00, -2.0396e-01, -2.6259e-04,  5.2023e-07, -2.0778e-06,\n",
       "        -2.6015e-05,  1.1760e+00, -3.2594e-06, -1.3979e-05, -1.2620e-07,\n",
       "         5.4683e-07, -1.1149e-05, -2.4389e-05,  8.3711e-01,  1.0547e+00,\n",
       "        -6.0049e-05,  2.1387e-07,  9.6728e-07, -1.0179e-04, -2.4040e-05,\n",
       "         4.7633e-06,  1.2126e+00, -1.0693e-05,  1.0251e+00,  6.2181e-01,\n",
       "         8.9824e-01,  1.1250e+00], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_tensor = mu_tensor.mul(-1)*gamma_over_sigma+beta_tensor\n",
    "bias_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1506e+00, -1.3018e-05,  9.5048e-01,  1.2050e+00, -3.9374e-04,\n",
       "         1.1813e+00, -2.0231e-01, -2.6259e-04,  5.2023e-07, -2.0778e-06,\n",
       "        -2.6015e-05,  1.1557e+00, -3.2594e-06, -1.3979e-05, -1.2620e-07,\n",
       "         5.4683e-07, -1.1149e-05, -2.4389e-05,  8.1169e-01,  1.0597e+00,\n",
       "        -6.0049e-05,  2.1387e-07,  9.6728e-07, -1.0179e-04, -2.4040e-05,\n",
       "         4.7633e-06,  1.2149e+00, -1.0693e-05,  1.0384e+00,  6.1218e-01,\n",
       "         9.1894e-01,  1.1263e+00], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_tensor = model.module.model[0][1].bias.data.clone()\n",
    "beta_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_tensor = model.module.model[0][1].weight.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_tensor = model.module.model[0][1].running_var.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0266,  0.1864,  0.1065],\n",
       "          [-0.1332,  0.1332,  0.2397],\n",
       "          [ 0.2264,  0.5194,  0.5726]],\n",
       "\n",
       "         [[-0.0799,  0.1465,  0.0932],\n",
       "          [-0.1598,  0.0932,  0.1998],\n",
       "          [ 0.2264,  0.4927,  0.4927]],\n",
       "\n",
       "         [[-0.1598,  0.1199,  0.0799],\n",
       "          [-0.1465,  0.1731,  0.3063],\n",
       "          [ 0.0533,  0.3862,  0.4528]]],\n",
       "\n",
       "\n",
       "        [[[-0.0133, -0.0133, -0.0133],\n",
       "          [-0.0133, -0.0133, -0.0133],\n",
       "          [-0.0133, -0.0133, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0133,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.4528, -0.5859, -0.4794],\n",
       "          [-0.7058, -0.7857, -0.5460],\n",
       "          [-0.6259, -0.6925, -0.4794]],\n",
       "\n",
       "         [[ 0.0932,  0.0266,  0.0133],\n",
       "          [ 0.0400,  0.0000,  0.0799],\n",
       "          [ 0.0000, -0.0266,  0.0000]],\n",
       "\n",
       "         [[ 0.4661,  0.5593,  0.4661],\n",
       "          [ 0.6126,  0.7058,  0.6659],\n",
       "          [ 0.5060,  0.5726,  0.4395]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4261,  0.4128,  0.3063],\n",
       "          [ 0.4128,  0.3196,  0.2397],\n",
       "          [ 0.3462,  0.3462,  0.3063]],\n",
       "\n",
       "         [[-0.7857, -0.9855, -0.9455],\n",
       "          [-0.9721, -1.2918, -1.2385],\n",
       "          [-0.7857, -1.0387, -1.0787]],\n",
       "\n",
       "         [[ 0.6259,  0.8656,  0.9056],\n",
       "          [ 0.6259,  0.7591,  0.7724],\n",
       "          [ 0.6259,  0.7990,  0.6792]]],\n",
       "\n",
       "\n",
       "        [[[-0.5460, -0.5060, -0.3995],\n",
       "          [-0.7724, -0.7591, -0.4927],\n",
       "          [-0.6925, -0.7058, -0.4528]],\n",
       "\n",
       "         [[ 0.8789,  0.9988,  0.7458],\n",
       "          [ 1.1053,  1.2385,  1.0387],\n",
       "          [ 1.0387,  1.1053,  0.9455]],\n",
       "\n",
       "         [[-0.3196, -0.3995, -0.3729],\n",
       "          [-0.4261, -0.5859, -0.4794],\n",
       "          [-0.2663, -0.5060, -0.4261]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3596, -0.6925, -0.2930],\n",
       "          [ 0.3995,  0.6259,  0.4395],\n",
       "          [ 0.0400,  0.0400,  0.0932]],\n",
       "\n",
       "         [[-0.9588, -1.3983, -0.6925],\n",
       "          [ 0.5460,  0.9855,  0.6659],\n",
       "          [ 0.1199,  0.1598, -0.0533]],\n",
       "\n",
       "         [[-0.1864, -0.4528, -0.1065],\n",
       "          [ 0.3063,  0.4261,  0.2264],\n",
       "          [ 0.0932, -0.0533, -0.0932]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0133,  0.3596,  0.4661],\n",
       "          [-0.1598, -0.5993,  0.5060],\n",
       "          [ 0.1199, -0.4794, -0.2264]],\n",
       "\n",
       "         [[-0.1598,  0.4661,  1.0920],\n",
       "          [-0.3862, -1.0920,  0.7324],\n",
       "          [ 0.4794, -0.6126, -0.4395]],\n",
       "\n",
       "         [[ 0.0932,  0.2663,  0.2930],\n",
       "          [-0.0799, -0.3196,  0.4794],\n",
       "          [ 0.0799, -0.2397, -0.1065]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5327,  0.8523,  0.6925],\n",
       "          [ 0.9988,  1.3051,  0.9588],\n",
       "          [ 0.8922,  1.0254,  0.7324]],\n",
       "\n",
       "         [[-0.2930, -0.3196, -0.0799],\n",
       "          [-0.3596, -0.3596, -0.2264],\n",
       "          [-0.2131, -0.2930, -0.1598]],\n",
       "\n",
       "         [[-0.2264, -0.4927, -0.3462],\n",
       "          [-0.6925, -0.8523, -0.6126],\n",
       "          [-0.6259, -0.7724, -0.5593]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3596,  0.8523, -0.1332],\n",
       "          [-0.7191,  1.0787, -0.5726],\n",
       "          [ 0.0133,  0.3063, -0.3329]],\n",
       "\n",
       "         [[-0.7724,  1.3051, -0.2797],\n",
       "          [-0.8123,  1.7845, -1.0121],\n",
       "          [-0.0666,  0.5194, -0.6792]],\n",
       "\n",
       "         [[ 0.0000,  0.3995, -0.0799],\n",
       "          [-0.2930,  0.6392, -0.4261],\n",
       "          [ 0.0932,  0.1465,  0.0133]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0932,  0.0666,  0.1065],\n",
       "          [ 0.0799, -0.4128, -0.1065],\n",
       "          [-0.0000, -0.8257, -0.2264]],\n",
       "\n",
       "         [[ 0.0666, -0.0133,  0.0533],\n",
       "          [-0.0400, -0.5460, -0.1598],\n",
       "          [-0.1998, -0.9855, -0.3329]],\n",
       "\n",
       "         [[ 0.1199,  0.0932,  0.1998],\n",
       "          [ 0.1864, -0.1864,  0.1731],\n",
       "          [ 0.0799, -0.5327,  0.0266]]],\n",
       "\n",
       "\n",
       "        [[[-0.0133,  0.0666, -0.0133],\n",
       "          [-0.0000,  0.2264,  0.1199],\n",
       "          [ 0.1065,  0.3995,  0.2530]],\n",
       "\n",
       "         [[-0.0932,  0.0400, -0.0000],\n",
       "          [-0.1199,  0.1864,  0.0799],\n",
       "          [-0.0133,  0.3729,  0.2264]],\n",
       "\n",
       "         [[-0.0932, -0.0533, -0.0932],\n",
       "          [-0.1598,  0.0533, -0.0533],\n",
       "          [-0.0400,  0.2264,  0.0932]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0799, -0.1199, -0.0666],\n",
       "          [-0.1998, -0.3729, -0.2930],\n",
       "          [-0.1864, -0.3862, -0.2397]],\n",
       "\n",
       "         [[ 0.0666, -0.1065,  0.0000],\n",
       "          [-0.1465, -0.3063, -0.1598],\n",
       "          [-0.1465, -0.2930, -0.0666]],\n",
       "\n",
       "         [[ 0.1864, -0.0533,  0.0133],\n",
       "          [-0.1332, -0.3462, -0.2264],\n",
       "          [-0.2131, -0.4128, -0.2131]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0799,  0.0932,  0.1065],\n",
       "          [-0.6525, -0.9189, -0.5060],\n",
       "          [ 0.2797,  0.5993,  0.4128]],\n",
       "\n",
       "         [[ 0.2663,  0.1864,  0.1199],\n",
       "          [-1.0121, -1.6114, -0.9988],\n",
       "          [ 0.7591,  0.9588,  0.6392]],\n",
       "\n",
       "         [[ 0.0799,  0.1598,  0.0533],\n",
       "          [-0.3462, -0.5593, -0.1065],\n",
       "          [ 0.1332,  0.3063,  0.2397]]],\n",
       "\n",
       "\n",
       "        [[[-0.3596, -0.3995, -0.4661],\n",
       "          [-0.4261, -0.5060, -0.6259],\n",
       "          [-0.3596, -0.3462, -0.5060]],\n",
       "\n",
       "         [[ 0.4528,  0.5593,  0.4395],\n",
       "          [ 0.4927,  0.6126,  0.4794],\n",
       "          [ 0.4528,  0.6925,  0.5859]],\n",
       "\n",
       "         [[ 0.0266,  0.0533, -0.0932],\n",
       "          [ 0.1065,  0.1199, -0.0533],\n",
       "          [ 0.0799,  0.2264,  0.0266]]],\n",
       "\n",
       "\n",
       "        [[[-0.1065, -0.3729, -0.1332],\n",
       "          [-0.0932, -0.4794, -0.2930],\n",
       "          [ 0.0799, -0.1199, -0.0000]],\n",
       "\n",
       "         [[-0.0400, -0.4128, -0.1998],\n",
       "          [-0.0799, -0.5859, -0.3862],\n",
       "          [ 0.1199, -0.1864, -0.0666]],\n",
       "\n",
       "         [[ 0.1065, -0.1998, -0.0533],\n",
       "          [ 0.0666, -0.3729, -0.2397],\n",
       "          [ 0.2397, -0.0533,  0.0400]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0799,  0.0400, -0.1199],\n",
       "          [ 0.3995, -0.0932, -0.6925],\n",
       "          [ 0.4927,  0.1199, -0.5460]],\n",
       "\n",
       "         [[ 0.2264,  0.1199, -0.3196],\n",
       "          [ 0.7324, -0.1998, -1.1985],\n",
       "          [ 0.6792,  0.1998, -0.9056]],\n",
       "\n",
       "         [[-0.0799, -0.0133, -0.0266],\n",
       "          [ 0.1864, -0.0666, -0.4261],\n",
       "          [ 0.2264,  0.1065, -0.3729]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0133,  0.0133,  0.0133],\n",
       "          [ 0.0000,  0.0133,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0133]],\n",
       "\n",
       "         [[ 0.0000,  0.0133,  0.0133],\n",
       "          [ 0.0000,  0.0133,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0533,  0.0266,  0.0266],\n",
       "          [ 0.0799, -0.0533, -0.0400],\n",
       "          [ 0.0400, -0.1465, -0.1731]],\n",
       "\n",
       "         [[ 0.0533,  0.0266,  0.0266],\n",
       "          [ 0.0533, -0.0666, -0.0666],\n",
       "          [-0.0000, -0.1731, -0.1731]],\n",
       "\n",
       "         [[-0.0533, -0.0666, -0.0799],\n",
       "          [-0.0400, -0.0932, -0.1065],\n",
       "          [-0.0533, -0.1598, -0.1598]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1864,  0.1465,  0.0932],\n",
       "          [ 0.0932,  0.0666,  0.0666],\n",
       "          [ 0.0932, -0.0133,  0.1465]],\n",
       "\n",
       "         [[ 0.0133, -0.0000,  0.0799],\n",
       "          [-0.2264, -0.2264, -0.0533],\n",
       "          [-0.2530, -0.3329,  0.0133]],\n",
       "\n",
       "         [[-0.3995, -0.3196, -0.1065],\n",
       "          [-0.6259, -0.5593, -0.2264],\n",
       "          [-0.4661, -0.4661, -0.0533]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = model.module.model[0][0].weight.data.clone()\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0,v1,v2,v3 = weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = model.module.model[0][1].eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_tensor = var_tensor.add(eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST = len(quantizer_load.param_to_quantize) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Same =:  3\n",
      "Conv2d Same =:  7\n",
      "Conv2d Same =:  11\n",
      "Conv2d Same =:  15\n",
      "Conv2d Same =:  19\n",
      "Conv2d Same =:  23\n",
      "Conv2d Same =:  27\n",
      "Conv2d Same =:  31\n",
      "Conv2d Same =:  35\n",
      "Conv2d Same =:  39\n",
      "Conv2d Same =:  43\n",
      "Conv2d Same =:  47\n",
      "Conv2d Same =:  51\n",
      "Conv2d Same =:  55\n",
      "Conv2d Same =:  59\n",
      "Conv2d Same =:  63\n",
      "Conv2d Same =:  67\n",
      "Conv2d Same =:  71\n",
      "Conv2d Same =:  75\n",
      "Conv2d Same =:  79\n",
      "Conv2d Same =:  83\n",
      "Conv2d Same =:  87\n",
      "Conv2d Same =:  91\n",
      "Conv2d Same =:  95\n",
      "Conv2d Same =:  99\n",
      "Conv2d Same =:  103\n",
      "Conv2d Same =:  107\n",
      "Linear =:  111\n",
      "*********** Layer  0 Start **************\n",
      "Layer  0  is OK\n",
      "*********** Layer  1 Start **************\n",
      "Iteration  1 with bits:  8 8\n",
      "Layer  1  is OK\n",
      "*********** Layer  2 Start **************\n",
      "Iteration  1 with bits:  4 8\n",
      "Layer  2  is OK\n",
      "*********** Layer  3 Start **************\n",
      "Layer  3  is OK\n",
      "*********** Layer  4 Start **************\n",
      "Layer  4  is OK\n",
      "*********** Layer  5 Start **************\n",
      "Iteration  1 with bits:  8 8\n",
      "Layer  5  is OK\n",
      "*********** Layer  6 Start **************\n",
      "Layer  6  is OK\n",
      "*********** Layer  7 Start **************\n",
      "Layer  7  is OK\n",
      "*********** Layer  8 Start **************\n",
      "Layer  8  is OK\n",
      "*********** Layer  9 Start **************\n",
      "Layer  9  is OK\n",
      "*********** Layer  10 Start **************\n",
      "Layer  10  is OK\n",
      "*********** Layer  11 Start **************\n",
      "Layer  11  is OK\n",
      "*********** Layer  12 Start **************\n",
      "Layer  12  is OK\n",
      "*********** Layer  13 Start **************\n",
      "Layer  13  is OK\n",
      "*********** Layer  14 Start **************\n",
      "Layer  14  is OK\n",
      "*********** Layer  15 Start **************\n",
      "Layer  15  is OK\n",
      "*********** Layer  16 Start **************\n",
      "Layer  16  is OK\n",
      "*********** Layer  17 Start **************\n",
      "Layer  17  is OK\n",
      "*********** Layer  18 Start **************\n",
      "Layer  18  is OK\n",
      "*********** Layer  19 Start **************\n",
      "Layer  19  is OK\n",
      "*********** Layer  20 Start **************\n",
      "Layer  20  is OK\n",
      "*********** Layer  21 Start **************\n",
      "Layer  21  is OK\n",
      "*********** Layer  22 Start **************\n",
      "Layer  22  is OK\n",
      "*********** Layer  23 Start **************\n",
      "Layer  23  is OK\n",
      "*********** Layer  24 Start **************\n",
      "Layer  24  is OK\n",
      "*********** Layer  25 Start **************\n",
      "Layer  25  is OK\n",
      "*********** Layer  26 Start **************\n",
      "Layer  26  is OK\n",
      "*********** Layer  27 Start **************\n",
      "Layer  27  is OK\n",
      "Input:  [8, 8, 4, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8] \n",
      "  Output: [8, 4, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# x = torch.Tensor(1,3,int(input_size),int(input_size))\n",
    "#compute input dimensions\n",
    "def print_size(model, input, output): \n",
    "    global si, so\n",
    "    si = input[0].size()\n",
    "    so = output[0].size()\n",
    "\n",
    "activation_vector_i = []\n",
    "activation_vector_o = []\n",
    "\n",
    "for i,module in enumerate(model.modules()):\n",
    "    if type(module) is models.linear_quantized_modules.Conv2d_SAME:\n",
    "        print('Conv2d Same =: ',i)\n",
    "        hook = module.register_forward_hook(print_size)\n",
    "        model(x)\n",
    "        hook.remove() \n",
    "        activation_vector_i.append(si)\n",
    "        activation_vector_o.append(so)\n",
    "        \n",
    "    elif type(module) is nn.Linear:\n",
    "        print('Linear =: ',i)\n",
    "        hook = module.register_forward_hook(print_size)\n",
    "        model(x)\n",
    "        hook.remove() \n",
    "        activation_vector_i.append(si)\n",
    "        activation_vector_o.append(so)\n",
    "\n",
    "    elif type(module) is nn.Conv2d:\n",
    "        print('Normal Conv =: ',i)\n",
    "\n",
    "def compute_activation_footprint(param_list):\n",
    "    i_params_mem = []\n",
    "    i_bits_mem = []\n",
    "    o_params_mem = []\n",
    "    o_bits_mem = []\n",
    "\n",
    "    for i,item in enumerate(param_list):\n",
    "        o_size = activation_vector_o[i]\n",
    "        i_size = activation_vector_i[i]\n",
    "        o_params = 1\n",
    "        for v in range(len(o_size)):\n",
    "            o_params *= o_size[v]\n",
    "        i_params = 1\n",
    "        for v in range(len(i_size)):\n",
    "            i_params *= i_size[v] \n",
    "        \n",
    "        if i == 0:\n",
    "            i_bits = 8\n",
    "            o_bits = param_list[i]['act_o_bits']\n",
    "        elif i == LAST:\n",
    "            i_bits = param_list[i-1]['act_o_bits']\n",
    "            o_bits = 8\n",
    "        else:\n",
    "            i_bits = param_list[i-1]['act_o_bits']\n",
    "            o_bits = param_list[i]['act_o_bits']\n",
    "        \n",
    "        i_mem = i_bits * i_params / 32\n",
    "        o_mem = o_bits * o_params / 32\n",
    "            \n",
    "        i_params_mem.append(i_params)\n",
    "        i_bits_mem.append(i_bits)\n",
    "        o_params_mem.append(o_params)\n",
    "        o_bits_mem.append(o_bits)\n",
    "        \n",
    "        #print('Input = ', i_params,'(bits = ', i_bits, ') | Output Params = ', o_params,'(bits = ', o_bits,')')            \n",
    "\n",
    "    return i_params_mem, i_bits_mem, o_params_mem, o_bits_mem\n",
    "\n",
    "def cut_activation_footprint(param_list, act_mem):\n",
    "    MIN_ACT_BITS = 2\n",
    "    i_params_mem, i_bits_mem, o_params_mem, o_bits_mem = compute_activation_footprint(param_list)\n",
    "    errQuant = False\n",
    "    for i in range(len(i_params_mem)):\n",
    "        print('*********** Layer ',i, 'Start **************')\n",
    "        tot_layer_mem = ((i_params_mem[i] * i_bits_mem[i]) + (o_params_mem[i] * o_bits_mem[i]) )/32\n",
    "        n_iter = 1\n",
    "        while tot_layer_mem > act_mem:\n",
    "            \n",
    "            print('Iteration ',n_iter, 'with bits: ',i_bits_mem[i],o_bits_mem[i] )\n",
    "            \n",
    "            i_mem = i_params_mem[i] * i_bits_mem[i]\n",
    "            o_mem = o_params_mem[i] * o_bits_mem[i]\n",
    "\n",
    "            if i == 0: # first layer\n",
    "                if o_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)\n",
    "                else:\n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution because of the first layer!')\n",
    "                    break\n",
    "            elif i == LAST: # last layer\n",
    "                if i_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)\n",
    "                else:\n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution because of the last layer!')\n",
    "                    break                \n",
    "            else:\n",
    "                if i_bits_mem[i] == MIN_ACT_BITS and o_bits_mem[i] == MIN_ACT_BITS :\n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution: layer ',i,'with i_bits,o_bits =',MIN_ACT_BITS)\n",
    "                    break\n",
    "\n",
    "                elif i_bits_mem[i] > o_bits_mem[i] and i_bits_mem[i] > MIN_ACT_BITS:\n",
    "                    param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)     \n",
    "\n",
    "                elif i_bits_mem[i] < o_bits_mem[i] and o_bits_mem[i] > MIN_ACT_BITS:\n",
    "                    param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)  \n",
    "\n",
    "                elif i_bits_mem[i] == o_bits_mem[i]:\n",
    "                    if i_mem > o_mem and i_bits_mem[i] > MIN_ACT_BITS :\n",
    "                        param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)\n",
    "                    elif o_bits_mem[i] > MIN_ACT_BITS:\n",
    "                        param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)\n",
    "                    else:\n",
    "                        print('Corner case!')\n",
    "\n",
    "                elif o_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i]['act_o_bits'] = int(o_bits_mem[i] / 2)\n",
    "\n",
    "                elif i_bits_mem[i] > MIN_ACT_BITS :\n",
    "                    param_list[i-1]['act_o_bits'] = int(i_bits_mem[i] / 2)\n",
    "\n",
    "                else: \n",
    "                    errQuant = True\n",
    "                    print('No way to find a solution!')\n",
    "                    break                \n",
    "\n",
    "            i_params_mem, i_bits_mem, o_params_mem, o_bits_mem = compute_activation_footprint(param_list)\n",
    "            tot_layer_mem = ((i_params_mem[i] * i_bits_mem[i]) + (o_params_mem[i] * o_bits_mem[i]) )/32\n",
    "            n_iter +=1\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Layer ',i,' is OK')\n",
    "            \n",
    "    i_params_mem, i_bits_mem, o_params_mem, o_bits_mem = compute_activation_footprint(param_list)\n",
    "    print('Input: ',i_bits_mem,'\\n  Output:', o_bits_mem)\n",
    "    if errQuant is True:\n",
    "        print('No way for quantization')\n",
    "        \n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "            \n",
    "ONLY_READ_MEM = 64*1024\n",
    "param_list = copy.deepcopy(quantizer_load.param_to_quantize)\n",
    "tt = cut_activation_footprint(param_list, ONLY_READ_MEM)\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Footprint:  2568144.0\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  27 equal to 0.299048651477 ratio\n",
      "Layer to cut:  27 to  4 bits\n",
      "Total Weights:  2132 kbytes | bias = 38 kbytes ( 1.823689280560256 %)\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  26 equal to 0.270048128695 ratio\n",
      "Layer to cut:  26 to  4 bits\n",
      "Total Weights:  1844 kbytes | bias = 41 kbytes ( 2.230324279919036 %)\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  27 equal to 0.203257196575 ratio\n",
      "26 0.15610152697\n",
      "24 0.15610152697\n",
      "Layer to cut:  24 to  4 bits\n",
      "Total Weights:  1700 kbytes | bias = 43 kbytes ( 2.551418781749203 %)\n",
      "***************************************************************\n",
      "[]\n",
      "most footprint on layer:  27 equal to 0.220464629206 ratio\n",
      "Layer to cut:  27 to  2 bits\n",
      "Total Weights:  1513 kbytes | bias = 43 kbytes ( 2.867511175808633 %)\n",
      "***************************************************************\n",
      "[1]\n",
      "most footprint on layer:  26 equal to 0.190293306904 ratio\n",
      "Layer to cut:  26 to  2 bits\n",
      "Total Weights:  1369 kbytes | bias = 43 kbytes ( 3.169034171943636 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.105151463289 ratio\n",
      "22 0.105151463289\n",
      "20 0.105151463289\n",
      "18 0.105151463289\n",
      "16 0.105151463289\n",
      "14 0.105151463289\n",
      "Layer to cut:  14 to  4 bits\n",
      "Total Weights:  1297 kbytes | bias = 44 kbytes ( 3.4316027794838444 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.110986668594 ratio\n",
      "22 0.110986668594\n",
      "20 0.110986668594\n",
      "18 0.110986668594\n",
      "16 0.110986668594\n",
      "Layer to cut:  16 to  4 bits\n",
      "Total Weights:  1225 kbytes | bias = 45 kbytes ( 3.725025182011756 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.117507554604 ratio\n",
      "22 0.117507554604\n",
      "20 0.117507554604\n",
      "18 0.117507554604\n",
      "Layer to cut:  18 to  4 bits\n",
      "Total Weights:  1153 kbytes | bias = 46 kbytes ( 4.055079178011677 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.124842524485 ratio\n",
      "22 0.124842524485\n",
      "20 0.124842524485\n",
      "Layer to cut:  20 to  4 bits\n",
      "Total Weights:  1081 kbytes | bias = 47 kbytes ( 4.429081241963215 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.133154176239 ratio\n",
      "22 0.133154176239\n",
      "Layer to cut:  22 to  4 bits\n",
      "Total Weights:  1009 kbytes | bias = 49 kbytes ( 4.85643526042876 %)\n",
      "***************************************************************\n",
      "[1, 0]\n",
      "most footprint on layer:  24 equal to 0.142651497562 ratio\n",
      "Layer to cut:  24 to  2 bits\n",
      "Total Weights:  937 kbytes | bias = 49 kbytes ( 5.229428138073571 %)\n",
      "***************************************************************\n",
      "[2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.076803840192 ratio\n",
      "20 0.076803840192\n",
      "18 0.076803840192\n",
      "16 0.076803840192\n",
      "12 0.076803840192\n",
      "10 0.038401920096\n",
      "Layer to cut:  10 to  4 bits\n",
      "Total Weights:  919 kbytes | bias = 49 kbytes ( 5.392981561729968 %)\n",
      "***************************************************************\n",
      "[2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0783074177925 ratio\n",
      "20 0.0783074177925\n",
      "18 0.0783074177925\n",
      "16 0.0783074177925\n",
      "12 0.0783074177925\n",
      "Layer to cut:  12 to  4 bits\n",
      "Total Weights:  883 kbytes | bias = 50 kbytes ( 5.7400824180683045 %)\n",
      "***************************************************************\n",
      "[2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0814983817053 ratio\n",
      "20 0.0814983817053\n",
      "18 0.0814983817053\n",
      "16 0.0814983817053\n",
      "14 0.0814983817053\n",
      "12 0.0407491908527\n",
      "Layer to cut:  12 to  2 bits\n",
      "Total Weights:  865 kbytes | bias = 50 kbytes ( 5.859466681109968 %)\n",
      "***************************************************************\n",
      "[10, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0831934138547 ratio\n",
      "20 0.0831934138547\n",
      "18 0.0831934138547\n",
      "16 0.0831934138547\n",
      "14 0.0831934138547\n",
      "Layer to cut:  14 to  2 bits\n",
      "Total Weights:  829 kbytes | bias = 50 kbytes ( 6.113779787133842 %)\n",
      "***************************************************************\n",
      "[10, 7, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0868041819723 ratio\n",
      "20 0.0868041819723\n",
      "18 0.0868041819723\n",
      "16 0.0868041819723\n",
      "Layer to cut:  16 to  2 bits\n",
      "Total Weights:  793 kbytes | bias = 50 kbytes ( 6.391169925759635 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.090742600579 ratio\n",
      "20 0.090742600579\n",
      "18 0.090742600579\n",
      "Layer to cut:  18 to  2 bits\n",
      "Total Weights:  757 kbytes | bias = 50 kbytes ( 6.694927491387668 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0950553870908 ratio\n",
      "20 0.0950553870908\n",
      "Layer to cut:  20 to  2 bits\n",
      "Total Weights:  721 kbytes | bias = 50 kbytes ( 7.028999631819461 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  22 equal to 0.0997985835878 ratio\n",
      "Layer to cut:  22 to  2 bits\n",
      "Total Weights:  685 kbytes | bias = 50 kbytes ( 7.398162711709864 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262600013677 ratio\n",
      "6 0.0131300006839\n",
      "4 0.00656500034193\n",
      "2 0.00164125008548\n",
      "0 0.000923203173083\n",
      "Layer to cut:  0 to  4 bits\n",
      "Total Weights:  685 kbytes | bias = 50 kbytes ( 7.4118418426979105 %)\n",
      "***************************************************************\n",
      "[10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.026272128624 ratio\n",
      "6 0.013136064312\n",
      "4 0.00656803215599\n",
      "2 0.001642008039\n",
      "0 0.000461814760968\n",
      "Layer to cut:  0 to  2 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.413553686959844 %)\n",
      "***************************************************************\n",
      "[27, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262781964535 ratio\n",
      "6 0.0131390982267\n",
      "4 0.00656954911337\n",
      "2 0.00164238727834\n",
      "1 0.000307947614689\n",
      "Layer to cut:  1 to  4 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.424961857096006 %)\n",
      "***************************************************************\n",
      "[26, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262822432305 ratio\n",
      "6 0.0131411216153\n",
      "4 0.00657056080763\n",
      "2 0.00164264020191\n",
      "1 0.000153997518929\n",
      "Layer to cut:  1 to  2 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.425533613972643 %)\n",
      "***************************************************************\n",
      "[26, 27, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0262842670865 ratio\n",
      "6 0.0131421335432\n",
      "4 0.00657106677162\n",
      "2 0.00164276669291\n",
      "Layer to cut:  2 to  4 bits\n",
      "Total Weights:  684 kbytes | bias = 50 kbytes ( 7.452189301821088 %)\n",
      "***************************************************************\n",
      "[26, 27, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263058742935 ratio\n",
      "6 0.0131529371468\n",
      "4 0.00657646857339\n",
      "2 0.000822058571673\n",
      "Layer to cut:  2 to  2 bits\n",
      "Total Weights:  683 kbytes | bias = 50 kbytes ( 7.455253629396109 %)\n",
      "***************************************************************\n",
      "[26, 27, 25, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263166912243 ratio\n",
      "6 0.0131583456122\n",
      "4 0.00657917280609\n",
      "3 0.000616797450571\n",
      "Layer to cut:  3 to  4 bits\n",
      "Total Weights:  683 kbytes | bias = 51 kbytes ( 7.478119787024976 %)\n",
      "***************************************************************\n",
      "[26, 27, 24, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263248097621 ratio\n",
      "6 0.0131624048811\n",
      "4 0.00658120244053\n",
      "3 0.0003084938644\n",
      "Layer to cut:  3 to  2 bits\n",
      "Total Weights:  683 kbytes | bias = 51 kbytes ( 7.47927344200849 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 10, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0263288709097 ratio\n",
      "6 0.0131644354548\n",
      "4 0.00658221772742\n",
      "Layer to cut:  4 to  4 bits\n",
      "Total Weights:  681 kbytes | bias = 51 kbytes ( 7.54524452393646 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264158082102 ratio\n",
      "6 0.0132079041051\n",
      "4 0.00330197602628\n",
      "Layer to cut:  4 to  2 bits\n",
      "Total Weights:  680 kbytes | bias = 51 kbytes ( 7.557722232749364 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 21, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264594925152 ratio\n",
      "6 0.0132297462576\n",
      "5 0.00124028871165\n",
      "Layer to cut:  5 to  4 bits\n",
      "Total Weights:  679 kbytes | bias = 51 kbytes ( 7.603780631445891 %)\n",
      "***************************************************************\n",
      "[25, 27, 24, 26, 21, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264759114022 ratio\n",
      "6 0.0132379557011\n",
      "5 0.00062052917349\n",
      "Layer to cut:  5 to  2 bits\n",
      "Total Weights:  679 kbytes | bias = 51 kbytes ( 7.606140547499583 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0264841284894 ratio\n",
      "6 0.0132420642447\n",
      "Layer to cut:  6 to  4 bits\n",
      "Total Weights:  675 kbytes | bias = 51 kbytes ( 7.698493974160924 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0266606495062 ratio\n",
      "6 0.00666516237655\n",
      "Layer to cut:  6 to  2 bits\n",
      "Total Weights:  672 kbytes | bias = 51 kbytes ( 7.724235616470165 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 18, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.026749795371 ratio\n",
      "7 0.00125389665802\n",
      "Layer to cut:  7 to  4 bits\n",
      "Total Weights:  672 kbytes | bias = 52 kbytes ( 7.770904127094769 %)\n",
      "***************************************************************\n",
      "[25, 27, 23, 26, 21, 24, 18, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0267665766315 ratio\n",
      "7 0.000627341639801\n",
      "Layer to cut:  7 to  2 bits\n",
      "Total Weights:  672 kbytes | bias = 52 kbytes ( 7.773342397778049 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 21, 24, 18, 23, 10, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  8 equal to 0.0267749751599 ratio\n",
      "Layer to cut:  8 to  4 bits\n",
      "Total Weights:  663 kbytes | bias = 52 kbytes ( 7.963627154778942 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 21, 24, 18, 23, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0271382886623 ratio\n",
      "8 0.0135691443312\n",
      "Layer to cut:  8 to  2 bits\n",
      "Total Weights:  658 kbytes | bias = 52 kbytes ( 8.018026031011889 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 21, 24, 18, 23, 11, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0273236680601 ratio\n",
      "9 0.00256159388064\n",
      "Layer to cut:  9 to  4 bits\n",
      "Total Weights:  657 kbytes | bias = 53 kbytes ( 8.11380462984403 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 20, 24, 18, 23, 11, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0273587090109 ratio\n",
      "9 0.00128243948489\n",
      "Layer to cut:  9 to  2 bits\n",
      "Total Weights:  657 kbytes | bias = 53 kbytes ( 8.119010699793847 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 20, 24, 18, 23, 11, 21, 9, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  10 equal to 0.0273762632114 ratio\n",
      "Layer to cut:  10 to  2 bits\n",
      "Total Weights:  648 kbytes | bias = 53 kbytes ( 8.231687116379648 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 20, 24, 18, 23, 11, 21, 9, 8, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104085726161 ratio\n",
      "13 0.00520428630803\n",
      "11 0.00260214315401\n",
      "Layer to cut:  11 to  4 bits\n",
      "Total Weights:  647 kbytes | bias = 53 kbytes ( 8.329262187803451 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 19, 24, 18, 23, 11, 21, 9, 8, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104221325565 ratio\n",
      "13 0.00521106627825\n",
      "11 0.00130276656956\n",
      "Layer to cut:  11 to  2 bits\n",
      "Total Weights:  647 kbytes | bias = 53 kbytes ( 8.334691266378181 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 19, 24, 18, 23, 11, 21, 9, 20, 8, 3, 7, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104289257844 ratio\n",
      "13 0.00521446289222\n",
      "Layer to cut:  13 to  4 bits\n",
      "Total Weights:  645 kbytes | bias = 55 kbytes ( 8.530748331427258 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 19, 24, 17, 23, 11, 20, 9, 21, 8, 4, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104561874853 ratio\n",
      "21 0.00522809374263\n",
      "19 0.00522809374263\n",
      "17 0.00522809374263\n",
      "15 0.00522809374263\n",
      "13 0.00261404687131\n",
      "Layer to cut:  13 to  2 bits\n",
      "Total Weights:  644 kbytes | bias = 55 kbytes ( 8.541912811657427 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 18, 24, 17, 23, 11, 20, 9, 21, 8, 19, 4, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104698718531 ratio\n",
      "21 0.00523493592657\n",
      "19 0.00523493592657\n",
      "17 0.00523493592657\n",
      "15 0.00523493592657\n",
      "Layer to cut:  15 to  4 bits\n",
      "Total Weights:  643 kbytes | bias = 56 kbytes ( 8.739285475630721 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 18, 24, 16, 23, 11, 20, 9, 21, 8, 19, 4, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0104973483261 ratio\n",
      "21 0.00524867416304\n",
      "19 0.00524867416304\n",
      "17 0.00524867416304\n",
      "15 0.00262433708152\n",
      "Layer to cut:  15 to  2 bits\n",
      "Total Weights:  642 kbytes | bias = 56 kbytes ( 8.750767958052762 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 17, 24, 16, 23, 11, 20, 9, 21, 8, 18, 4, 19, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105111407142 ratio\n",
      "21 0.00525557035712\n",
      "19 0.00525557035712\n",
      "17 0.00525557035712\n",
      "Layer to cut:  17 to  4 bits\n",
      "Total Weights:  640 kbytes | bias = 57 kbytes ( 8.949470923672735 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 17, 24, 15, 23, 11, 20, 9, 21, 8, 18, 4, 19, 5, 6, 3, 7, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105388345074 ratio\n",
      "21 0.00526941725368\n",
      "19 0.00526941725368\n",
      "17 0.00263470862684\n",
      "Layer to cut:  17 to  2 bits\n",
      "Total Weights:  639 kbytes | bias = 57 kbytes ( 8.961276099396027 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 16, 24, 15, 23, 11, 20, 9, 21, 8, 17, 7, 18, 3, 19, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105527361999 ratio\n",
      "21 0.00527636809996\n",
      "19 0.00527636809996\n",
      "Layer to cut:  19 to  4 bits\n",
      "Total Weights:  637 kbytes | bias = 58 kbytes ( 9.161324295694875 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 16, 24, 14, 23, 11, 20, 9, 21, 8, 17, 7, 18, 3, 19, 6, 5, 4, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105806499017 ratio\n",
      "21 0.00529032495086\n",
      "19 0.00264516247543\n",
      "Layer to cut:  19 to  2 bits\n",
      "Total Weights:  637 kbytes | bias = 58 kbytes ( 9.173456937725703 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 15, 24, 14, 23, 11, 21, 9, 20, 8, 16, 7, 17, 6, 18, 5, 19, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0105946622032 ratio\n",
      "23 0.00529733110159\n",
      "21 0.00529733110159\n",
      "Layer to cut:  21 to  4 bits\n",
      "Total Weights:  635 kbytes | bias = 59 kbytes ( 9.374865524469936 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 15, 24, 13, 23, 11, 21, 9, 20, 8, 16, 7, 17, 6, 18, 5, 19, 4, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106227984435 ratio\n",
      "23 0.00531139922173\n",
      "21 0.00265569961087\n",
      "Layer to cut:  21 to  2 bits\n",
      "Total Weights:  634 kbytes | bias = 59 kbytes ( 9.387330489433866 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 14, 24, 13, 23, 11, 21, 9, 20, 8, 15, 7, 16, 6, 17, 5, 18, 4, 19, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106369226792 ratio\n",
      "23 0.00531846133959\n",
      "Layer to cut:  23 to  4 bits\n",
      "Total Weights:  632 kbytes | bias = 60 kbytes ( 9.590114861653737 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 14, 24, 12, 23, 11, 21, 9, 20, 8, 15, 7, 16, 6, 17, 5, 18, 4, 19, 3, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106652841298 ratio\n",
      "23 0.00266632103246\n",
      "Layer to cut:  23 to  2 bits\n",
      "Total Weights:  632 kbytes | bias = 60 kbytes ( 9.60291709156083 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 13, 24, 12, 23, 11, 21, 9, 20, 8, 14, 7, 15, 6, 16, 5, 17, 4, 18, 3, 19, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.0106795216464 ratio\n",
      "Layer to cut:  25 to  4 bits\n",
      "Total Weights:  628 kbytes | bias = 62 kbytes ( 10.012364779639745 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 13, 24, 12, 23, 10, 21, 9, 20, 8, 14, 7, 15, 6, 16, 5, 17, 4, 18, 3, 19, 2, 1, 0]\n",
      "most footprint on layer:  25 equal to 0.00536842693906 ratio\n",
      "Layer to cut:  25 to  2 bits\n",
      "Total Weights:  626 kbytes | bias = 62 kbytes ( 10.039312437308812 %)\n",
      "***************************************************************\n",
      "[25, 27, 22, 26, 13, 24, 11, 23, 10, 21, 9, 20, 8, 14, 7, 15, 6, 16, 5, 17, 4, 18, 3, 19, 2, 12, 1, 0]\n",
      "No way\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_params_footprint(param_list):\n",
    "    LAST = len(param_list)\n",
    "    weight_footprint = [0 for x in range(LAST)]\n",
    "    weight_bits = [0 for x in range(LAST)]\n",
    "    for i,item in enumerate(param_list):\n",
    "        #number of bits\n",
    "        w_bits = item['w_bits']\n",
    "                \n",
    "        # size of convolutional parameters\n",
    "        len_w = len(item['weight'].size())\n",
    "        size_param = 1\n",
    "        for v in range(len_w):\n",
    "            size_param *= item['weight'].size(v)\n",
    "        w_footprint = (size_param * w_bits) / 8 #measure in bytes\n",
    "        weight_footprint[i] = w_footprint\n",
    "        weight_bits[i] = w_bits\n",
    "        \n",
    "        #print('------------------- This is the layer ', i, '---------------')\n",
    "        #print('Weight Bits: ',w_bits)\n",
    "        #print('Number of parameters: ',size_param)\n",
    "        #print('Parameters footprint: ',w_footprint)\n",
    "        #if i is LAST-1:\n",
    "        #    print('no')\n",
    "        #else:\n",
    "        #    print('Activation Bits = ',item['act_o_bits'])\n",
    "    \n",
    "    return weight_footprint,sum(weight_footprint), weight_bits\n",
    "\n",
    "\n",
    "def compute_bias_footprint(param_list):\n",
    "    BIAS_PACT_BITS = 32\n",
    "    BIAS_PACTCH_BITS = 8\n",
    "    BIAS_M0_BITS = 32\n",
    "    BIAS_N0_BITS = 8\n",
    "\n",
    "    LAST = len(param_list)\n",
    "    weight_footprint = [0 for x in range(LAST)]\n",
    "    for i,item in enumerate(param_list):\n",
    "\n",
    "        #number of bits\n",
    "        w_bits = item['w_bits']\n",
    "        n_out_ch = item['weight'].size(0)\n",
    "        \n",
    "        # size of extra parametes\n",
    "        quant_type = item['quant_type']\n",
    "        fold_type = item['fold_type']\n",
    "        if quant_type is None:\n",
    "            quant_type = 'PACT'\n",
    "            fold_type = 'folding_weights'\n",
    "        elif w_bits < 8:\n",
    "            quant_type = 'PACT_CHANNEL'\n",
    "            fold_type = 'fixed_batch'\n",
    "        item['quant_type'] = quant_type\n",
    "        item['fold_type'] = fold_type\n",
    "        \n",
    "        # compute number of bias\n",
    "        if quant_type == 'PACT':\n",
    "            bias_size = int(n_out_ch * BIAS_PACT_BITS / 8 )\n",
    "        elif quant_type == 'PACT_CHANNEL':\n",
    "            bias_size  = int(n_out_ch * BIAS_M0_BITS / 8 ) # M0 \n",
    "            bias_size += int(n_out_ch * BIAS_N0_BITS / 8 ) # N0 \n",
    "            bias_size += int(n_out_ch * BIAS_PACTCH_BITS / 8 ) # B \n",
    "            bias_size += int(n_out_ch * BIAS_PACTCH_BITS / 8 ) # ZW \n",
    "\n",
    "        else:\n",
    "            print('ERROR!!!')\n",
    "        \n",
    "        weight_footprint[i] = bias_size\n",
    "        \n",
    "    return weight_footprint\n",
    "\n",
    "def compute_footprint(param_list):\n",
    "    weight_footprint, _, _ = compute_params_footprint(param_list)\n",
    "    bias_footprint = compute_bias_footprint(param_list)\n",
    "    ratio = np.array(bias_footprint) / np.array(weight_footprint)\n",
    "    weight_footprint = sum(weight_footprint)\n",
    "    bias_footprint = sum(bias_footprint)\n",
    "    print('Total Weights: ',int(weight_footprint/1024),'kbytes | bias =' ,int(bias_footprint/1024),'kbytes ('\\\n",
    "          ,100*bias_footprint/weight_footprint ,'%)')\n",
    "    #print('max:', ratio * 100,'%')\n",
    "    return bias_footprint+weight_footprint\n",
    "        \n",
    "        \n",
    "def compute_next_cut(param_list):\n",
    "    MAX = 0.05\n",
    "\n",
    "    f,_,w_b = compute_params_footprint(param_list)\n",
    "    weight = np.array(f)\n",
    "    total = weight.sum()\n",
    "    perc = weight / total\n",
    "    arr = np.sort(perc)[::-1]\n",
    "    ind = np.argsort(perc)[::-1]\n",
    "    \n",
    "    # remove w_bit = 1 from search\n",
    "    rm = []\n",
    "    for i,item in enumerate(w_b):\n",
    "        if item == MIN_BIT:\n",
    "            for i_x,x in enumerate(ind):\n",
    "                if x == i:\n",
    "                    rm.append(i_x)\n",
    "                    continue\n",
    "    print(rm)\n",
    "    arr = np.delete(arr, rm)\n",
    "    ind = np.delete(ind, rm)\n",
    "    \n",
    "    if len(arr) == 0:\n",
    "        return -1\n",
    "        \n",
    "    #print(arr, ind)\n",
    "    LAST = len(arr)\n",
    "\n",
    "    cut_we = arr[0]\n",
    "    cut_i = ind[0]\n",
    "    print('most footprint on layer: ',cut_i, 'equal to', cut_we, 'ratio' )\n",
    "    thr = cut_we - MAX\n",
    "    for i in range(1, LAST):\n",
    "        if arr[i] > thr and ind[i] < cut_i:\n",
    "            cut_i = ind[i]\n",
    "            print(ind[i],arr[i])\n",
    "    return(cut_i)            \n",
    "\n",
    "    \n",
    "def memory_driven_quantization(param_list, read_only_mem):\n",
    "    _, s, _ = compute_params_footprint(param_list)\n",
    "    print('Total Footprint: ', s )\n",
    "    while (s>read_only_mem):\n",
    "        print('***************************************************************')\n",
    "        c = compute_next_cut(param_list)\n",
    "        if c == -1:\n",
    "            return -1\n",
    "        nb = int(param_list[c]['w_bits'] / 2)\n",
    "        print('Layer to cut: ', c, 'to ', nb  ,'bits')\n",
    "        param_list[c]['w_bits'] = nb\n",
    "        #_, s, _ = compute_params_footprint(param_list)\n",
    "        s = compute_footprint(param_list)\n",
    "    return(param_list)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "ONLY_READ_MEM = 512*1024\n",
    "param_list = copy.deepcopy(quantizer_load.param_to_quantize)\n",
    "param_list = memory_driven_quantization(param_list, ONLY_READ_MEM)\n",
    "if param_list == -1:\n",
    "    print('No way')\n",
    "else:\n",
    "    _, s, w_bits = compute_params_footprint(param_list)\n",
    "    print('-----------  Total Footprint: ', s )\n",
    "    print('Final Quantization: ', w_bits )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0032, 0.0032, 0.0032, 0.0032, 5.1141, 0.1086, 4.4318, 3.4280, 3.2614,\n",
       "        0.0032, 1.4061, 0.0032, 1.3680, 5.7485, 0.0032, 0.0032, 0.0032, 0.0074,\n",
       "        1.0969, 3.7479, 1.9707, 4.7107, 1.5083, 2.8547, 4.3182, 0.0032, 2.3813,\n",
       "        0.0032, 0.0868, 1.5090, 4.2023, 0.0032], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0080,  0.6109,  0.4207,  0.6073,  0.1026,  0.0430,  0.1581,  0.2290,\n",
       "         0.1794,  0.2899,  0.2980,  1.3301,  0.3118,  0.1344,  0.2803, -0.8490,\n",
       "         0.6654, -0.8486, -0.3533,  0.2579,  0.1002,  0.0991,  0.1978,  0.2172,\n",
       "         0.1020,  0.0547,  0.1285,  0.7153, -0.0120,  0.0705,  0.2167, -0.1538],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma = gamma_tensor / sigma_tensor\n",
    "gamma_over_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080]],\n",
       "\n",
       "         [[ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080]],\n",
       "\n",
       "         [[ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080],\n",
       "          [ 0.0080,  0.0080,  0.0080]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109]],\n",
       "\n",
       "         [[ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109]],\n",
       "\n",
       "         [[ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109],\n",
       "          [ 0.6109,  0.6109,  0.6109]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207]],\n",
       "\n",
       "         [[ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207]],\n",
       "\n",
       "         [[ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207],\n",
       "          [ 0.4207,  0.4207,  0.4207]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073]],\n",
       "\n",
       "         [[ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073]],\n",
       "\n",
       "         [[ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073],\n",
       "          [ 0.6073,  0.6073,  0.6073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026]],\n",
       "\n",
       "         [[ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026]],\n",
       "\n",
       "         [[ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026],\n",
       "          [ 0.1026,  0.1026,  0.1026]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430]],\n",
       "\n",
       "         [[ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430]],\n",
       "\n",
       "         [[ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430],\n",
       "          [ 0.0430,  0.0430,  0.0430]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581]],\n",
       "\n",
       "         [[ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581]],\n",
       "\n",
       "         [[ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581],\n",
       "          [ 0.1581,  0.1581,  0.1581]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290]],\n",
       "\n",
       "         [[ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290]],\n",
       "\n",
       "         [[ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290],\n",
       "          [ 0.2290,  0.2290,  0.2290]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794]],\n",
       "\n",
       "         [[ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794]],\n",
       "\n",
       "         [[ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794],\n",
       "          [ 0.1794,  0.1794,  0.1794]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899]],\n",
       "\n",
       "         [[ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899]],\n",
       "\n",
       "         [[ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899],\n",
       "          [ 0.2899,  0.2899,  0.2899]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980]],\n",
       "\n",
       "         [[ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980]],\n",
       "\n",
       "         [[ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980],\n",
       "          [ 0.2980,  0.2980,  0.2980]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301]],\n",
       "\n",
       "         [[ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301]],\n",
       "\n",
       "         [[ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301],\n",
       "          [ 1.3301,  1.3301,  1.3301]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118]],\n",
       "\n",
       "         [[ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118]],\n",
       "\n",
       "         [[ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118],\n",
       "          [ 0.3118,  0.3118,  0.3118]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344]],\n",
       "\n",
       "         [[ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344]],\n",
       "\n",
       "         [[ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344],\n",
       "          [ 0.1344,  0.1344,  0.1344]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803]],\n",
       "\n",
       "         [[ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803]],\n",
       "\n",
       "         [[ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803],\n",
       "          [ 0.2803,  0.2803,  0.2803]]],\n",
       "\n",
       "\n",
       "        [[[-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490]],\n",
       "\n",
       "         [[-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490]],\n",
       "\n",
       "         [[-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490],\n",
       "          [-0.8490, -0.8490, -0.8490]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654]],\n",
       "\n",
       "         [[ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654]],\n",
       "\n",
       "         [[ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654],\n",
       "          [ 0.6654,  0.6654,  0.6654]]],\n",
       "\n",
       "\n",
       "        [[[-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486]],\n",
       "\n",
       "         [[-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486]],\n",
       "\n",
       "         [[-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486],\n",
       "          [-0.8486, -0.8486, -0.8486]]],\n",
       "\n",
       "\n",
       "        [[[-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533]],\n",
       "\n",
       "         [[-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533]],\n",
       "\n",
       "         [[-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533],\n",
       "          [-0.3533, -0.3533, -0.3533]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579]],\n",
       "\n",
       "         [[ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579]],\n",
       "\n",
       "         [[ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579],\n",
       "          [ 0.2579,  0.2579,  0.2579]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002]],\n",
       "\n",
       "         [[ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002]],\n",
       "\n",
       "         [[ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002],\n",
       "          [ 0.1002,  0.1002,  0.1002]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991]],\n",
       "\n",
       "         [[ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991]],\n",
       "\n",
       "         [[ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991],\n",
       "          [ 0.0991,  0.0991,  0.0991]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978]],\n",
       "\n",
       "         [[ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978]],\n",
       "\n",
       "         [[ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978],\n",
       "          [ 0.1978,  0.1978,  0.1978]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172]],\n",
       "\n",
       "         [[ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172]],\n",
       "\n",
       "         [[ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172],\n",
       "          [ 0.2172,  0.2172,  0.2172]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020]],\n",
       "\n",
       "         [[ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020]],\n",
       "\n",
       "         [[ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020],\n",
       "          [ 0.1020,  0.1020,  0.1020]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547]],\n",
       "\n",
       "         [[ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547]],\n",
       "\n",
       "         [[ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547],\n",
       "          [ 0.0547,  0.0547,  0.0547]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285]],\n",
       "\n",
       "         [[ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285]],\n",
       "\n",
       "         [[ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285],\n",
       "          [ 0.1285,  0.1285,  0.1285]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153]],\n",
       "\n",
       "         [[ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153]],\n",
       "\n",
       "         [[ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153],\n",
       "          [ 0.7153,  0.7153,  0.7153]]],\n",
       "\n",
       "\n",
       "        [[[-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120]],\n",
       "\n",
       "         [[-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120]],\n",
       "\n",
       "         [[-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120],\n",
       "          [-0.0120, -0.0120, -0.0120]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705]],\n",
       "\n",
       "         [[ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705]],\n",
       "\n",
       "         [[ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705],\n",
       "          [ 0.0705,  0.0705,  0.0705]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167]],\n",
       "\n",
       "         [[ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167]],\n",
       "\n",
       "         [[ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167],\n",
       "          [ 0.2167,  0.2167,  0.2167]]],\n",
       "\n",
       "\n",
       "        [[[-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538]],\n",
       "\n",
       "         [[-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538]],\n",
       "\n",
       "         [[-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538],\n",
       "          [-0.1538, -0.1538, -0.1538]]]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_over_sigma_tensor = gamma_over_sigma.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(v0,v1,v2,v3 )\n",
    "gamma_over_sigma_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = weight * gamma_over_sigma_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1085, -0.2053, -0.0891],\n",
       "         [ 0.1201,  0.1860,  0.1317],\n",
       "         [ 0.0116,  0.0116,  0.0271]],\n",
       "\n",
       "        [[-0.2867, -0.4184, -0.2053],\n",
       "         [ 0.1627,  0.2944,  0.1976],\n",
       "         [ 0.0349,  0.0465, -0.0155]],\n",
       "\n",
       "        [[-0.0542, -0.1356, -0.0310],\n",
       "         [ 0.0930,  0.1278,  0.0659],\n",
       "         [ 0.0271, -0.0155, -0.0271]]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tensor[10].div(d255).round().mul(d255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039, device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = weight_tensor.data.max()-weight_tensor.data.min()\n",
    "d255 = delta / 255\n",
    "d255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1085, -0.2053, -0.0891],\n",
       "         [ 0.1201,  0.1860,  0.1317],\n",
       "         [ 0.0116,  0.0116,  0.0271]],\n",
       "\n",
       "        [[-0.2867, -0.4184, -0.2053],\n",
       "         [ 0.1627,  0.2944,  0.1976],\n",
       "         [ 0.0349,  0.0465, -0.0155]],\n",
       "\n",
       "        [[-0.0542, -0.1356, -0.0310],\n",
       "         [ 0.0930,  0.1278,  0.0659],\n",
       "         [ 0.0271, -0.0155, -0.0271]]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].weight.data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6001, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore_real_value()            \n",
    "quantizer.backprop_quant_gradients()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReLU6' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-db7f3e6b9759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 366\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReLU6' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "quantizer.deployment_model.model[1].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.9390e-03\n",
       " 6.2338e-04\n",
       " 5.6052e-45\n",
       " 1.7143e-01\n",
       " 1.7013e-02\n",
       " 4.5158e-02\n",
       " 2.7970e-03\n",
       " 7.9009e-02\n",
       " 1.0388e-01\n",
       " 5.0715e-02\n",
       " 4.5062e-02\n",
       " 3.5231e-01\n",
       " 8.3555e-03\n",
       " 9.0936e-05\n",
       " 2.9523e-02\n",
       " 8.0683e-01\n",
       " 4.9630e-03\n",
       " 5.5362e-01\n",
       " 5.4003e-03\n",
       " 9.3962e-03\n",
       " 5.6052e-45\n",
       " 7.3168e-04\n",
       " 1.3904e-03\n",
       " 8.2610e-03\n",
       " 9.0941e-04\n",
       " 1.1302e-02\n",
       " 5.6052e-45\n",
       " 1.0920e-02\n",
       " 7.0269e-02\n",
       " 1.7943e-03\n",
       " 8.2472e-03\n",
       " 4.1882e-02\n",
       "[torch.cuda.FloatTensor of size 32 (GPU 2)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0][1].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 0  , 1  ,.,.) = \n",
       "  0.2660\n",
       "\n",
       "( 0  , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 0  ,1021,.,.) = \n",
       "  0.0112\n",
       "\n",
       "( 0  ,1022,.,.) = \n",
       "  0.0024\n",
       "\n",
       "( 0  ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 1  , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 1  , 1  ,.,.) = \n",
       "  0.2753\n",
       "\n",
       "( 1  , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 1  ,1021,.,.) = \n",
       "  0.0117\n",
       "\n",
       "( 1  ,1022,.,.) = \n",
       "  0.0023\n",
       "\n",
       "( 1  ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 2  , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 2  , 1  ,.,.) = \n",
       "  0.2900\n",
       "\n",
       "( 2  , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 2  ,1021,.,.) = \n",
       "  0.0102\n",
       "\n",
       "( 2  ,1022,.,.) = \n",
       "  0.0023\n",
       "\n",
       "( 2  ,1023,.,.) = \n",
       "  0.0000\n",
       " ...      \n",
       "          \n",
       "\n",
       "( 61 , 0  ,.,.) = \n",
       "  0.0021\n",
       "\n",
       "( 61 , 1  ,.,.) = \n",
       "  0.2783\n",
       "\n",
       "( 61 , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 61 ,1021,.,.) = \n",
       "  0.0094\n",
       "\n",
       "( 61 ,1022,.,.) = \n",
       "  0.0023\n",
       "\n",
       "( 61 ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 62 , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 62 , 1  ,.,.) = \n",
       "  0.2816\n",
       "\n",
       "( 62 , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 62 ,1021,.,.) = \n",
       "  0.0087\n",
       "\n",
       "( 62 ,1022,.,.) = \n",
       "  0.0024\n",
       "\n",
       "( 62 ,1023,.,.) = \n",
       "  0.0000\n",
       "          \n",
       "\n",
       "( 63 , 0  ,.,.) = \n",
       "  0.0022\n",
       "\n",
       "( 63 , 1  ,.,.) = \n",
       "  0.2836\n",
       "\n",
       "( 63 , 2  ,.,.) = \n",
       "  0.0000\n",
       "      ... \n",
       "\n",
       "( 63 ,1021,.,.) = \n",
       "  0.0113\n",
       "\n",
       "( 63 ,1022,.,.) = \n",
       "  0.0024\n",
       "\n",
       "( 63 ,1023,.,.) = \n",
       "  0.0000\n",
       "[torch.cuda.FloatTensor of size 64x1024x1x1 (GPU 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = quantizer.deployment_model.model(input_var)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0000  0.0045  0.0045\n",
       "  0.0045  0.0045  0.0045\n",
       "  0.0000  0.0045  0.0045\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0045 -0.0045 -0.0000\n",
       " -0.0045 -0.0000 -0.0045\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  0.0000  0.0045  0.0000\n",
       "  0.0000  0.0000  0.0045\n",
       "  0.0000  0.0045  0.0045\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.0180  0.0271 -0.0225\n",
       " -0.0135 -0.0406 -0.0406\n",
       " -0.0361 -0.0406 -0.0000\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.0406  0.0180 -0.0271\n",
       " -0.0406 -0.0857 -0.0722\n",
       " -0.0496 -0.0496 -0.0000\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.0406  0.0225 -0.0271\n",
       " -0.0361 -0.0406 -0.0451\n",
       " -0.0406 -0.0271  0.0045\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       " -0.0000 -0.0135 -0.0000\n",
       " -0.0000 -0.0045 -0.0045\n",
       " -0.0045  0.0045 -0.0045\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0135 -0.0045 -0.0000\n",
       " -0.0135 -0.0000 -0.0045\n",
       "  0.0000  0.0000  0.0045\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.0045 -0.0135  0.0000\n",
       " -0.0000 -0.0000 -0.0045\n",
       " -0.0045  0.0045  0.0045\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       "  0.0045 -0.0135 -0.0045\n",
       "  0.0045 -0.0045  0.0045\n",
       "  0.0000 -0.0045 -0.0045\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0045 -0.0045 -0.0000\n",
       "  0.0135 -0.0045  0.0000\n",
       "  0.0135  0.0135  0.0135\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       "  0.0135 -0.0045  0.0135\n",
       "  0.0045  0.0135  0.0135\n",
       "  0.0135  0.0135  0.0135\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1759 -0.1894 -0.0541\n",
       "  0.3066  0.2480  0.0857\n",
       " -0.0722 -0.0947  0.0225\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       " -0.2976 -0.2886 -0.1127\n",
       "  0.4374  0.4194  0.1623\n",
       " -0.0722 -0.1218 -0.0090\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       " -0.0316 -0.0586  0.0045\n",
       "  0.2165  0.1759  0.0767\n",
       " -0.0586 -0.0767  0.0361\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       "  0.0045  0.0045 -0.0000\n",
       "  0.0045  0.0000  0.0045\n",
       "  0.0000  0.0045  0.0045\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       " -0.0000  0.0045  0.0045\n",
       " -0.0000  0.0000 -0.0000\n",
       "  0.0045  0.0045 -0.0000\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       " -0.0000  0.0000  0.0000\n",
       " -0.0045  0.0000  0.0045\n",
       "  0.0000 -0.0000  0.0045\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.0225  0.1443  0.0406\n",
       "  0.0857  0.2300  0.0857\n",
       "  0.1308  0.1939  0.0271\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.0406  0.0135 -0.0406\n",
       " -0.0767 -0.0045 -0.0451\n",
       " -0.0361 -0.0180 -0.0631\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.0225  0.0135 -0.0225\n",
       " -0.1263 -0.0812 -0.0676\n",
       " -0.0902 -0.0767 -0.0631\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       "  0.0045  0.0135 -0.0180\n",
       " -0.0135 -0.0000 -0.0180\n",
       " -0.0135  0.0045  0.0045\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.0135  0.0135 -0.0135\n",
       "  0.0135  0.0045  0.0225\n",
       "  0.0225  0.0135  0.0225\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       " -0.0045  0.0045  0.0045\n",
       " -0.0225  0.0135 -0.0000\n",
       " -0.0135 -0.0000  0.0045\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.0812 -0.0586 -0.1443\n",
       " -0.1984 -0.1668 -0.2390\n",
       " -0.1353 -0.0857 -0.1939\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.0406  0.1533  0.0631\n",
       "  0.0135  0.1353  0.0271\n",
       "  0.0496  0.1488 -0.0045\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.0451  0.1894  0.0676\n",
       "  0.1127  0.2435  0.0992\n",
       "  0.1082  0.2119  0.0271\n",
       "\n",
       "(10,0 ,.,.) = \n",
       "  0.0406  0.0586  0.0631\n",
       "  0.0406  0.0496  0.0586\n",
       "  0.0406  0.0451  0.0496\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.0045  0.0045  0.0135\n",
       " -0.0135 -0.0135  0.0000\n",
       " -0.0045 -0.0180 -0.0135\n",
       "\n",
       "(10,2 ,.,.) = \n",
       " -0.0361 -0.0361 -0.0225\n",
       " -0.0451 -0.0586 -0.0406\n",
       " -0.0361 -0.0586 -0.0496\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.0361  0.0180  0.0135\n",
       "  0.0406  0.0271  0.0180\n",
       "  0.0451  0.0361  0.0225\n",
       "\n",
       "(11,1 ,.,.) = \n",
       " -0.0135 -0.0225 -0.0406\n",
       " -0.0045 -0.0180 -0.0361\n",
       " -0.0000 -0.0135 -0.0271\n",
       "\n",
       "(11,2 ,.,.) = \n",
       " -0.0225 -0.0271 -0.0271\n",
       " -0.0135 -0.0225 -0.0361\n",
       " -0.0135 -0.0180 -0.0271\n",
       "\n",
       "(12,0 ,.,.) = \n",
       "  0.0496  0.0676 -0.1037\n",
       "  0.0045  0.1082 -0.0135\n",
       " -0.1037 -0.0045  0.0451\n",
       "\n",
       "(12,1 ,.,.) = \n",
       "  0.1037  0.1398 -0.0451\n",
       "  0.0586  0.1759  0.0631\n",
       " -0.0676  0.0180  0.0902\n",
       "\n",
       "(12,2 ,.,.) = \n",
       "  0.0406  0.0722 -0.0271\n",
       "  0.0045  0.0857  0.0361\n",
       " -0.0812 -0.0135  0.0722\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0045  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(14,0 ,.,.) = \n",
       "  0.0045  0.0045  0.0000\n",
       "  0.0045  0.0045  0.0000\n",
       "  0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,1 ,.,.) = \n",
       "  0.0045  0.0045  0.0000\n",
       "  0.0000  0.0045  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.0000  0.0045  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       " -0.0045 -0.0000 -0.0000\n",
       "\n",
       "(15,0 ,.,.) = \n",
       "  0.1759 -0.0271 -0.1939\n",
       " -0.2255  0.5772  0.5772\n",
       "  0.0541 -0.5727 -0.5727\n",
       "\n",
       "(15,1 ,.,.) = \n",
       " -0.1804  0.0631  0.1578\n",
       " -0.0406  0.5772  0.5772\n",
       "  0.2165 -0.5727 -0.5727\n",
       "\n",
       "(15,2 ,.,.) = \n",
       "  0.2796 -0.0812 -0.2345\n",
       " -0.1939  0.4239  0.5772\n",
       "  0.0722 -0.4104 -0.5727\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0000  0.0857 -0.0722\n",
       "  0.1037  0.0722 -0.2345\n",
       "  0.0947  0.0496 -0.2570\n",
       "\n",
       "(16,1 ,.,.) = \n",
       "  0.0406  0.1533 -0.1218\n",
       "  0.2435  0.0676 -0.4600\n",
       "  0.1668  0.0451 -0.4149\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.0406  0.1037 -0.0045\n",
       "  0.0676  0.1037 -0.1533\n",
       " -0.0676  0.0361 -0.1488\n",
       "\n",
       "(17,0 ,.,.) = \n",
       " -0.0000  0.0045  0.0045\n",
       "  0.0225  0.0225  0.0225\n",
       "  0.0271  0.0271  0.0271\n",
       "\n",
       "(17,1 ,.,.) = \n",
       "  0.0045  0.0135  0.0135\n",
       "  0.0225  0.0361  0.0361\n",
       "  0.0271  0.0361  0.0361\n",
       "\n",
       "(17,2 ,.,.) = \n",
       " -0.0045 -0.0000  0.0000\n",
       "  0.0045  0.0135  0.0180\n",
       "  0.0135  0.0180  0.0180\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.0586 -0.0451 -0.0451\n",
       " -0.0541 -0.0541 -0.0451\n",
       " -0.0631 -0.0676 -0.0586\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  0.0812  0.0902  0.0812\n",
       "  0.1037  0.1082  0.0992\n",
       "  0.0812  0.0812  0.0767\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.0586 -0.0406 -0.0180\n",
       " -0.0451 -0.0541 -0.0271\n",
       " -0.0361 -0.0631 -0.0541\n",
       "\n",
       "(19,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.0045\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.0000 -0.0045 -0.0000\n",
       " -0.0000 -0.0045 -0.0045\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(20,0 ,.,.) = \n",
       " -0.0045 -0.0045  0.0000\n",
       " -0.0000 -0.0045 -0.0000\n",
       " -0.0045 -0.0000 -0.0045\n",
       "\n",
       "(20,1 ,.,.) = \n",
       " -0.0000  0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0045 -0.0000  0.0045\n",
       "\n",
       "(20,2 ,.,.) = \n",
       " -0.0045 -0.0000 -0.0045\n",
       "  0.0045 -0.0000 -0.0000\n",
       " -0.0045  0.0000  0.0000\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0045  0.0135  0.0135\n",
       "  0.0045  0.0135  0.0180\n",
       "  0.0045  0.0135  0.0180\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.0135  0.0180  0.0180\n",
       "  0.0135  0.0180  0.0180\n",
       "  0.0000  0.0045  0.0135\n",
       "\n",
       "(21,2 ,.,.) = \n",
       "  0.0000  0.0045  0.0045\n",
       "  0.0000  0.0135  0.0135\n",
       " -0.0045  0.0045  0.0135\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.0812  0.0225  0.0722\n",
       "  0.0676  0.0135  0.0722\n",
       "  0.0631 -0.0135  0.0812\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.0812 -0.0361  0.0586\n",
       " -0.0045 -0.1082  0.0180\n",
       " -0.0406 -0.1443  0.0225\n",
       "\n",
       "(22,2 ,.,.) = \n",
       " -0.0135 -0.1127  0.0135\n",
       " -0.1263 -0.2165 -0.0361\n",
       " -0.1488 -0.2165 -0.0225\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.0496  0.0676  0.0586\n",
       "  0.1218  0.1263  0.0676\n",
       "  0.0812  0.1488  0.1037\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.2751 -0.3202 -0.2931\n",
       " -0.3157 -0.3562 -0.3608\n",
       " -0.3202 -0.2751 -0.2570\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.2390  0.2119  0.1759\n",
       "  0.2119  0.2029  0.1668\n",
       "  0.1398  0.2029  0.1894\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0045 -0.0045 -0.0000\n",
       "  0.0045  0.0045  0.0000\n",
       " -0.0045 -0.0045 -0.0045\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0045  0.0000 -0.0000\n",
       " -0.0000 -0.0045 -0.0045\n",
       " -0.0000 -0.0000  0.0000\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       " -0.0045  0.0000  0.0045\n",
       " -0.0045  0.0045 -0.0045\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0045  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,2 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.1398  0.1443  0.0496\n",
       "  0.1939  0.2255  0.1533\n",
       "  0.1488  0.1804  0.1533\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.0676 -0.0767 -0.0767\n",
       " -0.0902 -0.0992 -0.0902\n",
       " -0.0767 -0.0992 -0.0812\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.0767 -0.0586 -0.0000\n",
       " -0.1037 -0.1263 -0.0676\n",
       " -0.0496 -0.0902 -0.0451\n",
       "\n",
       "(27,0 ,.,.) = \n",
       " -0.0406 -0.1714 -0.0271\n",
       " -0.1714 -0.2931 -0.1037\n",
       " -0.0451 -0.1533 -0.0180\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0271 -0.0812  0.0496\n",
       " -0.0722 -0.1759  0.0135\n",
       "  0.0180 -0.0586  0.0857\n",
       "\n",
       "(27,2 ,.,.) = \n",
       "  0.1082  0.0045  0.0586\n",
       "  0.0496 -0.0406  0.0722\n",
       "  0.0857  0.0225  0.0812\n",
       "\n",
       "(28,0 ,.,.) = \n",
       "  0.0271  0.0767  0.0857\n",
       "  0.0225  0.0361  0.0992\n",
       "  0.0406 -0.0406  0.0406\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.1037 -0.0857 -0.0767\n",
       " -0.1263 -0.1533 -0.0857\n",
       " -0.0857 -0.2029 -0.1218\n",
       "\n",
       "(28,2 ,.,.) = \n",
       "  0.0676  0.1037  0.0676\n",
       "  0.0812  0.0857  0.0992\n",
       "  0.0992  0.0271  0.0767\n",
       "\n",
       "(29,0 ,.,.) = \n",
       "  0.0000  0.0000  0.0045\n",
       " -0.0045 -0.0000  0.0000\n",
       " -0.0045 -0.0045 -0.0000\n",
       "\n",
       "(29,1 ,.,.) = \n",
       " -0.0045 -0.0045  0.0000\n",
       " -0.0045 -0.0045 -0.0000\n",
       " -0.0045 -0.0045 -0.0000\n",
       "\n",
       "(29,2 ,.,.) = \n",
       " -0.0000  0.0000  0.0000\n",
       " -0.0000 -0.0000  0.0000\n",
       " -0.0045 -0.0000  0.0000\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.0947  0.1172 -0.0361\n",
       "  0.0135  0.1714 -0.0045\n",
       " -0.0135  0.1714 -0.1263\n",
       "\n",
       "(30,1 ,.,.) = \n",
       " -0.0676  0.1308  0.0180\n",
       "  0.0180  0.2255  0.0631\n",
       " -0.0180  0.1849 -0.1127\n",
       "\n",
       "(30,2 ,.,.) = \n",
       " -0.0812  0.1263 -0.0045\n",
       "  0.0180  0.1488 -0.0045\n",
       " -0.0135  0.1353 -0.0947\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0000 -0.0045 -0.0045\n",
       " -0.0045 -0.0180 -0.0090\n",
       " -0.0045 -0.0000 -0.0090\n",
       "\n",
       "(31,1 ,.,.) = \n",
       " -0.0045  0.0045  0.0045\n",
       " -0.0045 -0.0045 -0.0045\n",
       "  0.0000 -0.0045  0.0000\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0000 -0.0045 -0.0000\n",
       " -0.0045 -0.0045  0.0000\n",
       "  0.0000 -0.0045 -0.0000\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model.model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0000  0.0838  0.0838\n",
       "  0.0838  0.0838  0.0838\n",
       "  0.0000  0.0838  0.0838\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0838 -0.0838 -0.0000\n",
       " -0.0838 -0.0000 -0.0838\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  0.0000  0.0838  0.0000\n",
       "  0.0000  0.0000  0.0838\n",
       "  0.0000  0.0838  0.0838\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.1329  0.1994 -0.1662\n",
       " -0.0997 -0.2991 -0.2991\n",
       " -0.2659 -0.2991 -0.0000\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.2991  0.1329 -0.1994\n",
       " -0.2991 -0.6314 -0.5317\n",
       " -0.3656 -0.3656 -0.0000\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.2991  0.1662 -0.1994\n",
       " -0.2659 -0.2991 -0.3323\n",
       " -0.2991 -0.1994  0.0332\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       " -0.0000 -0.0540 -0.0000\n",
       " -0.0000 -0.0180 -0.0180\n",
       " -0.0180  0.0180 -0.0180\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0540 -0.0180 -0.0000\n",
       " -0.0540 -0.0000 -0.0180\n",
       "  0.0000  0.0000  0.0180\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.0180 -0.0540  0.0000\n",
       " -0.0000 -0.0000 -0.0180\n",
       " -0.0180  0.0180  0.0180\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       "  0.0076 -0.0228 -0.0076\n",
       "  0.0076 -0.0076  0.0076\n",
       "  0.0000 -0.0076 -0.0076\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0076 -0.0076 -0.0000\n",
       "  0.0228 -0.0076  0.0000\n",
       "  0.0228  0.0228  0.0228\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       "  0.0228 -0.0076  0.0228\n",
       "  0.0076  0.0228  0.0228\n",
       "  0.0228  0.0228  0.0228\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.7093 -0.7638 -0.2182\n",
       "  1.2367  1.0003  0.3455\n",
       " -0.2910 -0.3819  0.0909\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       " -1.2003 -1.1639 -0.4547\n",
       "  1.7641  1.6914  0.6547\n",
       " -0.2910 -0.4910 -0.0364\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       " -0.1273 -0.2364  0.0182\n",
       "  0.8730  0.7093  0.3092\n",
       " -0.2364 -0.3092  0.1455\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       " -0.0226 -0.0226  0.0000\n",
       " -0.0226 -0.0000 -0.0226\n",
       " -0.0000 -0.0226 -0.0226\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       "  0.0000 -0.0226 -0.0226\n",
       "  0.0000 -0.0000  0.0000\n",
       " -0.0226 -0.0226  0.0000\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0226 -0.0000 -0.0226\n",
       " -0.0000  0.0000 -0.0226\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.1238  0.7921  0.2228\n",
       "  0.4703  1.2625  0.4703\n",
       "  0.7179  1.0644  0.1485\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.2228  0.0743 -0.2228\n",
       " -0.4208 -0.0248 -0.2475\n",
       " -0.1980 -0.0990 -0.3466\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.1238  0.0743 -0.1238\n",
       " -0.6931 -0.4456 -0.3713\n",
       " -0.4951 -0.4208 -0.3466\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       "  0.0058  0.0174 -0.0233\n",
       " -0.0174 -0.0000 -0.0233\n",
       " -0.0174  0.0058  0.0058\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.0174  0.0174 -0.0174\n",
       "  0.0174  0.0058  0.0291\n",
       "  0.0291  0.0174  0.0291\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       " -0.0058  0.0058  0.0058\n",
       " -0.0291  0.0174 -0.0000\n",
       " -0.0174 -0.0000  0.0058\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.2433 -0.1757 -0.4326\n",
       " -0.5948 -0.5002 -0.7165\n",
       " -0.4056 -0.2568 -0.5813\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.1217  0.4596  0.1893\n",
       "  0.0406  0.4056  0.0811\n",
       "  0.1487  0.4461 -0.0135\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.1352  0.5678  0.2028\n",
       "  0.3380  0.7300  0.2974\n",
       "  0.3244  0.6354  0.0811\n",
       "\n",
       "(10,0 ,.,.) = \n",
       "  0.2705  0.3907  0.4207\n",
       "  0.2705  0.3306  0.3907\n",
       "  0.2705  0.3005  0.3306\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.0301  0.0301  0.0902\n",
       " -0.0902 -0.0902  0.0000\n",
       " -0.0301 -0.1202 -0.0902\n",
       "\n",
       "(10,2 ,.,.) = \n",
       " -0.2404 -0.2404 -0.1503\n",
       " -0.3005 -0.3907 -0.2705\n",
       " -0.2404 -0.3907 -0.3306\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.2536  0.1268  0.0951\n",
       "  0.2853  0.1902  0.1268\n",
       "  0.3170  0.2536  0.1585\n",
       "\n",
       "(11,1 ,.,.) = \n",
       " -0.0951 -0.1585 -0.2853\n",
       " -0.0317 -0.1268 -0.2536\n",
       " -0.0000 -0.0951 -0.1902\n",
       "\n",
       "(11,2 ,.,.) = \n",
       " -0.1585 -0.1902 -0.1902\n",
       " -0.0951 -0.1585 -0.2536\n",
       " -0.0951 -0.1268 -0.1902\n",
       "\n",
       "(12,0 ,.,.) = \n",
       "  0.2250  0.3068 -0.4704\n",
       "  0.0205  0.4909 -0.0614\n",
       " -0.4704 -0.0205  0.2045\n",
       "\n",
       "(12,1 ,.,.) = \n",
       "  0.4704  0.6341 -0.2045\n",
       "  0.2659  0.7977  0.2864\n",
       " -0.3068  0.0818  0.4091\n",
       "\n",
       "(12,2 ,.,.) = \n",
       "  0.1841  0.3273 -0.1227\n",
       "  0.0205  0.3886  0.1636\n",
       " -0.3682 -0.0614  0.3273\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0760  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(14,0 ,.,.) = \n",
       "  0.2159  0.2159  0.0000\n",
       "  0.2159  0.2159  0.0000\n",
       "  0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,1 ,.,.) = \n",
       "  0.2159  0.2159  0.0000\n",
       "  0.0000  0.2159  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.0000  0.2159  0.0000\n",
       " -0.0000  0.0000 -0.0000\n",
       " -0.2159 -0.0000 -0.0000\n",
       "\n",
       "(15,0 ,.,.) = \n",
       "  0.1760 -0.0271 -0.1941\n",
       " -0.2257  0.5777  0.5777\n",
       "  0.0542 -0.5732 -0.5732\n",
       "\n",
       "(15,1 ,.,.) = \n",
       " -0.1805  0.0632  0.1580\n",
       " -0.0406  0.5777  0.5777\n",
       "  0.2167 -0.5732 -0.5732\n",
       "\n",
       "(15,2 ,.,.) = \n",
       "  0.2798 -0.0812 -0.2347\n",
       " -0.1941  0.4243  0.5777\n",
       "  0.0722 -0.4107 -0.5732\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0000  0.2458 -0.2070\n",
       "  0.2975  0.2070 -0.6726\n",
       "  0.2716  0.1423 -0.7373\n",
       "\n",
       "(16,1 ,.,.) = \n",
       "  0.1164  0.4398 -0.3492\n",
       "  0.6985  0.1940 -1.3194\n",
       "  0.4786  0.1293 -1.1900\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.1164  0.2975 -0.0129\n",
       "  0.1940  0.2975 -0.4398\n",
       " -0.1940  0.1035 -0.4269\n",
       "\n",
       "(17,0 ,.,.) = \n",
       " -0.0000  0.0280  0.0280\n",
       "  0.1401  0.1401  0.1401\n",
       "  0.1681  0.1681  0.1681\n",
       "\n",
       "(17,1 ,.,.) = \n",
       "  0.0280  0.0840  0.0840\n",
       "  0.1401  0.2241  0.2241\n",
       "  0.1681  0.2241  0.2241\n",
       "\n",
       "(17,2 ,.,.) = \n",
       " -0.0280 -0.0000  0.0000\n",
       "  0.0280  0.0840  0.1120\n",
       "  0.0840  0.1120  0.1120\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.7227 -0.5560 -0.5560\n",
       " -0.6671 -0.6671 -0.5560\n",
       " -0.7783 -0.8339 -0.7227\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  1.0007  1.1119  1.0007\n",
       "  1.2787  1.3343  1.2231\n",
       "  1.0007  1.0007  0.9451\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.7227 -0.5004 -0.2224\n",
       " -0.5560 -0.6671 -0.3336\n",
       " -0.4448 -0.7783 -0.6671\n",
       "\n",
       "(19,0 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.1031\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.0000 -0.1031 -0.0000\n",
       " -0.0000 -0.1031 -0.1031\n",
       " -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0000 -0.0000  0.0000\n",
       "\n",
       "(20,0 ,.,.) = \n",
       "  0.0278  0.0278 -0.0000\n",
       "  0.0000  0.0278  0.0000\n",
       "  0.0278  0.0000  0.0278\n",
       "\n",
       "(20,1 ,.,.) = \n",
       "  0.0000 -0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0278  0.0000 -0.0278\n",
       "\n",
       "(20,2 ,.,.) = \n",
       "  0.0278  0.0000  0.0278\n",
       " -0.0278  0.0000  0.0000\n",
       "  0.0278 -0.0000 -0.0000\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0715  0.2144  0.2144\n",
       "  0.0715  0.2144  0.2859\n",
       "  0.0715  0.2144  0.2859\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.2144  0.2859  0.2859\n",
       "  0.2144  0.2859  0.2859\n",
       "  0.0000  0.0715  0.2144\n",
       "\n",
       "(21,2 ,.,.) = \n",
       "  0.0000  0.0715  0.0715\n",
       "  0.0000  0.2144  0.2144\n",
       " -0.0715  0.0715  0.2144\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.2948  0.0819  0.2620\n",
       "  0.2457  0.0491  0.2620\n",
       "  0.2293 -0.0491  0.2948\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.2948 -0.1310  0.2129\n",
       " -0.0164 -0.3931  0.0655\n",
       " -0.1474 -0.5241  0.0819\n",
       "\n",
       "(22,2 ,.,.) = \n",
       " -0.0491 -0.4095  0.0491\n",
       " -0.4586 -0.7861 -0.1310\n",
       " -0.5405 -0.7861 -0.0819\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.1562  0.2130  0.1846\n",
       "  0.3835  0.3977  0.2130\n",
       "  0.2556  0.4687  0.3266\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.8663 -1.0083 -0.9231\n",
       " -0.9941 -1.1220 -1.1362\n",
       " -1.0083 -0.8663 -0.8095\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.7527  0.6675  0.5539\n",
       "  0.6675  0.6391  0.5255\n",
       "  0.4403  0.6391  0.5965\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0173 -0.0173 -0.0000\n",
       "  0.0173  0.0173  0.0000\n",
       " -0.0173 -0.0173 -0.0173\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0173  0.0000 -0.0000\n",
       " -0.0000 -0.0173 -0.0173\n",
       " -0.0000 -0.0000  0.0000\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0000 -0.0000  0.0000\n",
       " -0.0173  0.0000  0.0173\n",
       " -0.0173  0.0173 -0.0173\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.1789  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,1 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(25,2 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.8173  0.8437  0.2900\n",
       "  1.1337  1.3183  0.8964\n",
       "  0.8701  1.0546  0.8964\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.3955 -0.4482 -0.4482\n",
       " -0.5273 -0.5800 -0.5273\n",
       " -0.4482 -0.5800 -0.4746\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.4482 -0.3427 -0.0000\n",
       " -0.6064 -0.7382 -0.3955\n",
       " -0.2900 -0.5273 -0.2637\n",
       "\n",
       "(27,0 ,.,.) = \n",
       " -0.1273 -0.5376 -0.0849\n",
       " -0.5376 -0.9195 -0.3254\n",
       " -0.1415 -0.4810 -0.0566\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0849 -0.2546  0.1556\n",
       " -0.2263 -0.5517  0.0424\n",
       "  0.0566 -0.1839  0.2688\n",
       "\n",
       "(27,2 ,.,.) = \n",
       "  0.3395  0.0141  0.1839\n",
       "  0.1556 -0.1273  0.2263\n",
       "  0.2688  0.0707  0.2546\n",
       "\n",
       "(28,0 ,.,.) = \n",
       "  0.1519  0.4304  0.4810\n",
       "  0.1266  0.2025  0.5569\n",
       "  0.2278 -0.2278  0.2278\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.5823 -0.4810 -0.4304\n",
       " -0.7088 -0.8607 -0.4810\n",
       " -0.4810 -1.1392 -0.6835\n",
       "\n",
       "(28,2 ,.,.) = \n",
       "  0.3797  0.5823  0.3797\n",
       "  0.4557  0.4810  0.5569\n",
       "  0.5569  0.1519  0.4304\n",
       "\n",
       "(29,0 ,.,.) = \n",
       " -0.0000 -0.0000 -0.0830\n",
       "  0.0830  0.0000 -0.0000\n",
       "  0.0830  0.0830  0.0000\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "  0.0830  0.0830 -0.0000\n",
       "  0.0830  0.0830  0.0000\n",
       "  0.0830  0.0830  0.0000\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "  0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000\n",
       "  0.0830  0.0000 -0.0000\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.1820  0.2254 -0.0693\n",
       "  0.0260  0.3294 -0.0087\n",
       " -0.0260  0.3294 -0.2427\n",
       "\n",
       "(30,1 ,.,.) = \n",
       " -0.1300  0.2514  0.0347\n",
       "  0.0347  0.4334  0.1214\n",
       " -0.0347  0.3554 -0.2167\n",
       "\n",
       "(30,2 ,.,.) = \n",
       " -0.1560  0.2427 -0.0087\n",
       "  0.0347  0.2861 -0.0087\n",
       " -0.0260  0.2601 -0.1820\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0000 -0.0152 -0.0152\n",
       " -0.0152 -0.0608 -0.0304\n",
       " -0.0152 -0.0000 -0.0304\n",
       "\n",
       "(31,1 ,.,.) = \n",
       " -0.0152  0.0152  0.0152\n",
       " -0.0152 -0.0152 -0.0152\n",
       "  0.0000 -0.0152  0.0000\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0000 -0.0152 -0.0000\n",
       " -0.0152 -0.0152  0.0000\n",
       "  0.0000 -0.0152 -0.0000\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0][0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.FloatTensor' object has no attribute 'zeros'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3dcf64850002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.FloatTensor' object has no attribute 'zeros'"
     ]
    }
   ],
   "source": [
    "cc.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-476d1b94c172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clip' is not defined"
     ]
    }
   ],
   "source": [
    "torch.clamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  2.5504e+00  2.9247e+00  2.8491e+00  ...   1.3634e+00  2.1798e+00  2.3784e+00\n",
       "  2.5581e+00  2.8316e+00  2.8244e+00  ...   1.3436e+00  1.9383e+00  2.1978e+00\n",
       "  2.5185e+00  2.7827e+00  2.7916e+00  ...   1.1708e+00  1.3936e+00  1.2473e+00\n",
       "                 ...                                      ...                \n",
       "  1.4324e+00  1.1519e+00  1.1440e+00  ...   2.3858e+00  2.6389e+00  2.6338e+00\n",
       "  1.2796e+00  1.0198e+00  9.3218e-01  ...   2.3512e+00  2.3562e+00  2.3163e+00\n",
       "  1.3625e+00  1.1382e+00  1.0658e+00  ...   2.4910e+00  2.4283e+00  2.4607e+00\n",
       "    ... \n",
       "\n",
       "( 0 ,29 ,.,.) = \n",
       " -3.9294e-01 -3.1780e-01 -3.2797e-01  ...  -4.4508e-01 -4.1689e-01 -3.8600e-01\n",
       " -3.9389e-01 -3.2135e-01 -3.1451e-01  ...  -4.6912e-01 -4.4305e-01 -4.1408e-01\n",
       " -3.9460e-01 -3.2656e-01 -3.1425e-01  ...  -4.6941e-01 -4.8057e-01 -4.7490e-01\n",
       "                 ...                                      ...                \n",
       " -4.2900e-01 -4.8440e-01 -4.8981e-01  ...  -3.7408e-01 -3.5938e-01 -3.3601e-01\n",
       " -4.2923e-01 -4.8513e-01 -4.9904e-01  ...  -3.5546e-01 -4.0375e-01 -3.6996e-01\n",
       " -4.2520e-01 -4.7567e-01 -4.8224e-01  ...  -3.4522e-01 -3.8132e-01 -3.7620e-01\n",
       "\n",
       "( 0 ,30 ,.,.) = \n",
       " -7.2077e-01 -9.8162e-01 -5.0869e-01  ...   1.6688e+00 -2.7374e-01 -2.6053e-01\n",
       " -1.1289e+00 -1.0253e+00 -6.9074e-01  ...   2.0373e+00 -1.2818e-01 -3.0190e-02\n",
       " -1.0781e+00 -8.5870e-01 -1.0816e+00  ...   2.0079e+00  6.1596e-01  1.5991e+00\n",
       "                 ...                                      ...                \n",
       "  1.5206e+00  1.3369e+00  1.4081e+00  ...  -3.3409e-01 -6.2011e-01 -8.2409e-01\n",
       "  1.8912e+00  1.7455e+00  1.8824e+00  ...  -1.0218e-01 -6.2166e-01 -4.0202e-01\n",
       "  1.7334e+00  1.5949e+00  1.7162e+00  ...  -6.4006e-01 -2.4545e-01 -3.2626e-01\n",
       "\n",
       "( 0 ,31 ,.,.) = \n",
       "  1.2349e-01  1.6336e-01  1.5356e-01  ...  -4.0085e-02  7.4508e-02  1.0540e-01\n",
       "  1.2065e-01  1.6922e-01  1.4449e-01  ...  -6.9298e-02  4.8290e-02  7.5447e-02\n",
       "  1.1660e-01  1.5824e-01  1.5772e-01  ...  -1.1698e-01 -5.7831e-02 -7.2172e-02\n",
       "                 ...                                      ...                \n",
       " -1.1397e-01 -1.5083e-01 -1.5543e-01  ...   1.1547e-01  1.2634e-01  1.1947e-01\n",
       " -1.2286e-01 -1.7222e-01 -1.9095e-01  ...   6.9490e-02  1.1199e-01  1.0913e-01\n",
       " -1.1504e-01 -1.6062e-01 -1.7840e-01  ...   1.2450e-01  9.7319e-02  1.1543e-01\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  2.1652e+00  2.2983e+00  2.2912e+00  ...   2.4805e+00  2.5144e+00  2.5117e+00\n",
       "  2.1053e+00  2.1079e+00  2.0917e+00  ...   2.3739e+00  2.3881e+00  2.4226e+00\n",
       "  2.0647e+00  2.0649e+00  2.0259e+00  ...   2.3347e+00  2.3915e+00  2.4468e+00\n",
       "                 ...                                      ...                \n",
       "  1.2073e+00  9.7454e-01  1.0287e+00  ...   1.7552e+00  1.5021e+00  1.5719e+00\n",
       "  1.2190e+00  9.8558e-01  9.7137e-01  ...   1.9867e+00  1.7107e+00  1.6225e+00\n",
       "  1.2294e+00  9.4958e-01  9.2410e-01  ...   2.2029e+00  1.9933e+00  1.8875e+00\n",
       "    ... \n",
       "\n",
       "( 1 ,29 ,.,.) = \n",
       " -4.0551e-01 -3.8159e-01 -3.8464e-01  ...  -3.6606e-01 -3.6252e-01 -3.6064e-01\n",
       " -4.0693e-01 -3.9337e-01 -3.9808e-01  ...  -3.6676e-01 -3.6630e-01 -3.6205e-01\n",
       " -4.0859e-01 -3.9647e-01 -4.0212e-01  ...  -3.7311e-01 -3.6770e-01 -3.6227e-01\n",
       "                 ...                                      ...                \n",
       " -4.3327e-01 -4.9934e-01 -4.9838e-01  ...  -4.0645e-01 -4.4181e-01 -4.5547e-01\n",
       " -4.3279e-01 -4.9839e-01 -5.0357e-01  ...  -3.8711e-01 -4.1968e-01 -4.4275e-01\n",
       " -4.3303e-01 -4.9839e-01 -5.0617e-01  ...  -3.7574e-01 -3.9323e-01 -4.1493e-01\n",
       "\n",
       "( 1 ,30 ,.,.) = \n",
       " -5.3574e-02 -6.9420e-02 -3.7330e-02  ...  -2.7473e-01 -3.1808e-01 -3.3157e-01\n",
       " -1.6205e-01  4.7503e-02  8.3373e-02  ...  -3.1430e-01 -3.5270e-01 -4.0825e-01\n",
       " -7.6118e-02  1.3931e-01  1.9085e-01  ...  -2.5542e-01 -3.3549e-01 -4.4369e-01\n",
       "                 ...                                      ...                \n",
       "  2.0368e+00  1.8447e+00  1.6947e+00  ...   5.4610e-01  1.0388e+00  9.9141e-01\n",
       "  2.0761e+00  1.8284e+00  1.7804e+00  ...   3.2040e-01  8.0358e-01  9.8177e-01\n",
       "  2.0537e+00  1.8634e+00  1.8729e+00  ...  -7.9313e-02  3.3124e-01  5.8786e-01\n",
       "\n",
       "( 1 ,31 ,.,.) = \n",
       "  3.4404e-02  3.6159e-02  3.6809e-02  ...   7.2958e-02  7.8270e-02  7.7025e-02\n",
       "  1.9428e-02  1.3376e-02  9.6745e-03  ...   6.5767e-02  6.8895e-02  7.4208e-02\n",
       "  1.5038e-02  7.1684e-03 -1.2726e-03  ...   6.2950e-02  6.8573e-02  7.7942e-02\n",
       "                 ...                                      ...                \n",
       " -1.2820e-01 -1.6600e-01 -1.6282e-01  ...  -3.1952e-02 -7.6030e-02 -6.5094e-02\n",
       " -1.2849e-01 -1.6817e-01 -1.6721e-01  ...  -2.5948e-03 -5.5375e-02 -6.6666e-02\n",
       " -1.2755e-01 -1.7098e-01 -1.7440e-01  ...   4.1455e-02 -1.3279e-03 -2.7268e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  2.2496e+00  2.3510e+00  2.3291e+00  ...   6.7004e-01  1.1983e+00  1.4919e+00\n",
       "  2.1008e+00  2.2463e+00  2.2566e+00  ...   1.1697e+00  1.1578e+00  1.5957e+00\n",
       "  2.1304e+00  2.1689e+00  2.3702e+00  ...   1.6473e+00  1.5865e+00  1.2911e+00\n",
       "                 ...                                      ...                \n",
       "  1.4625e+00  1.2392e+00  1.2121e+00  ...   9.7569e-01  9.5290e-01  9.8694e-01\n",
       "  1.4615e+00  1.2611e+00  1.2429e+00  ...   9.7201e-01  9.5837e-01  9.7155e-01\n",
       "  1.3139e+00  1.1112e+00  1.1468e+00  ...   1.0134e+00  9.8011e-01  1.0025e+00\n",
       "    ... \n",
       "\n",
       "( 2 ,29 ,.,.) = \n",
       " -4.0646e-01 -3.7256e-01 -3.7631e-01  ...  -5.2206e-01 -4.7631e-01 -4.4726e-01\n",
       " -4.1547e-01 -3.7634e-01 -3.6733e-01  ...  -4.8224e-01 -4.9611e-01 -4.4367e-01\n",
       " -4.1476e-01 -4.0899e-01 -3.6123e-01  ...  -4.2125e-01 -4.7022e-01 -4.7136e-01\n",
       "                 ...                                      ...                \n",
       " -4.2781e-01 -4.7723e-01 -4.8500e-01  ...  -5.0650e-01 -5.0814e-01 -5.0908e-01\n",
       " -4.3066e-01 -4.8334e-01 -4.8499e-01  ...  -5.0837e-01 -5.0648e-01 -5.1121e-01\n",
       " -4.3279e-01 -4.9444e-01 -4.9186e-01  ...  -5.0084e-01 -5.0626e-01 -5.0577e-01\n",
       "\n",
       "( 2 ,30 ,.,.) = \n",
       " -1.2701e-01 -6.9721e-02 -5.7072e-01  ...   1.9633e+00  1.0849e+00  5.8860e-01\n",
       "  1.4136e-01  1.3164e-01 -5.2330e-01  ...   1.3851e+00  1.4043e+00  7.0251e-01\n",
       "  6.5687e-02  1.1454e-01 -5.9608e-01  ...   4.9523e-01  9.0405e-01  1.5742e+00\n",
       "                 ...                                      ...                \n",
       "  1.4688e+00  1.3851e+00  1.3727e+00  ...   1.8729e+00  1.8840e+00  1.8327e+00\n",
       "  1.4141e+00  1.3880e+00  1.4260e+00  ...   1.8611e+00  1.8451e+00  1.8601e+00\n",
       "  1.7501e+00  1.6533e+00  1.5474e+00  ...   1.8275e+00  1.8545e+00  1.8574e+00\n",
       "\n",
       "( 2 ,31 ,.,.) = \n",
       "  7.2959e-02  8.0582e-02  5.4170e-02  ...  -1.4746e-01 -9.5906e-02 -4.6827e-02\n",
       "  5.7294e-02  7.1486e-02  8.1921e-02  ...  -1.3144e-01 -8.5359e-02 -2.5183e-02\n",
       "  2.9537e-02  8.6823e-02  8.5989e-02  ...  -6.5810e-02 -5.3646e-02 -8.8732e-02\n",
       "                 ...                                      ...                \n",
       " -7.7768e-02 -1.1186e-01 -1.2056e-01  ...  -1.5721e-01 -1.5752e-01 -1.5003e-01\n",
       " -7.8057e-02 -1.0682e-01 -1.1154e-01  ...  -1.5065e-01 -1.5533e-01 -1.5189e-01\n",
       " -1.0247e-01 -1.2873e-01 -1.2591e-01  ...  -1.5254e-01 -1.5253e-01 -1.5129e-01\n",
       "...     \n",
       "        \n",
       "\n",
       "(61 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "(61 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "(61 , 2 ,.,.) = \n",
       "  1.5708e+00  1.3416e+00  1.4072e+00  ...   2.2121e+00  2.4150e+00  2.3901e+00\n",
       "  1.6689e+00  1.6019e+00  1.6494e+00  ...   1.6353e+00  2.2411e+00  2.2824e+00\n",
       "  1.8206e+00  1.7893e+00  1.7690e+00  ...   1.0356e+00  1.7503e+00  2.1911e+00\n",
       "                 ...                                      ...                \n",
       "  1.6527e+00  1.5038e+00  1.4176e+00  ...   2.6727e+00  2.8532e+00  2.8119e+00\n",
       "  1.7102e+00  1.5718e+00  1.4929e+00  ...   2.8211e+00  2.8511e+00  2.7318e+00\n",
       "  1.7792e+00  1.6739e+00  1.5890e+00  ...   2.8234e+00  2.7951e+00  2.7456e+00\n",
       "    ... \n",
       "\n",
       "(61 ,29 ,.,.) = \n",
       " -4.2235e-01 -4.5395e-01 -4.5087e-01  ...  -4.0272e-01 -3.6956e-01 -3.6676e-01\n",
       " -4.1832e-01 -4.3554e-01 -4.3574e-01  ...  -4.8388e-01 -3.9165e-01 -3.7216e-01\n",
       " -4.1358e-01 -4.1712e-01 -4.2181e-01  ...  -5.3716e-01 -4.6307e-01 -3.8769e-01\n",
       "                 ...                                      ...                \n",
       " -4.2045e-01 -4.4563e-01 -4.5413e-01  ...  -3.4084e-01 -3.2224e-01 -3.1943e-01\n",
       " -4.1761e-01 -4.3549e-01 -4.4894e-01  ...  -3.2577e-01 -3.2152e-01 -3.2012e-01\n",
       " -4.1547e-01 -4.2840e-01 -4.3877e-01  ...  -3.1753e-01 -3.2366e-01 -3.2224e-01\n",
       "\n",
       "(61 ,30 ,.,.) = \n",
       "  9.3591e-01  1.1438e+00  1.0571e+00  ...  -4.0473e-02 -2.0707e-01 -1.8155e-01\n",
       "  9.5338e-01  9.9405e-01  8.9433e-01  ...   5.6920e-01 -2.6011e-01 -1.8759e-01\n",
       "  6.2776e-01  6.5024e-01  7.1498e-01  ...   1.7962e+00  3.5022e-01 -7.1436e-02\n",
       "                 ...                                      ...                \n",
       "  9.8077e-01  1.0403e+00  1.2054e+00  ...  -7.7697e-01 -1.0226e+00 -9.9059e-01\n",
       "  8.3786e-01  9.3452e-01  1.1210e+00  ...  -1.0030e+00 -1.0667e+00 -8.2549e-01\n",
       "  6.5009e-01  7.8957e-01  9.6934e-01  ...  -1.0333e+00 -9.6132e-01 -9.6053e-01\n",
       "\n",
       "(61 ,31 ,.,.) = \n",
       " -4.9593e-02 -6.6271e-02 -5.7191e-02  ...   4.2072e-02  5.8598e-02  5.0152e-02\n",
       " -2.9905e-02 -4.4387e-02 -4.2143e-02  ...  -2.2878e-02  5.2058e-02  4.9846e-02\n",
       " -7.4161e-03 -1.7486e-02 -2.6517e-02  ...  -1.5162e-01 -8.1797e-03  3.5170e-02\n",
       "                 ...                                      ...                \n",
       " -4.2314e-02 -6.3009e-02 -7.7385e-02  ...   1.3060e-01  1.5214e-01  1.4654e-01\n",
       " -3.1372e-02 -5.4285e-02 -6.8344e-02  ...   1.4279e-01  1.5746e-01  1.3684e-01\n",
       " -2.2609e-02 -4.2060e-02 -5.4229e-02  ...   1.4903e-01  1.4809e-01  1.3313e-01\n",
       "        \n",
       "\n",
       "(62 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "(62 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "(62 , 2 ,.,.) = \n",
       "  2.0530e+00  2.1560e+00  2.1500e+00  ...   2.3509e+00  2.3618e+00  2.3384e+00\n",
       "  2.0574e+00  2.1166e+00  2.1205e+00  ...   2.3176e+00  2.2913e+00  2.2815e+00\n",
       "  2.0617e+00  2.1111e+00  2.1124e+00  ...   2.3138e+00  2.3151e+00  2.3036e+00\n",
       "                 ...                                      ...                \n",
       "  1.9062e+00  2.0593e+00  2.1218e+00  ...   2.1058e+00  2.1276e+00  2.0752e+00\n",
       "  1.8466e+00  1.8920e+00  1.9952e+00  ...   1.8102e+00  1.7463e+00  1.9646e+00\n",
       "  1.7386e+00  1.7690e+00  1.8136e+00  ...   1.9423e+00  2.2012e+00  2.1660e+00\n",
       "    ... \n",
       "\n",
       "(62 ,29 ,.,.) = \n",
       " -4.0907e-01 -4.0020e-01 -4.0020e-01  ...  -3.7974e-01 -3.7551e-01 -3.7786e-01\n",
       " -4.0907e-01 -4.0089e-01 -4.0066e-01  ...  -3.7741e-01 -3.7669e-01 -3.7905e-01\n",
       " -4.0907e-01 -4.0020e-01 -4.0044e-01  ...  -3.7644e-01 -3.7597e-01 -3.7739e-01\n",
       "                 ...                                      ...                \n",
       " -4.1358e-01 -4.0963e-01 -4.0113e-01  ...  -4.0256e-01 -3.9837e-01 -4.0520e-01\n",
       " -4.1761e-01 -4.3032e-01 -4.1690e-01  ...  -4.2595e-01 -4.3366e-01 -4.1769e-01\n",
       " -4.1832e-01 -4.3626e-01 -4.3155e-01  ...  -4.1351e-01 -3.9675e-01 -3.9319e-01\n",
       "\n",
       "(62 ,30 ,.,.) = \n",
       "  1.4599e-01  1.3789e-01  1.3259e-01  ...  -1.0759e-01 -1.1539e-01 -1.0290e-01\n",
       " -1.1982e-02  5.8786e-02  4.8112e-02  ...  -2.0992e-01 -1.7741e-01 -1.4798e-01\n",
       " -2.7444e-02  6.0444e-02  3.3983e-02  ...  -2.3328e-01 -2.1408e-01 -1.8571e-01\n",
       "                 ...                                      ...                \n",
       "  5.2489e-01  2.0243e-01  3.6006e-02  ...   4.5301e-02 -6.0187e-03  9.6876e-02\n",
       "  5.6642e-01  3.5019e-01  1.6788e-01  ...   4.4052e-01  5.5325e-01  3.6909e-01\n",
       "  8.3402e-01  5.8485e-01  4.6268e-01  ...   5.6204e-01  5.3625e-02  7.6164e-02\n",
       "\n",
       "(62 ,31 ,.,.) = \n",
       "  5.8131e-03  4.1781e-03  3.2169e-03  ...   5.4580e-02  5.3308e-02  5.1747e-02\n",
       "  7.0524e-03  6.6456e-03  9.1572e-03  ...   5.7070e-02  5.3630e-02  5.1130e-02\n",
       "  6.7578e-03  3.2446e-03  2.6222e-03  ...   5.5820e-02  5.6453e-02  5.5814e-02\n",
       "                 ...                                      ...                \n",
       " -1.9215e-02 -1.0220e-02  5.9900e-03  ...   2.4017e-02  2.3711e-02  1.1591e-02\n",
       " -3.6031e-02 -2.4540e-02 -1.0187e-02  ...  -4.1755e-02 -3.7393e-02 -1.2420e-02\n",
       " -5.2607e-02 -5.9199e-02 -4.5762e-02  ...  -2.0305e-02  2.3117e-02  2.2761e-02\n",
       "        \n",
       "\n",
       "(63 , 0 ,.,.) = \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "                 ...                                      ...                \n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       " -2.0669e-01 -2.0669e-01 -2.0669e-01  ...  -2.0669e-01 -2.0669e-01 -2.0669e-01\n",
       "\n",
       "(63 , 1 ,.,.) = \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "                 ...                                      ...                \n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "  3.1326e-02  3.1326e-02  3.1326e-02  ...   3.1326e-02  3.1326e-02  3.1326e-02\n",
       "\n",
       "(63 , 2 ,.,.) = \n",
       "  1.4460e+00  1.2233e+00  1.2054e+00  ...   9.3768e-01  9.2162e-01  9.6274e-01\n",
       "  1.4197e+00  1.2717e+00  1.2962e+00  ...   1.1285e+00  1.2022e+00  1.1709e+00\n",
       "  1.4379e+00  1.2739e+00  1.2397e+00  ...   1.1359e+00  1.1321e+00  1.1755e+00\n",
       "                 ...                                      ...                \n",
       "  1.3002e+00  1.1626e+00  1.1957e+00  ...   2.2888e+00  2.2868e+00  2.3737e+00\n",
       "  1.2839e+00  1.1005e+00  1.1694e+00  ...   2.4097e+00  2.3781e+00  2.4555e+00\n",
       "  1.3568e+00  1.1071e+00  1.0814e+00  ...   2.4332e+00  2.4362e+00  2.5110e+00\n",
       "    ... \n",
       "\n",
       "(63 ,29 ,.,.) = \n",
       " -4.2828e-01 -4.6923e-01 -4.6853e-01  ...  -4.8784e-01 -4.9161e-01 -4.8877e-01\n",
       " -4.2947e-01 -4.6970e-01 -4.7159e-01  ...  -4.8523e-01 -4.7770e-01 -4.7984e-01\n",
       " -4.2781e-01 -4.7017e-01 -4.7348e-01  ...  -4.8855e-01 -4.8857e-01 -4.8405e-01\n",
       "                 ...                                      ...                \n",
       " -4.3327e-01 -4.8454e-01 -4.8264e-01  ...  -3.7134e-01 -3.7015e-01 -3.6638e-01\n",
       " -4.3113e-01 -4.8529e-01 -4.8433e-01  ...  -3.5743e-01 -3.6166e-01 -3.6405e-01\n",
       " -4.2947e-01 -4.8315e-01 -4.8926e-01  ...  -3.5717e-01 -3.5763e-01 -3.4964e-01\n",
       "\n",
       "(63 ,30 ,.,.) = \n",
       "  1.2247e+00  1.2233e+00  1.2452e+00  ...   1.5468e+00  1.6306e+00  1.5151e+00\n",
       "  1.6462e+00  1.3764e+00  1.3464e+00  ...   1.6173e+00  1.5193e+00  1.5568e+00\n",
       "  1.6058e+00  1.3622e+00  1.4667e+00  ...   1.5889e+00  1.5688e+00  1.4517e+00\n",
       "                 ...                                      ...                \n",
       "  1.9219e+00  1.5256e+00  1.5724e+00  ...  -1.2842e-01 -1.1948e-01 -2.1381e-01\n",
       "  1.9438e+00  1.7440e+00  1.3929e+00  ...  -3.1828e-01 -2.1845e-01 -3.8648e-01\n",
       "  1.8020e+00  1.6733e+00  1.5826e+00  ...  -3.4447e-01 -3.9936e-01 -4.4304e-01\n",
       "\n",
       "(63 ,31 ,.,.) = \n",
       " -7.6761e-02 -9.1150e-02 -9.2089e-02  ...  -1.4390e-01 -1.4360e-01 -1.3891e-01\n",
       " -7.8011e-02 -9.6445e-02 -9.3961e-02  ...  -1.2609e-01 -1.2202e-01 -1.2391e-01\n",
       " -7.7372e-02 -9.6462e-02 -1.0179e-01  ...  -1.2235e-01 -1.2048e-01 -1.1609e-01\n",
       "                 ...                                      ...                \n",
       " -1.0018e-01 -1.1766e-01 -1.1334e-01  ...   6.5890e-02  7.1236e-02  8.2466e-02\n",
       " -1.0083e-01 -1.2866e-01 -1.1797e-01  ...   8.5928e-02  8.3078e-02  9.8993e-02\n",
       " -9.3954e-02 -1.2395e-01 -1.2489e-01  ...   8.9329e-02  1.0120e-01  1.0184e-01\n",
       "[torch.cuda.FloatTensor of size 64x32x112x112 (GPU 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = nn.Sequential(model.model[0][0],model.model[0][1]) (input_var)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function FloatTensor.clone>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi gpus\n",
    "if gpus and len(gpus) > 1:\n",
    "    model_dist = torch.nn.DataParallel(model, gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_dist(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.store_and_quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.3363e+00  1.3145e+00  1.3089e+00  ...   1.3162e+00  1.3258e+00  1.3259e+00\n",
       "  1.3862e+00  1.3369e+00  1.3300e+00  ...   1.3595e+00  1.3505e+00  1.3573e+00\n",
       "  1.3731e+00  1.3275e+00  1.3182e+00  ...   1.3518e+00  1.3675e+00  1.3635e+00\n",
       "                 ...                                      ...                \n",
       "  9.3571e-01  9.7869e-01  8.4257e-01  ...   8.7363e-01  8.7672e-01  8.8933e-01\n",
       "  9.8799e-01  8.2247e-01  6.9976e-01  ...   8.5056e-01  8.6704e-01  8.7351e-01\n",
       "  8.6465e-01  7.6496e-01  7.3176e-01  ...   8.5165e-01  8.8781e-01  8.9473e-01\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  2.3497e+00  2.5492e+00  2.5428e+00  ...   2.5717e+00  2.5678e+00  2.5981e+00\n",
       "  2.3835e+00  2.7356e+00  2.7238e+00  ...   2.7793e+00  2.7772e+00  2.7869e+00\n",
       "  2.3540e+00  2.7192e+00  2.6982e+00  ...   2.7431e+00  2.7900e+00  2.8017e+00\n",
       "                 ...                                      ...                \n",
       "  1.8327e+00  2.1647e+00  2.1655e+00  ...   1.4009e+00  1.4049e+00  1.4236e+00\n",
       "  1.8569e+00  2.0481e+00  1.7447e+00  ...   1.3416e+00  1.3727e+00  1.3950e+00\n",
       "  1.7091e+00  1.8087e+00  1.7261e+00  ...   1.3323e+00  1.3966e+00  1.4636e+00\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -2.9540e-01  1.0573e+00  1.0665e+00  ...   1.0822e+00  1.0001e+00  1.0400e+00\n",
       " -1.0241e+00  6.3840e-01  6.0864e-01  ...   5.6922e-01  6.3254e-01  5.3445e-01\n",
       " -9.7486e-01  6.5131e-01  6.3060e-01  ...   5.5343e-01  5.5851e-01  5.9261e-01\n",
       "                 ...                                      ...                \n",
       "  9.5400e-02  5.6090e-01  1.4976e+00  ...   1.7165e+00  1.7006e+00  1.6348e+00\n",
       " -3.9346e-02  1.4902e+00  1.4729e+00  ...   1.7516e+00  1.6988e+00  1.7114e+00\n",
       "  4.9181e-01  1.3599e+00  1.3940e+00  ...   1.7428e+00  1.6041e+00  1.6787e+00\n",
       "    ... \n",
       "\n",
       "( 0 ,29 ,.,.) = \n",
       "  4.2042e-01  3.5728e-01  3.5831e-01  ...   3.5116e-01  3.5103e-01  3.4401e-01\n",
       "  2.2544e-01  9.6014e-02  9.6531e-02  ...   7.2965e-02  7.3764e-02  6.8168e-02\n",
       "  2.3335e-01  1.0520e-01  1.0831e-01  ...   9.0141e-02  7.6894e-02  7.0810e-02\n",
       "                 ...                                      ...                \n",
       "  3.5686e-01  2.2027e-01  2.3697e-01  ...   6.0537e-01  5.9801e-01  5.8463e-01\n",
       "  3.6141e-01  2.7081e-01  3.6512e-01  ...   6.2921e-01  6.2110e-01  6.1151e-01\n",
       "  4.2083e-01  3.7330e-01  3.8912e-01  ...   6.3206e-01  6.0753e-01  5.8898e-01\n",
       "\n",
       "( 0 ,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "( 0 ,31 ,.,.) = \n",
       " -2.0977e-01 -2.3717e-01 -2.3711e-01  ...  -2.3642e-01 -2.3623e-01 -2.3572e-01\n",
       " -2.1054e-01 -2.4709e-01 -2.4631e-01  ...  -2.4525e-01 -2.4634e-01 -2.4494e-01\n",
       " -2.1124e-01 -2.4669e-01 -2.4617e-01  ...  -2.4492e-01 -2.4590e-01 -2.4559e-01\n",
       "                 ...                                      ...                \n",
       " -2.1461e-01 -2.2768e-01 -2.3397e-01  ...  -2.3732e-01 -2.3758e-01 -2.3733e-01\n",
       " -2.1244e-01 -2.3255e-01 -2.2908e-01  ...  -2.3685e-01 -2.3674e-01 -2.3772e-01\n",
       " -2.1651e-01 -2.2924e-01 -2.2671e-01  ...  -2.3672e-01 -2.3604e-01 -2.3754e-01\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  8.3131e-01  7.8908e-01  8.7961e-01  ...   7.5248e-01  7.9315e-01  7.9698e-01\n",
       "  7.7897e-01  6.6795e-01  6.7889e-01  ...   7.6038e-01  7.5238e-01  7.4445e-01\n",
       "  8.1730e-01  7.8493e-01  8.5419e-01  ...   7.4464e-01  7.3595e-01  7.3606e-01\n",
       "                 ...                                      ...                \n",
       "  8.9419e-01  7.2514e-01  7.1808e-01  ...   8.0047e-01  8.4330e-01  7.1594e-01\n",
       "  8.7780e-01  8.5376e-01  8.6655e-01  ...   6.0681e-01  5.4292e-01  7.6247e-01\n",
       "  9.2695e-01  8.5097e-01  7.9564e-01  ...   7.9114e-01  6.6468e-01  8.1315e-01\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  1.5638e+00  1.5843e+00  1.7393e+00  ...   1.4499e+00  1.5804e+00  1.6271e+00\n",
       "  1.5042e+00  1.4509e+00  1.4689e+00  ...   1.5928e+00  1.5659e+00  1.5814e+00\n",
       "  1.5345e+00  1.6321e+00  1.7235e+00  ...   1.6557e+00  1.4912e+00  1.5507e+00\n",
       "                 ...                                      ...                \n",
       "  1.8289e+00  1.7166e+00  1.6912e+00  ...   1.8219e+00  1.8876e+00  1.7520e+00\n",
       "  1.7491e+00  1.9443e+00  1.9293e+00  ...   1.6282e+00  1.2381e+00  1.5218e+00\n",
       "  1.7561e+00  1.9501e+00  1.9596e+00  ...   1.8954e+00  1.6364e+00  1.5480e+00\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  1.1972e+00  1.5116e+00  1.2873e+00  ...   1.4876e+00  1.4338e+00  1.4194e+00\n",
       "  1.1599e+00  1.6326e+00  1.2523e+00  ...   1.6396e+00  1.3317e+00  1.3729e+00\n",
       "  1.0994e+00  1.3474e+00  9.3165e-01  ...   1.6560e+00  1.5862e+00  1.5972e+00\n",
       "                 ...                                      ...                \n",
       "  3.7627e-01  1.6095e+00  1.3602e+00  ...   1.3841e+00  1.1779e+00  1.5181e+00\n",
       "  2.3871e-01  1.1470e+00  1.1713e+00  ...   2.0181e+00  1.6768e+00  7.9595e-01\n",
       "  9.4456e-02  1.2601e+00  1.3693e+00  ...   1.3730e+00  1.8557e+00  9.5447e-01\n",
       "    ... \n",
       "\n",
       "( 1 ,29 ,.,.) = \n",
       "  5.5152e-01  5.3105e-01  5.0575e-01  ...   5.6361e-01  5.3392e-01  5.2342e-01\n",
       "  5.1486e-01  5.0663e-01  4.7854e-01  ...   4.7973e-01  4.6125e-01  4.4458e-01\n",
       "  5.1918e-01  4.8860e-01  4.3774e-01  ...   4.7369e-01  5.0584e-01  4.9786e-01\n",
       "                 ...                                      ...                \n",
       "  4.2398e-01  4.3470e-01  4.6495e-01  ...   3.9276e-01  3.8013e-01  4.0864e-01\n",
       "  3.9689e-01  3.7400e-01  3.5181e-01  ...   4.4075e-01  5.5313e-01  4.7324e-01\n",
       "  4.0170e-01  2.8708e-01  3.2910e-01  ...   3.6952e-01  4.8934e-01  4.8759e-01\n",
       "\n",
       "( 1 ,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "( 1 ,31 ,.,.) = \n",
       " -2.2389e-01 -2.2964e-01 -2.2485e-01  ...  -2.2940e-01 -2.2900e-01 -2.2992e-01\n",
       " -2.2478e-01 -2.3176e-01 -2.2630e-01  ...  -2.3269e-01 -2.3107e-01 -2.3224e-01\n",
       " -2.2408e-01 -2.2977e-01 -2.2513e-01  ...  -2.3609e-01 -2.3389e-01 -2.3105e-01\n",
       "                 ...                                      ...                \n",
       " -2.1899e-01 -2.3482e-01 -2.2886e-01  ...  -2.3768e-01 -2.2908e-01 -2.3388e-01\n",
       " -2.1912e-01 -2.3194e-01 -2.3420e-01  ...  -2.4526e-01 -2.3593e-01 -2.2242e-01\n",
       " -2.1645e-01 -2.3448e-01 -2.3283e-01  ...  -2.2856e-01 -2.3461e-01 -2.2614e-01\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  9.5676e-01  9.6177e-01  9.4148e-01  ...   6.8269e-01  6.8790e-01  6.8975e-01\n",
       "  9.7636e-01  9.9796e-01  9.8153e-01  ...   6.8260e-01  6.8533e-01  7.1026e-01\n",
       "  9.5745e-01  9.8253e-01  9.6418e-01  ...   6.9254e-01  6.9679e-01  7.3371e-01\n",
       "                 ...                                      ...                \n",
       "  1.3722e+00  1.3794e+00  1.3725e+00  ...   7.8317e-01  8.2560e-01  8.8911e-01\n",
       "  1.3776e+00  1.3886e+00  1.3768e+00  ...   7.7355e-01  8.1016e-01  8.5319e-01\n",
       "  1.3733e+00  1.3828e+00  1.3788e+00  ...   7.1207e-01  6.9996e-01  6.5855e-01\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1.5073e+00  1.4934e+00  1.4401e+00  ...   1.0988e+00  1.1275e+00  1.1223e+00\n",
       "  1.5034e+00  1.5348e+00  1.4766e+00  ...   1.0339e+00  1.0566e+00  1.1142e+00\n",
       "  1.4608e+00  1.4715e+00  1.4262e+00  ...   1.0520e+00  1.0622e+00  1.1186e+00\n",
       "                 ...                                      ...                \n",
       "  2.1624e+00  2.4553e+00  2.4687e+00  ...   1.1136e+00  1.2212e+00  1.3332e+00\n",
       "  2.1677e+00  2.4767e+00  2.4858e+00  ...   1.0822e+00  1.2117e+00  1.2810e+00\n",
       "  2.1619e+00  2.4757e+00  2.4897e+00  ...   1.0063e+00  1.0277e+00  9.8312e-01\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  1.5745e+00  1.7956e+00  1.8270e+00  ...   1.7582e+00  1.8772e+00  1.8731e+00\n",
       "  1.5884e+00  1.7660e+00  1.8058e+00  ...   1.9559e+00  1.9290e+00  1.8274e+00\n",
       "  1.7271e+00  1.7944e+00  1.8627e+00  ...   1.9592e+00  1.9454e+00  1.7449e+00\n",
       "                 ...                                      ...                \n",
       " -1.2700e-01  9.2114e-01  9.2117e-01  ...   1.7901e+00  1.7611e+00  1.5655e+00\n",
       " -1.5502e-01  8.9004e-01  9.2635e-01  ...   1.7931e+00  1.7920e+00  1.6919e+00\n",
       " -1.5450e-01  8.8763e-01  9.2364e-01  ...   1.9667e+00  2.0113e+00  2.1385e+00\n",
       "    ... \n",
       "\n",
       "( 2 ,29 ,.,.) = \n",
       "  6.1150e-01  6.1391e-01  6.2726e-01  ...   6.6254e-01  6.5845e-01  6.6131e-01\n",
       "  5.8783e-01  5.9001e-01  6.1242e-01  ...   7.2569e-01  7.1473e-01  6.9863e-01\n",
       "  6.0799e-01  6.1383e-01  6.3362e-01  ...   7.2648e-01  7.2103e-01  6.8705e-01\n",
       "                 ...                                      ...                \n",
       "  3.5423e-01  2.4857e-01  2.4060e-01  ...   7.1171e-01  6.7676e-01  6.3090e-01\n",
       "  3.5031e-01  2.3936e-01  2.3345e-01  ...   7.1854e-01  6.7920e-01  6.4686e-01\n",
       "  3.5044e-01  2.3787e-01  2.3183e-01  ...   7.4327e-01  7.2416e-01  7.2906e-01\n",
       "\n",
       "( 2 ,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "( 2 ,31 ,.,.) = \n",
       " -2.3286e-01 -2.4111e-01 -2.4070e-01  ...  -2.3116e-01 -2.3309e-01 -2.3354e-01\n",
       " -2.3432e-01 -2.4441e-01 -2.4381e-01  ...  -2.3225e-01 -2.3265e-01 -2.3132e-01\n",
       " -2.3559e-01 -2.4387e-01 -2.4411e-01  ...  -2.3270e-01 -2.3297e-01 -2.3157e-01\n",
       "                 ...                                      ...                \n",
       " -2.1944e-01 -2.4972e-01 -2.4960e-01  ...  -2.3386e-01 -2.3609e-01 -2.3562e-01\n",
       " -2.1925e-01 -2.4960e-01 -2.4986e-01  ...  -2.3366e-01 -2.3537e-01 -2.3598e-01\n",
       " -2.1925e-01 -2.4953e-01 -2.4973e-01  ...  -2.3434e-01 -2.3473e-01 -2.3535e-01\n",
       "...     \n",
       "        \n",
       "\n",
       "(125, 0 ,.,.) = \n",
       "  7.2241e-01  7.7881e-01  7.7837e-01  ...   8.6145e-01  8.6191e-01  8.5368e-01\n",
       "  7.4920e-01  8.1740e-01  8.1359e-01  ...   8.8993e-01  8.8272e-01  8.7428e-01\n",
       "  7.5849e-01  8.2216e-01  8.1277e-01  ...   8.8647e-01  8.8113e-01  8.7510e-01\n",
       "                 ...                                      ...                \n",
       "  1.1385e+00  1.1218e+00  1.1221e+00  ...   1.1212e+00  1.1272e+00  1.1228e+00\n",
       "  1.1466e+00  1.1341e+00  1.1327e+00  ...   1.1385e+00  1.1437e+00  1.1263e+00\n",
       "  1.1279e+00  1.1252e+00  1.1431e+00  ...   1.1582e+00  1.1562e+00  1.1455e+00\n",
       "\n",
       "(125, 1 ,.,.) = \n",
       "  1.0857e+00  1.0484e+00  1.0624e+00  ...   1.4510e+00  1.4397e+00  1.4178e+00\n",
       "  1.1159e+00  1.0721e+00  1.0865e+00  ...   1.5135e+00  1.4937e+00  1.4677e+00\n",
       "  1.1246e+00  1.0806e+00  1.0913e+00  ...   1.4914e+00  1.4894e+00  1.4719e+00\n",
       "                 ...                                      ...                \n",
       "  1.9896e+00  2.2442e+00  2.2431e+00  ...   2.2809e+00  2.2791e+00  2.2718e+00\n",
       "  1.9962e+00  2.2584e+00  2.2564e+00  ...   2.3372e+00  2.3230e+00  2.2987e+00\n",
       "  1.9948e+00  2.2203e+00  2.2968e+00  ...   2.3553e+00  2.3468e+00  2.3429e+00\n",
       "\n",
       "(125, 2 ,.,.) = \n",
       "  2.3710e+00  1.9561e+00  1.9502e+00  ...   1.7155e+00  1.7373e+00  1.7468e+00\n",
       "  2.5481e+00  2.0020e+00  1.9879e+00  ...   1.6543e+00  1.6822e+00  1.7033e+00\n",
       "  2.5278e+00  2.0035e+00  1.9942e+00  ...   1.6508e+00  1.6760e+00  1.6921e+00\n",
       "                 ...                                      ...                \n",
       " -4.0175e-02  9.7561e-01  9.8335e-01  ...   1.0057e+00  9.7448e-01  9.8974e-01\n",
       " -1.7879e-02  9.7444e-01  1.0152e+00  ...   9.7014e-01  9.2425e-01  1.0002e+00\n",
       "  5.7168e-02  8.9145e-01  9.2321e-01  ...   8.9899e-01  9.3065e-01  1.0191e+00\n",
       "    ... \n",
       "\n",
       "(125,29 ,.,.) = \n",
       "  6.8941e-01  7.0962e-01  7.0669e-01  ...   6.0013e-01  6.0340e-01  6.0916e-01\n",
       "  7.2954e-01  7.5939e-01  7.5042e-01  ...   5.6228e-01  5.7088e-01  5.7851e-01\n",
       "  7.2851e-01  7.5566e-01  7.4828e-01  ...   5.6869e-01  5.6966e-01  5.7695e-01\n",
       "                 ...                                      ...                \n",
       "  3.5766e-01  2.6171e-01  2.5760e-01  ...   2.5401e-01  2.5900e-01  2.6030e-01\n",
       "  3.6337e-01  2.6892e-01  2.6906e-01  ...   2.3793e-01  2.3819e-01  2.3931e-01\n",
       "  3.6743e-01  2.7364e-01  2.5339e-01  ...   2.2726e-01  2.2559e-01  2.3441e-01\n",
       "\n",
       "(125,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "(125,31 ,.,.) = \n",
       " -2.4284e-01 -2.3958e-01 -2.3946e-01  ...  -2.3597e-01 -2.3623e-01 -2.3641e-01\n",
       " -2.4265e-01 -2.3903e-01 -2.3892e-01  ...  -2.3743e-01 -2.3788e-01 -2.3774e-01\n",
       " -2.4253e-01 -2.3929e-01 -2.3904e-01  ...  -2.3775e-01 -2.3775e-01 -2.3749e-01\n",
       "                 ...                                      ...                \n",
       " -2.1690e-01 -2.4011e-01 -2.4030e-01  ...  -2.4077e-01 -2.3980e-01 -2.4039e-01\n",
       " -2.1690e-01 -2.3947e-01 -2.4012e-01  ...  -2.4047e-01 -2.4015e-01 -2.4149e-01\n",
       " -2.1740e-01 -2.3862e-01 -2.3994e-01  ...  -2.3996e-01 -2.4093e-01 -2.4117e-01\n",
       "        \n",
       "\n",
       "(126, 0 ,.,.) = \n",
       "  5.5948e-01  4.8506e-01  4.5144e-01  ...   4.0062e-01  3.9923e-01  3.9925e-01\n",
       "  5.6471e-01  4.7860e-01  4.3835e-01  ...   3.4301e-01  3.4773e-01  3.5194e-01\n",
       "  6.1252e-01  5.0088e-01  4.5962e-01  ...   3.1194e-01  3.1892e-01  3.2039e-01\n",
       "                 ...                                      ...                \n",
       "  1.2519e+00  1.0541e+00  1.0084e+00  ...   6.6774e-01  6.6110e-01  6.8109e-01\n",
       "  1.2180e+00  1.0337e+00  9.9561e-01  ...   6.5247e-01  6.7875e-01  6.9388e-01\n",
       "  1.1038e+00  9.9428e-01  9.7650e-01  ...   6.4553e-01  6.8654e-01  6.8063e-01\n",
       "\n",
       "(126, 1 ,.,.) = \n",
       "  1.1894e+00  1.0485e+00  9.1149e-01  ...   9.7312e-01  9.8110e-01  9.7896e-01\n",
       "  1.2410e+00  1.0624e+00  9.4383e-01  ...   7.1789e-01  7.4981e-01  7.8308e-01\n",
       "  1.3195e+00  1.1414e+00  1.0057e+00  ...   4.6331e-01  4.9957e-01  5.3321e-01\n",
       "                 ...                                      ...                \n",
       "  2.3262e+00  2.4031e+00  2.1653e+00  ...   1.4004e+00  1.3846e+00  1.3820e+00\n",
       "  2.2669e+00  2.2955e+00  2.1180e+00  ...   1.3697e+00  1.4041e+00  1.4102e+00\n",
       "  2.0874e+00  2.0937e+00  2.0148e+00  ...   1.3548e+00  1.3950e+00  1.4057e+00\n",
       "\n",
       "(126, 2 ,.,.) = \n",
       "  1.8829e+00  2.0110e+00  2.0969e+00  ...   1.8371e+00  1.8411e+00  1.8414e+00\n",
       "  1.8305e+00  2.0788e+00  2.2359e+00  ...   2.0618e+00  2.0344e+00  1.9928e+00\n",
       "  1.6333e+00  2.0560e+00  2.0850e+00  ...   2.3129e+00  2.2413e+00  2.1894e+00\n",
       "                 ...                                      ...                \n",
       " -9.0999e-01  1.3555e+00  1.2973e+00  ...   1.6383e+00  1.6554e+00  1.5852e+00\n",
       " -6.7618e-01  1.3797e+00  1.2895e+00  ...   1.6241e+00  1.5545e+00  1.7075e+00\n",
       " -1.4015e-01  1.3249e+00  1.3459e+00  ...   1.6256e+00  1.5438e+00  1.7766e+00\n",
       "    ... \n",
       "\n",
       "(126,29 ,.,.) = \n",
       "  6.0646e-01  6.3609e-01  6.7197e-01  ...   6.2984e-01  6.2746e-01  6.2797e-01\n",
       "  6.1119e-01  6.6845e-01  7.2643e-01  ...   7.4713e-01  7.3800e-01  7.2570e-01\n",
       "  5.8517e-01  6.3729e-01  6.8537e-01  ...   8.6298e-01  8.4634e-01  8.2597e-01\n",
       "                 ...                                      ...                \n",
       "  2.3031e-01  1.9393e-01  2.8543e-01  ...   5.4280e-01  5.5172e-01  5.4806e-01\n",
       "  2.4393e-01  2.2959e-01  3.0374e-01  ...   5.4907e-01  5.4306e-01  5.4797e-01\n",
       "  3.1414e-01  3.0207e-01  3.4114e-01  ...   5.5470e-01  5.3891e-01  5.5165e-01\n",
       "\n",
       "(126,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "(126,31 ,.,.) = \n",
       " -2.3025e-01 -2.2843e-01 -2.2965e-01  ...  -2.2115e-01 -2.2103e-01 -2.2122e-01\n",
       " -2.2898e-01 -2.2655e-01 -2.2598e-01  ...  -2.1800e-01 -2.1788e-01 -2.1693e-01\n",
       " -2.2714e-01 -2.2780e-01 -2.2556e-01  ...  -2.2052e-01 -2.1963e-01 -2.1913e-01\n",
       "                 ...                                      ...                \n",
       " -2.0755e-01 -2.4464e-01 -2.4035e-01  ...  -2.2813e-01 -2.2806e-01 -2.2755e-01\n",
       " -2.1034e-01 -2.4349e-01 -2.3923e-01  ...  -2.2755e-01 -2.2672e-01 -2.3025e-01\n",
       " -2.1492e-01 -2.4032e-01 -2.3899e-01  ...  -2.2690e-01 -2.2755e-01 -2.3095e-01\n",
       "        \n",
       "\n",
       "(127, 0 ,.,.) = \n",
       "  5.6952e-01  5.5051e-01  4.7313e-01  ...   6.9723e-01  5.2132e-01  3.6818e-01\n",
       "  6.0446e-01  6.5989e-01  5.6126e-01  ...   6.3035e-01  5.0671e-01  3.5863e-01\n",
       "  6.0447e-01  6.3926e-01  6.3272e-01  ...   5.6210e-01  5.6562e-01  4.4077e-01\n",
       "                 ...                                      ...                \n",
       "  6.3400e-01  6.0464e-01  3.9064e-01  ...   4.2518e-01  3.1850e-01  4.6805e-01\n",
       "  5.6201e-01  3.5155e-01  2.4482e-01  ...   3.6370e-01  3.5393e-01  4.4989e-01\n",
       "  6.3648e-01  4.0841e-01  2.6657e-01  ...   4.2617e-01  3.8547e-01  4.6055e-01\n",
       "\n",
       "(127, 1 ,.,.) = \n",
       "  1.0695e+00  9.7406e-01  8.7613e-01  ...   1.3276e+00  1.0638e+00  4.3637e-01\n",
       "  1.1132e+00  1.1445e+00  1.0758e+00  ...   1.1822e+00  1.0212e+00  4.5041e-01\n",
       "  1.1111e+00  1.1302e+00  1.1412e+00  ...   1.1153e+00  9.2242e-01  8.1351e-01\n",
       "                 ...                                      ...                \n",
       "  1.2980e+00  1.3554e+00  1.0904e+00  ...   1.2253e+00  1.0292e+00  1.0997e+00\n",
       "  1.1969e+00  1.0145e+00  8.5266e-01  ...   1.0890e+00  1.0484e+00  1.0712e+00\n",
       "  1.3343e+00  1.2195e+00  7.8553e-01  ...   1.1540e+00  1.1057e+00  1.0743e+00\n",
       "\n",
       "(127, 2 ,.,.) = \n",
       "  2.1643e+00  1.9123e+00  2.1987e+00  ...   1.7024e+00  2.9007e+00  2.5232e+00\n",
       "  2.1147e+00  1.7878e+00  2.2491e+00  ...   1.7470e+00  2.8368e+00  3.0559e+00\n",
       "  2.1037e+00  1.7989e+00  1.9838e+00  ...   2.3179e+00  1.9393e+00  2.7971e+00\n",
       "                 ...                                      ...                \n",
       "  1.5594e+00  1.4400e+00  2.0165e+00  ...   1.7506e+00  1.7448e+00  1.4940e+00\n",
       "  1.7360e+00  2.0204e+00  1.8572e+00  ...   1.5995e+00  1.7173e+00  1.4202e+00\n",
       "  1.5577e+00  2.0817e+00  1.8742e+00  ...   1.5673e+00  1.8529e+00  1.6596e+00\n",
       "    ... \n",
       "\n",
       "(127,29 ,.,.) = \n",
       "  6.4761e-01  6.7156e-01  6.9681e-01  ...   5.9703e-01  6.8615e-01  8.1323e-01\n",
       "  6.7200e-01  6.9117e-01  7.3370e-01  ...   6.2434e-01  7.7623e-01  9.6629e-01\n",
       "  6.6544e-01  6.7045e-01  6.8428e-01  ...   6.8616e-01  7.5749e-01  8.5234e-01\n",
       "                 ...                                      ...                \n",
       "  5.4438e-01  5.2780e-01  5.6654e-01  ...   5.9752e-01  6.4927e-01  6.3614e-01\n",
       "  6.4085e-01  6.4203e-01  7.0738e-01  ...   6.0705e-01  6.1946e-01  5.8435e-01\n",
       "  5.9406e-01  6.4404e-01  7.4352e-01  ...   6.2029e-01  6.3476e-01  6.4983e-01\n",
       "\n",
       "(127,30 ,.,.) = \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "                 ...                                      ...                \n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       " -3.0151e-01 -3.0151e-01 -3.0151e-01  ...  -3.0151e-01 -3.0151e-01 -3.0151e-01\n",
       "\n",
       "(127,31 ,.,.) = \n",
       " -2.3725e-01 -2.3306e-01 -2.3636e-01  ...  -2.3119e-01 -2.4462e-01 -2.3746e-01\n",
       " -2.3629e-01 -2.2979e-01 -2.3381e-01  ...  -2.3116e-01 -2.3909e-01 -2.3804e-01\n",
       " -2.3604e-01 -2.3035e-01 -2.3260e-01  ...  -2.3543e-01 -2.2886e-01 -2.3851e-01\n",
       "                 ...                                      ...                \n",
       " -2.2942e-01 -2.2467e-01 -2.3406e-01  ...  -2.2847e-01 -2.2577e-01 -2.2080e-01\n",
       " -2.3197e-01 -2.3150e-01 -2.2499e-01  ...  -2.2723e-01 -2.2489e-01 -2.2118e-01\n",
       " -2.2949e-01 -2.3159e-01 -2.2482e-01  ...  -2.2562e-01 -2.2980e-01 -2.2047e-01\n",
       "[torch.cuda.FloatTensor of size 128x32x112x112 (GPU 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.deployment_model(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant = torch.nn.DataParallel(quantizer.deployment_model, gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = model_quant(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1552e+00  9.5017e-01  8.7872e-01  ...  -1.8631e-01  1.9957e+00  9.9282e-01\n",
       "-3.9337e+00 -4.2530e+00 -1.2152e-01  ...  -2.8191e+00 -7.4017e-01  1.6144e+00\n",
       "-5.5881e+00  3.0396e+00 -5.6240e-01  ...  -7.4701e-01 -1.7197e+00  4.4811e+00\n",
       "                ...                                      ...                \n",
       " 1.9808e+00  2.0554e+00  1.9562e+00  ...   6.3680e-01  1.1966e+00 -4.0874e-01\n",
       " 4.5948e-01 -1.6623e+00 -3.7264e-01  ...  -1.6432e+00  9.9374e-01  2.9577e+00\n",
       "-9.2918e-01 -3.6385e+00  2.4017e+00  ...  -1.0772e+00 -1.9507e-01 -2.8710e-02\n",
       "[torch.cuda.FloatTensor of size 128x1000 (GPU 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.9494e+00 -3.9579e-01  3.2307e-01  ...   4.3272e-01  2.4295e+00  3.2377e-01\n",
       "-5.2903e+00 -5.2021e+00  9.0071e-03  ...  -3.1000e+00 -1.9988e+00  2.0122e+00\n",
       "-6.1509e+00  2.0534e+00 -1.4551e+00  ...  -3.8637e-01 -2.2865e+00  4.8162e+00\n",
       "                ...                                      ...                \n",
       " 1.6031e+00 -1.6777e+00 -1.1769e+00  ...   1.7391e+00 -1.1841e-01 -1.9975e+00\n",
       " 2.4302e+00 -2.1855e+00  4.6637e-01  ...  -2.0303e+00  1.1533e+00  5.9466e-01\n",
       "-6.8305e-01 -4.2784e+00  1.8464e+00  ...  -1.1086e+00  1.0207e-01 -1.4453e+00\n",
       "[torch.cuda.FloatTensor of size 128x1000 (GPU 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data_loader, model, criterion, epoch=0, training=True, optimizer=None, quantizer=None, get_quant_params=False):\n",
    "\n",
    "#    if args.gpus and len(args.gpus) > 1:\n",
    "#        model = torch.nn.DataParallel(model, args.gpus)\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if gpus is not None:\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "\n",
    "        # quantization before computing output\n",
    "        if quantizer is not None:\n",
    "            quantizer.store_and_quantize(training=training, get_quant_params=False)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "\n",
    "        loss = criterion(output, target_var)\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        if training:\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # restore real value parameters before update\n",
    "            if quantizer is not None:\n",
    "                quantizer.restore_real_value()            \n",
    "                quantizer.backprop_quant_gradients()    \n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        elif quantizer is not None:\n",
    "            quantizer.restore_real_value()\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            logging.info('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "def train(data_loader, model, criterion, epoch, optimizer, quantizer):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    return forward(data_loader, model, criterion, epoch,\n",
    "                   training=True, optimizer=optimizer, quantizer=quantizer, get_quant_params=False)\n",
    "\n",
    "\n",
    "def validate(data_loader, model, criterion, epoch, quantizer, get_quant_params=False):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    return forward(data_loader, model, criterion, epoch,\n",
    "                   training=False, optimizer=None, quantizer=quantizer, get_quant_params=get_quant_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:38: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "EVALUATING - Epoch: [0][0/5000]\tTime 1.692 (1.692)\tData 1.321 (1.321)\tLoss 1.8149 (1.8149)\tPrec@1 60.000 (60.000)\tPrec@5 60.000 (60.000)\n",
      "EVALUATING - Epoch: [0][10/5000]\tTime 0.061 (0.206)\tData 0.000 (0.120)\tLoss 2.1649 (2.1501)\tPrec@1 40.000 (55.455)\tPrec@5 80.000 (71.818)\n",
      "EVALUATING - Epoch: [0][20/5000]\tTime 0.058 (0.140)\tData 0.000 (0.063)\tLoss 2.1839 (2.0736)\tPrec@1 60.000 (47.143)\tPrec@5 80.000 (79.524)\n",
      "EVALUATING - Epoch: [0][30/5000]\tTime 0.060 (0.114)\tData 0.000 (0.043)\tLoss 3.6015 (2.2907)\tPrec@1 10.000 (44.839)\tPrec@5 50.000 (75.484)\n",
      "EVALUATING - Epoch: [0][40/5000]\tTime 0.049 (0.098)\tData 0.000 (0.033)\tLoss 3.0640 (2.3257)\tPrec@1 30.000 (44.390)\tPrec@5 70.000 (73.902)\n",
      "EVALUATING - Epoch: [0][50/5000]\tTime 0.053 (0.091)\tData 0.000 (0.026)\tLoss 2.3611 (2.4076)\tPrec@1 50.000 (44.118)\tPrec@5 90.000 (72.549)\n",
      "EVALUATING - Epoch: [0][60/5000]\tTime 0.080 (0.087)\tData 0.000 (0.022)\tLoss 2.4272 (2.3161)\tPrec@1 40.000 (45.902)\tPrec@5 80.000 (73.115)\n",
      "EVALUATING - Epoch: [0][70/5000]\tTime 0.072 (0.083)\tData 0.000 (0.019)\tLoss 1.8615 (2.2721)\tPrec@1 50.000 (45.915)\tPrec@5 80.000 (73.803)\n",
      "EVALUATING - Epoch: [0][80/5000]\tTime 0.051 (0.079)\tData 0.000 (0.017)\tLoss 1.3479 (2.1763)\tPrec@1 70.000 (48.395)\tPrec@5 90.000 (74.815)\n",
      "EVALUATING - Epoch: [0][90/5000]\tTime 0.059 (0.077)\tData 0.000 (0.015)\tLoss 1.8275 (2.1140)\tPrec@1 60.000 (50.110)\tPrec@5 90.000 (75.495)\n",
      "EVALUATING - Epoch: [0][100/5000]\tTime 0.076 (0.075)\tData 0.000 (0.014)\tLoss 0.6850 (2.0989)\tPrec@1 90.000 (51.386)\tPrec@5 90.000 (75.743)\n",
      "EVALUATING - Epoch: [0][110/5000]\tTime 0.049 (0.074)\tData 0.000 (0.012)\tLoss 2.4065 (2.1108)\tPrec@1 30.000 (51.441)\tPrec@5 80.000 (76.126)\n",
      "EVALUATING - Epoch: [0][120/5000]\tTime 0.049 (0.072)\tData 0.000 (0.011)\tLoss 2.3020 (2.1581)\tPrec@1 60.000 (49.504)\tPrec@5 70.000 (75.785)\n",
      "EVALUATING - Epoch: [0][130/5000]\tTime 0.052 (0.070)\tData 0.000 (0.011)\tLoss 4.3341 (2.1174)\tPrec@1 10.000 (51.069)\tPrec@5 20.000 (76.183)\n",
      "EVALUATING - Epoch: [0][140/5000]\tTime 0.052 (0.069)\tData 0.000 (0.010)\tLoss 3.4103 (2.2674)\tPrec@1 40.000 (48.936)\tPrec@5 60.000 (73.688)\n",
      "EVALUATING - Epoch: [0][150/5000]\tTime 0.070 (0.069)\tData 0.000 (0.009)\tLoss 2.7749 (2.3319)\tPrec@1 40.000 (48.013)\tPrec@5 70.000 (73.046)\n",
      "EVALUATING - Epoch: [0][160/5000]\tTime 0.060 (0.068)\tData 0.000 (0.009)\tLoss 5.0807 (2.3307)\tPrec@1 0.000 (47.888)\tPrec@5 20.000 (72.919)\n",
      "EVALUATING - Epoch: [0][170/5000]\tTime 0.057 (0.068)\tData 0.000 (0.008)\tLoss 4.1278 (2.4486)\tPrec@1 20.000 (46.199)\tPrec@5 20.000 (70.760)\n",
      "EVALUATING - Epoch: [0][180/5000]\tTime 0.055 (0.067)\tData 0.000 (0.008)\tLoss 2.8795 (2.5211)\tPrec@1 20.000 (44.807)\tPrec@5 80.000 (69.724)\n",
      "EVALUATING - Epoch: [0][190/5000]\tTime 0.066 (0.067)\tData 0.000 (0.007)\tLoss 4.9767 (2.5089)\tPrec@1 10.000 (44.660)\tPrec@5 30.000 (70.052)\n",
      "EVALUATING - Epoch: [0][200/5000]\tTime 0.053 (0.066)\tData 0.000 (0.007)\tLoss 1.3633 (2.5641)\tPrec@1 40.000 (43.781)\tPrec@5 100.000 (69.055)\n",
      "EVALUATING - Epoch: [0][210/5000]\tTime 0.059 (0.066)\tData 0.000 (0.007)\tLoss 5.9563 (2.6176)\tPrec@1 20.000 (42.938)\tPrec@5 30.000 (68.389)\n",
      "EVALUATING - Epoch: [0][220/5000]\tTime 0.059 (0.066)\tData 0.000 (0.006)\tLoss 4.7591 (2.7323)\tPrec@1 10.000 (41.312)\tPrec@5 30.000 (66.154)\n",
      "EVALUATING - Epoch: [0][230/5000]\tTime 0.058 (0.065)\tData 0.000 (0.006)\tLoss 3.2207 (2.8175)\tPrec@1 0.000 (39.913)\tPrec@5 40.000 (64.675)\n",
      "EVALUATING - Epoch: [0][240/5000]\tTime 0.047 (0.064)\tData 0.000 (0.006)\tLoss 2.1971 (2.8298)\tPrec@1 30.000 (39.087)\tPrec@5 70.000 (64.647)\n",
      "EVALUATING - Epoch: [0][250/5000]\tTime 0.048 (0.064)\tData 0.000 (0.006)\tLoss 3.8623 (2.8185)\tPrec@1 10.000 (39.203)\tPrec@5 50.000 (65.060)\n",
      "EVALUATING - Epoch: [0][260/5000]\tTime 0.047 (0.064)\tData 0.000 (0.006)\tLoss 3.3093 (2.8452)\tPrec@1 30.000 (38.812)\tPrec@5 60.000 (64.636)\n",
      "EVALUATING - Epoch: [0][270/5000]\tTime 0.061 (0.064)\tData 0.000 (0.005)\tLoss 5.4381 (2.8924)\tPrec@1 0.000 (38.376)\tPrec@5 20.000 (63.801)\n",
      "EVALUATING - Epoch: [0][280/5000]\tTime 0.055 (0.063)\tData 0.000 (0.005)\tLoss 3.6334 (2.9090)\tPrec@1 10.000 (37.616)\tPrec@5 40.000 (63.274)\n",
      "EVALUATING - Epoch: [0][290/5000]\tTime 0.050 (0.063)\tData 0.000 (0.005)\tLoss 3.9019 (2.9341)\tPrec@1 10.000 (37.216)\tPrec@5 40.000 (62.887)\n",
      "EVALUATING - Epoch: [0][300/5000]\tTime 0.051 (0.063)\tData 0.000 (0.005)\tLoss 4.2774 (2.9561)\tPrec@1 0.000 (36.711)\tPrec@5 40.000 (62.458)\n",
      "EVALUATING - Epoch: [0][310/5000]\tTime 0.045 (0.063)\tData 0.000 (0.005)\tLoss 4.6751 (3.0102)\tPrec@1 10.000 (35.852)\tPrec@5 30.000 (61.608)\n",
      "EVALUATING - Epoch: [0][320/5000]\tTime 0.054 (0.062)\tData 0.000 (0.005)\tLoss 1.1879 (3.0406)\tPrec@1 70.000 (35.296)\tPrec@5 90.000 (61.277)\n",
      "EVALUATING - Epoch: [0][330/5000]\tTime 0.052 (0.062)\tData 0.000 (0.005)\tLoss 5.0008 (3.0472)\tPrec@1 0.000 (35.136)\tPrec@5 10.000 (61.239)\n",
      "EVALUATING - Epoch: [0][340/5000]\tTime 0.052 (0.062)\tData 0.000 (0.005)\tLoss 5.1908 (3.1101)\tPrec@1 0.000 (34.194)\tPrec@5 20.000 (60.029)\n",
      "EVALUATING - Epoch: [0][350/5000]\tTime 0.031 (0.062)\tData 0.000 (0.005)\tLoss 2.3808 (3.1157)\tPrec@1 40.000 (34.217)\tPrec@5 70.000 (60.028)\n",
      "EVALUATING - Epoch: [0][360/5000]\tTime 0.031 (0.061)\tData 0.000 (0.005)\tLoss 0.7359 (3.1023)\tPrec@1 90.000 (34.571)\tPrec@5 100.000 (60.305)\n",
      "EVALUATING - Epoch: [0][370/5000]\tTime 0.049 (0.061)\tData 0.000 (0.005)\tLoss 2.1286 (3.0944)\tPrec@1 40.000 (34.690)\tPrec@5 80.000 (60.323)\n",
      "EVALUATING - Epoch: [0][380/5000]\tTime 0.028 (0.060)\tData 0.000 (0.005)\tLoss 2.6456 (3.0602)\tPrec@1 60.000 (35.407)\tPrec@5 70.000 (60.787)\n",
      "EVALUATING - Epoch: [0][390/5000]\tTime 0.027 (0.060)\tData 0.000 (0.005)\tLoss 3.0381 (3.0459)\tPrec@1 40.000 (35.627)\tPrec@5 40.000 (60.972)\n",
      "EVALUATING - Epoch: [0][400/5000]\tTime 0.142 (0.060)\tData 0.000 (0.005)\tLoss 1.3888 (3.0768)\tPrec@1 80.000 (35.187)\tPrec@5 90.000 (60.549)\n",
      "EVALUATING - Epoch: [0][410/5000]\tTime 0.051 (0.060)\tData 0.000 (0.005)\tLoss 3.1172 (3.0611)\tPrec@1 30.000 (35.450)\tPrec@5 50.000 (60.730)\n",
      "EVALUATING - Epoch: [0][420/5000]\tTime 0.055 (0.059)\tData 0.000 (0.005)\tLoss 4.0700 (3.0451)\tPrec@1 40.000 (35.796)\tPrec@5 40.000 (60.998)\n",
      "EVALUATING - Epoch: [0][430/5000]\tTime 0.055 (0.059)\tData 0.000 (0.005)\tLoss 1.3421 (3.0108)\tPrec@1 50.000 (36.497)\tPrec@5 90.000 (61.578)\n",
      "EVALUATING - Epoch: [0][440/5000]\tTime 0.051 (0.059)\tData 0.000 (0.004)\tLoss 2.0673 (2.9872)\tPrec@1 50.000 (36.961)\tPrec@5 80.000 (61.973)\n",
      "EVALUATING - Epoch: [0][450/5000]\tTime 0.065 (0.059)\tData 0.000 (0.004)\tLoss 0.7397 (2.9459)\tPrec@1 70.000 (37.871)\tPrec@5 100.000 (62.616)\n",
      "EVALUATING - Epoch: [0][460/5000]\tTime 0.052 (0.059)\tData 0.000 (0.004)\tLoss 1.1827 (2.9150)\tPrec@1 60.000 (38.308)\tPrec@5 90.000 (63.102)\n",
      "EVALUATING - Epoch: [0][470/5000]\tTime 0.051 (0.059)\tData 0.000 (0.004)\tLoss 1.0216 (2.8836)\tPrec@1 60.000 (38.832)\tPrec@5 100.000 (63.673)\n",
      "EVALUATING - Epoch: [0][480/5000]\tTime 0.051 (0.059)\tData 0.000 (0.004)\tLoss 1.1806 (2.8536)\tPrec@1 70.000 (39.459)\tPrec@5 90.000 (64.179)\n",
      "EVALUATING - Epoch: [0][490/5000]\tTime 0.049 (0.059)\tData 0.000 (0.004)\tLoss 2.3980 (2.8275)\tPrec@1 40.000 (39.959)\tPrec@5 70.000 (64.562)\n",
      "EVALUATING - Epoch: [0][500/5000]\tTime 0.049 (0.059)\tData 0.000 (0.004)\tLoss 0.7963 (2.8127)\tPrec@1 100.000 (40.200)\tPrec@5 100.000 (64.870)\n",
      "EVALUATING - Epoch: [0][510/5000]\tTime 0.054 (0.059)\tData 0.000 (0.004)\tLoss 1.4483 (2.7947)\tPrec@1 50.000 (40.489)\tPrec@5 90.000 (65.264)\n",
      "EVALUATING - Epoch: [0][520/5000]\tTime 0.052 (0.058)\tData 0.000 (0.004)\tLoss 2.7946 (2.7801)\tPrec@1 30.000 (40.614)\tPrec@5 80.000 (65.605)\n",
      "EVALUATING - Epoch: [0][530/5000]\tTime 0.053 (0.058)\tData 0.000 (0.004)\tLoss 5.0378 (2.8036)\tPrec@1 0.000 (40.301)\tPrec@5 20.000 (65.122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][540/5000]\tTime 0.057 (0.058)\tData 0.000 (0.004)\tLoss 2.0687 (2.8226)\tPrec@1 40.000 (40.129)\tPrec@5 70.000 (64.787)\n",
      "EVALUATING - Epoch: [0][550/5000]\tTime 0.054 (0.058)\tData 0.000 (0.004)\tLoss 3.6695 (2.8129)\tPrec@1 20.000 (40.109)\tPrec@5 60.000 (64.991)\n",
      "EVALUATING - Epoch: [0][560/5000]\tTime 0.048 (0.058)\tData 0.000 (0.004)\tLoss 3.2118 (2.8152)\tPrec@1 50.000 (40.000)\tPrec@5 60.000 (65.045)\n",
      "EVALUATING - Epoch: [0][570/5000]\tTime 0.052 (0.058)\tData 0.000 (0.004)\tLoss 4.9135 (2.8378)\tPrec@1 10.000 (39.667)\tPrec@5 20.000 (64.676)\n",
      "EVALUATING - Epoch: [0][580/5000]\tTime 0.055 (0.058)\tData 0.000 (0.004)\tLoss 2.3143 (2.8488)\tPrec@1 40.000 (39.449)\tPrec@5 80.000 (64.509)\n",
      "EVALUATING - Epoch: [0][590/5000]\tTime 0.050 (0.058)\tData 0.000 (0.003)\tLoss 3.0146 (2.8411)\tPrec@1 30.000 (39.662)\tPrec@5 70.000 (64.619)\n",
      "EVALUATING - Epoch: [0][600/5000]\tTime 0.051 (0.058)\tData 0.000 (0.003)\tLoss 2.1911 (2.8642)\tPrec@1 60.000 (39.201)\tPrec@5 60.000 (64.193)\n",
      "EVALUATING - Epoch: [0][610/5000]\tTime 0.054 (0.058)\tData 0.000 (0.003)\tLoss 2.4565 (2.8734)\tPrec@1 30.000 (38.953)\tPrec@5 90.000 (64.141)\n",
      "EVALUATING - Epoch: [0][620/5000]\tTime 0.055 (0.058)\tData 0.000 (0.003)\tLoss 4.2569 (2.8809)\tPrec@1 10.000 (38.808)\tPrec@5 20.000 (63.929)\n",
      "EVALUATING - Epoch: [0][630/5000]\tTime 0.050 (0.058)\tData 0.000 (0.003)\tLoss 2.7224 (2.9020)\tPrec@1 30.000 (38.352)\tPrec@5 80.000 (63.582)\n",
      "EVALUATING - Epoch: [0][640/5000]\tTime 0.150 (0.058)\tData 0.000 (0.003)\tLoss 2.2833 (2.9015)\tPrec@1 30.000 (38.393)\tPrec@5 80.000 (63.635)\n",
      "EVALUATING - Epoch: [0][650/5000]\tTime 0.051 (0.058)\tData 0.000 (0.003)\tLoss 2.0972 (2.8928)\tPrec@1 50.000 (38.387)\tPrec@5 80.000 (63.810)\n",
      "EVALUATING - Epoch: [0][660/5000]\tTime 0.055 (0.058)\tData 0.000 (0.003)\tLoss 1.6599 (2.8806)\tPrec@1 50.000 (38.563)\tPrec@5 90.000 (64.054)\n",
      "EVALUATING - Epoch: [0][670/5000]\tTime 0.050 (0.058)\tData 0.000 (0.003)\tLoss 1.8945 (2.8713)\tPrec@1 40.000 (38.703)\tPrec@5 80.000 (64.262)\n",
      "EVALUATING - Epoch: [0][680/5000]\tTime 0.064 (0.058)\tData 0.000 (0.003)\tLoss 1.9386 (2.8618)\tPrec@1 60.000 (38.781)\tPrec@5 90.000 (64.435)\n",
      "EVALUATING - Epoch: [0][690/5000]\tTime 0.056 (0.058)\tData 0.000 (0.003)\tLoss 2.0200 (2.8437)\tPrec@1 50.000 (39.088)\tPrec@5 80.000 (64.660)\n",
      "EVALUATING - Epoch: [0][700/5000]\tTime 0.054 (0.058)\tData 0.000 (0.003)\tLoss 3.2536 (2.8318)\tPrec@1 20.000 (39.358)\tPrec@5 50.000 (64.793)\n",
      "EVALUATING - Epoch: [0][710/5000]\tTime 0.053 (0.058)\tData 0.000 (0.003)\tLoss 2.7202 (2.8347)\tPrec@1 30.000 (39.269)\tPrec@5 50.000 (64.740)\n",
      "EVALUATING - Epoch: [0][720/5000]\tTime 0.053 (0.058)\tData 0.000 (0.003)\tLoss 1.3678 (2.8261)\tPrec@1 80.000 (39.473)\tPrec@5 90.000 (64.854)\n",
      "EVALUATING - Epoch: [0][730/5000]\tTime 0.060 (0.058)\tData 0.000 (0.003)\tLoss 3.0544 (2.8123)\tPrec@1 40.000 (39.808)\tPrec@5 70.000 (65.048)\n",
      "EVALUATING - Epoch: [0][740/5000]\tTime 0.052 (0.058)\tData 0.000 (0.003)\tLoss 1.2699 (2.8033)\tPrec@1 70.000 (40.013)\tPrec@5 100.000 (65.304)\n",
      "EVALUATING - Epoch: [0][750/5000]\tTime 0.059 (0.058)\tData 0.000 (0.003)\tLoss 2.7748 (2.7851)\tPrec@1 40.000 (40.346)\tPrec@5 50.000 (65.553)\n",
      "EVALUATING - Epoch: [0][760/5000]\tTime 0.053 (0.058)\tData 0.000 (0.003)\tLoss 2.1046 (2.7881)\tPrec@1 0.000 (40.184)\tPrec@5 100.000 (65.532)\n",
      "EVALUATING - Epoch: [0][770/5000]\tTime 0.050 (0.062)\tData 0.000 (0.007)\tLoss 2.6217 (2.7803)\tPrec@1 60.000 (40.130)\tPrec@5 60.000 (65.694)\n",
      "EVALUATING - Epoch: [0][780/5000]\tTime 0.051 (0.063)\tData 0.000 (0.009)\tLoss 0.7580 (2.7759)\tPrec@1 80.000 (40.064)\tPrec@5 100.000 (65.762)\n",
      "EVALUATING - Epoch: [0][790/5000]\tTime 0.053 (0.063)\tData 0.000 (0.009)\tLoss 3.9747 (2.7542)\tPrec@1 10.000 (40.518)\tPrec@5 50.000 (66.106)\n",
      "EVALUATING - Epoch: [0][800/5000]\tTime 0.047 (0.063)\tData 0.000 (0.009)\tLoss 2.8832 (2.7572)\tPrec@1 20.000 (40.449)\tPrec@5 70.000 (66.092)\n",
      "EVALUATING - Epoch: [0][810/5000]\tTime 0.053 (0.063)\tData 0.000 (0.009)\tLoss 2.0077 (2.7478)\tPrec@1 30.000 (40.617)\tPrec@5 80.000 (66.264)\n",
      "EVALUATING - Epoch: [0][820/5000]\tTime 0.050 (0.063)\tData 0.000 (0.008)\tLoss 2.3401 (2.7558)\tPrec@1 50.000 (40.402)\tPrec@5 80.000 (66.139)\n",
      "EVALUATING - Epoch: [0][830/5000]\tTime 0.047 (0.063)\tData 0.000 (0.008)\tLoss 2.6808 (2.7654)\tPrec@1 40.000 (40.060)\tPrec@5 60.000 (66.005)\n",
      "EVALUATING - Epoch: [0][840/5000]\tTime 0.053 (0.063)\tData 0.000 (0.008)\tLoss 3.0407 (2.7725)\tPrec@1 0.000 (39.750)\tPrec@5 80.000 (65.898)\n",
      "EVALUATING - Epoch: [0][850/5000]\tTime 0.053 (0.063)\tData 0.000 (0.008)\tLoss 5.2933 (2.7718)\tPrec@1 10.000 (39.530)\tPrec@5 20.000 (65.993)\n",
      "EVALUATING - Epoch: [0][860/5000]\tTime 0.057 (0.063)\tData 0.000 (0.008)\tLoss 3.8541 (2.7799)\tPrec@1 0.000 (39.419)\tPrec@5 40.000 (65.865)\n",
      "EVALUATING - Epoch: [0][870/5000]\tTime 0.056 (0.063)\tData 0.000 (0.008)\tLoss 2.7516 (2.7862)\tPrec@1 50.000 (39.323)\tPrec@5 70.000 (65.706)\n",
      "EVALUATING - Epoch: [0][880/5000]\tTime 0.051 (0.062)\tData 0.000 (0.008)\tLoss 2.8433 (2.7979)\tPrec@1 30.000 (39.103)\tPrec@5 60.000 (65.494)\n",
      "EVALUATING - Epoch: [0][890/5000]\tTime 0.054 (0.062)\tData 0.000 (0.008)\tLoss 1.3622 (2.8055)\tPrec@1 70.000 (38.956)\tPrec@5 90.000 (65.320)\n",
      "EVALUATING - Epoch: [0][900/5000]\tTime 0.051 (0.062)\tData 0.000 (0.008)\tLoss 2.5493 (2.8031)\tPrec@1 10.000 (38.946)\tPrec@5 60.000 (65.283)\n",
      "EVALUATING - Epoch: [0][910/5000]\tTime 0.052 (0.062)\tData 0.000 (0.008)\tLoss 2.8636 (2.8072)\tPrec@1 30.000 (38.760)\tPrec@5 50.000 (65.115)\n",
      "EVALUATING - Epoch: [0][920/5000]\tTime 0.057 (0.062)\tData 0.000 (0.008)\tLoss 2.4900 (2.8076)\tPrec@1 50.000 (38.817)\tPrec@5 70.000 (65.092)\n",
      "EVALUATING - Epoch: [0][930/5000]\tTime 0.146 (0.062)\tData 0.000 (0.007)\tLoss 3.3358 (2.8100)\tPrec@1 10.000 (38.711)\tPrec@5 40.000 (65.038)\n",
      "EVALUATING - Epoch: [0][940/5000]\tTime 0.060 (0.062)\tData 0.000 (0.007)\tLoss 2.8228 (2.8170)\tPrec@1 40.000 (38.459)\tPrec@5 80.000 (64.973)\n",
      "EVALUATING - Epoch: [0][950/5000]\tTime 0.059 (0.062)\tData 0.000 (0.007)\tLoss 2.6489 (2.8278)\tPrec@1 20.000 (38.212)\tPrec@5 80.000 (64.795)\n",
      "EVALUATING - Epoch: [0][960/5000]\tTime 0.059 (0.062)\tData 0.000 (0.007)\tLoss 3.4103 (2.8325)\tPrec@1 30.000 (38.106)\tPrec@5 60.000 (64.703)\n",
      "EVALUATING - Epoch: [0][970/5000]\tTime 0.060 (0.062)\tData 0.000 (0.007)\tLoss 3.6374 (2.8350)\tPrec@1 20.000 (38.002)\tPrec@5 50.000 (64.655)\n",
      "EVALUATING - Epoch: [0][980/5000]\tTime 0.061 (0.062)\tData 0.000 (0.007)\tLoss 2.0029 (2.8328)\tPrec@1 60.000 (37.992)\tPrec@5 80.000 (64.689)\n",
      "EVALUATING - Epoch: [0][990/5000]\tTime 0.059 (0.062)\tData 0.000 (0.007)\tLoss 1.9613 (2.8312)\tPrec@1 30.000 (37.962)\tPrec@5 70.000 (64.743)\n",
      "EVALUATING - Epoch: [0][1000/5000]\tTime 0.060 (0.062)\tData 0.000 (0.007)\tLoss 3.8148 (2.8362)\tPrec@1 0.000 (37.742)\tPrec@5 50.000 (64.695)\n",
      "EVALUATING - Epoch: [0][1010/5000]\tTime 0.050 (0.062)\tData 0.000 (0.007)\tLoss 4.0168 (2.8386)\tPrec@1 10.000 (37.606)\tPrec@5 40.000 (64.629)\n",
      "EVALUATING - Epoch: [0][1020/5000]\tTime 0.048 (0.062)\tData 0.000 (0.007)\tLoss 1.9999 (2.8367)\tPrec@1 60.000 (37.591)\tPrec@5 90.000 (64.643)\n",
      "EVALUATING - Epoch: [0][1030/5000]\tTime 0.053 (0.062)\tData 0.000 (0.007)\tLoss 3.8601 (2.8323)\tPrec@1 0.000 (37.633)\tPrec@5 30.000 (64.685)\n",
      "EVALUATING - Epoch: [0][1040/5000]\tTime 0.093 (0.062)\tData 0.000 (0.007)\tLoss 2.4125 (2.8391)\tPrec@1 30.000 (37.512)\tPrec@5 70.000 (64.553)\n",
      "EVALUATING - Epoch: [0][1050/5000]\tTime 0.057 (0.062)\tData 0.000 (0.007)\tLoss 3.9415 (2.8434)\tPrec@1 30.000 (37.393)\tPrec@5 40.000 (64.462)\n",
      "EVALUATING - Epoch: [0][1060/5000]\tTime 0.061 (0.062)\tData 0.000 (0.007)\tLoss 2.0490 (2.8477)\tPrec@1 60.000 (37.191)\tPrec@5 80.000 (64.383)\n",
      "EVALUATING - Epoch: [0][1070/5000]\tTime 0.051 (0.062)\tData 0.000 (0.007)\tLoss 2.9855 (2.8442)\tPrec@1 60.000 (37.227)\tPrec@5 80.000 (64.444)\n",
      "EVALUATING - Epoch: [0][1080/5000]\tTime 0.053 (0.062)\tData 0.000 (0.007)\tLoss 2.1567 (2.8421)\tPrec@1 60.000 (37.188)\tPrec@5 60.000 (64.505)\n",
      "EVALUATING - Epoch: [0][1090/5000]\tTime 0.052 (0.062)\tData 0.000 (0.006)\tLoss 1.8208 (2.8296)\tPrec@1 40.000 (37.461)\tPrec@5 100.000 (64.748)\n",
      "EVALUATING - Epoch: [0][1100/5000]\tTime 0.049 (0.062)\tData 0.000 (0.006)\tLoss 3.6961 (2.8300)\tPrec@1 30.000 (37.275)\tPrec@5 60.000 (64.768)\n",
      "EVALUATING - Epoch: [0][1110/5000]\tTime 0.053 (0.062)\tData 0.000 (0.006)\tLoss 1.8584 (2.8270)\tPrec@1 50.000 (37.318)\tPrec@5 90.000 (64.851)\n",
      "EVALUATING - Epoch: [0][1120/5000]\tTime 0.063 (0.062)\tData 0.000 (0.006)\tLoss 2.8078 (2.8269)\tPrec@1 40.000 (37.306)\tPrec@5 70.000 (64.888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][1130/5000]\tTime 0.056 (0.062)\tData 0.000 (0.006)\tLoss 4.8854 (2.8243)\tPrec@1 10.000 (37.409)\tPrec@5 30.000 (64.951)\n",
      "EVALUATING - Epoch: [0][1140/5000]\tTime 0.057 (0.061)\tData 0.000 (0.006)\tLoss 3.6886 (2.8326)\tPrec@1 30.000 (37.309)\tPrec@5 60.000 (64.812)\n",
      "EVALUATING - Epoch: [0][1150/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 1.3710 (2.8288)\tPrec@1 50.000 (37.341)\tPrec@5 90.000 (64.857)\n",
      "EVALUATING - Epoch: [0][1160/5000]\tTime 0.061 (0.061)\tData 0.000 (0.006)\tLoss 2.5221 (2.8218)\tPrec@1 50.000 (37.347)\tPrec@5 60.000 (64.961)\n",
      "EVALUATING - Epoch: [0][1170/5000]\tTime 0.059 (0.061)\tData 0.000 (0.006)\tLoss 2.4675 (2.8273)\tPrec@1 50.000 (37.199)\tPrec@5 70.000 (64.842)\n",
      "EVALUATING - Epoch: [0][1180/5000]\tTime 0.058 (0.061)\tData 0.000 (0.006)\tLoss 3.1728 (2.8266)\tPrec@1 30.000 (37.172)\tPrec@5 60.000 (64.936)\n",
      "EVALUATING - Epoch: [0][1190/5000]\tTime 0.064 (0.061)\tData 0.000 (0.006)\tLoss 2.8026 (2.8265)\tPrec@1 20.000 (37.145)\tPrec@5 60.000 (64.962)\n",
      "EVALUATING - Epoch: [0][1200/5000]\tTime 0.053 (0.062)\tData 0.000 (0.006)\tLoss 1.6055 (2.8201)\tPrec@1 70.000 (37.202)\tPrec@5 80.000 (65.096)\n",
      "EVALUATING - Epoch: [0][1210/5000]\tTime 0.060 (0.062)\tData 0.000 (0.006)\tLoss 2.8998 (2.8146)\tPrec@1 30.000 (37.168)\tPrec@5 70.000 (65.260)\n",
      "EVALUATING - Epoch: [0][1220/5000]\tTime 0.066 (0.062)\tData 0.000 (0.006)\tLoss 1.9009 (2.8073)\tPrec@1 30.000 (37.224)\tPrec@5 100.000 (65.430)\n",
      "EVALUATING - Epoch: [0][1230/5000]\tTime 0.050 (0.061)\tData 0.000 (0.006)\tLoss 3.3335 (2.8036)\tPrec@1 30.000 (37.303)\tPrec@5 60.000 (65.508)\n",
      "EVALUATING - Epoch: [0][1240/5000]\tTime 0.052 (0.061)\tData 0.000 (0.006)\tLoss 3.1304 (2.7982)\tPrec@1 10.000 (37.381)\tPrec@5 60.000 (65.608)\n",
      "EVALUATING - Epoch: [0][1250/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 2.3120 (2.7987)\tPrec@1 40.000 (37.154)\tPrec@5 80.000 (65.659)\n",
      "EVALUATING - Epoch: [0][1260/5000]\tTime 0.052 (0.061)\tData 0.000 (0.006)\tLoss 1.1467 (2.7878)\tPrec@1 90.000 (37.431)\tPrec@5 90.000 (65.813)\n",
      "EVALUATING - Epoch: [0][1270/5000]\tTime 0.061 (0.061)\tData 0.000 (0.006)\tLoss 2.6129 (2.7823)\tPrec@1 30.000 (37.537)\tPrec@5 70.000 (65.924)\n",
      "EVALUATING - Epoch: [0][1280/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 2.5443 (2.7735)\tPrec@1 50.000 (37.760)\tPrec@5 70.000 (66.066)\n",
      "EVALUATING - Epoch: [0][1290/5000]\tTime 0.056 (0.061)\tData 0.000 (0.006)\tLoss 1.4263 (2.7711)\tPrec@1 90.000 (37.769)\tPrec@5 90.000 (66.112)\n",
      "EVALUATING - Epoch: [0][1300/5000]\tTime 0.052 (0.061)\tData 0.000 (0.006)\tLoss 1.5408 (2.7591)\tPrec@1 60.000 (37.978)\tPrec@5 100.000 (66.326)\n",
      "EVALUATING - Epoch: [0][1310/5000]\tTime 0.056 (0.061)\tData 0.000 (0.005)\tLoss 3.2572 (2.7513)\tPrec@1 30.000 (38.146)\tPrec@5 60.000 (66.453)\n",
      "EVALUATING - Epoch: [0][1320/5000]\tTime 0.056 (0.061)\tData 0.000 (0.005)\tLoss 3.8337 (2.7525)\tPrec@1 0.000 (38.070)\tPrec@5 40.000 (66.382)\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion, 0, quantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09921024739742279"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0][0].weight.data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:33: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "EVALUATING - Epoch: [0][0/1563]\tTime 12.467 (12.467)\tData 0.731 (0.731)\tLoss 0.7861 (0.7861)\tPrec@1 87.500 (87.500)\tPrec@5 87.500 (87.500)\n",
      "EVALUATING - Epoch: [0][10/1563]\tTime 0.890 (2.030)\tData 0.000 (0.067)\tLoss 3.5623 (2.1899)\tPrec@1 9.375 (48.295)\tPrec@5 46.875 (76.705)\n",
      "EVALUATING - Epoch: [0][20/1563]\tTime 0.820 (1.532)\tData 0.000 (0.035)\tLoss 2.4930 (2.4698)\tPrec@1 56.250 (47.173)\tPrec@5 65.625 (72.024)\n",
      "EVALUATING - Epoch: [0][30/1563]\tTime 0.770 (1.369)\tData 0.000 (0.024)\tLoss 3.3504 (2.4294)\tPrec@1 21.875 (47.581)\tPrec@5 43.750 (72.379)\n",
      "EVALUATING - Epoch: [0][40/1563]\tTime 0.820 (1.286)\tData 0.000 (0.018)\tLoss 1.9706 (2.5265)\tPrec@1 62.500 (47.104)\tPrec@5 71.875 (70.351)\n",
      "EVALUATING - Epoch: [0][50/1563]\tTime 1.312 (1.241)\tData 0.000 (0.015)\tLoss 5.4116 (2.8164)\tPrec@1 0.000 (42.157)\tPrec@5 15.625 (66.544)\n",
      "EVALUATING - Epoch: [0][60/1563]\tTime 0.824 (1.194)\tData 0.000 (0.012)\tLoss 4.7464 (2.9200)\tPrec@1 15.625 (39.805)\tPrec@5 25.000 (64.805)\n",
      "EVALUATING - Epoch: [0][70/1563]\tTime 0.877 (1.166)\tData 0.000 (0.011)\tLoss 3.8762 (3.1567)\tPrec@1 18.750 (35.431)\tPrec@5 50.000 (59.683)\n",
      "EVALUATING - Epoch: [0][80/1563]\tTime 0.951 (1.148)\tData 0.000 (0.009)\tLoss 5.8859 (3.2523)\tPrec@1 12.500 (34.066)\tPrec@5 18.750 (58.333)\n",
      "EVALUATING - Epoch: [0][90/1563]\tTime 0.876 (1.130)\tData 0.000 (0.009)\tLoss 4.2909 (3.3466)\tPrec@1 12.500 (31.834)\tPrec@5 31.250 (56.628)\n",
      "EVALUATING - Epoch: [0][100/1563]\tTime 1.190 (1.110)\tData 0.000 (0.008)\tLoss 4.2443 (3.4538)\tPrec@1 12.500 (29.765)\tPrec@5 43.750 (54.765)\n",
      "EVALUATING - Epoch: [0][110/1563]\tTime 1.337 (1.100)\tData 0.000 (0.007)\tLoss 3.1694 (3.4570)\tPrec@1 37.500 (29.307)\tPrec@5 53.125 (54.758)\n",
      "EVALUATING - Epoch: [0][120/1563]\tTime 1.048 (1.089)\tData 0.000 (0.007)\tLoss 2.8059 (3.4458)\tPrec@1 34.375 (29.778)\tPrec@5 65.625 (54.804)\n",
      "EVALUATING - Epoch: [0][130/1563]\tTime 0.774 (1.083)\tData 0.000 (0.006)\tLoss 1.8274 (3.4530)\tPrec@1 59.375 (29.461)\tPrec@5 78.125 (54.532)\n",
      "EVALUATING - Epoch: [0][140/1563]\tTime 0.805 (1.074)\tData 0.000 (0.006)\tLoss 2.5287 (3.3638)\tPrec@1 50.000 (31.228)\tPrec@5 81.250 (56.095)\n",
      "EVALUATING - Epoch: [0][150/1563]\tTime 1.438 (1.072)\tData 0.000 (0.005)\tLoss 3.3961 (3.3301)\tPrec@1 18.750 (31.602)\tPrec@5 62.500 (56.850)\n",
      "EVALUATING - Epoch: [0][160/1563]\tTime 0.898 (1.065)\tData 0.000 (0.005)\tLoss 1.2440 (3.3036)\tPrec@1 71.875 (32.220)\tPrec@5 90.625 (57.531)\n",
      "EVALUATING - Epoch: [0][170/1563]\tTime 1.671 (1.064)\tData 0.000 (0.005)\tLoss 2.2097 (3.3189)\tPrec@1 43.750 (32.036)\tPrec@5 68.750 (56.999)\n",
      "EVALUATING - Epoch: [0][180/1563]\tTime 0.720 (1.056)\tData 0.000 (0.005)\tLoss 4.1735 (3.3612)\tPrec@1 9.375 (31.319)\tPrec@5 37.500 (56.336)\n",
      "EVALUATING - Epoch: [0][190/1563]\tTime 0.827 (1.051)\tData 0.000 (0.004)\tLoss 3.0070 (3.3936)\tPrec@1 15.625 (30.481)\tPrec@5 65.625 (55.759)\n",
      "EVALUATING - Epoch: [0][200/1563]\tTime 0.798 (1.049)\tData 0.000 (0.004)\tLoss 4.0025 (3.4041)\tPrec@1 12.500 (30.239)\tPrec@5 53.125 (55.535)\n",
      "EVALUATING - Epoch: [0][210/1563]\tTime 0.758 (1.044)\tData 0.000 (0.004)\tLoss 5.6285 (3.4173)\tPrec@1 6.250 (29.887)\tPrec@5 21.875 (55.687)\n",
      "EVALUATING - Epoch: [0][220/1563]\tTime 1.341 (1.043)\tData 0.000 (0.004)\tLoss 2.7478 (3.3697)\tPrec@1 50.000 (30.953)\tPrec@5 71.875 (56.533)\n",
      "EVALUATING - Epoch: [0][230/1563]\tTime 0.788 (1.037)\tData 0.000 (0.004)\tLoss 3.1757 (3.3423)\tPrec@1 43.750 (31.521)\tPrec@5 65.625 (57.035)\n",
      "EVALUATING - Epoch: [0][240/1563]\tTime 1.234 (1.035)\tData 0.000 (0.003)\tLoss 3.2396 (3.3382)\tPrec@1 25.000 (31.406)\tPrec@5 53.125 (56.963)\n",
      "Process Process-8:\n",
      "Process Process-1:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Process Process-4:\n",
      "Process Process-7:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-abfda8629711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_prec5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cbe724eb105c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(data_loader, model, criterion, epoch, quantizer, gpus)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     return forward(data_loader, model, criterion, epoch,\n\u001b[0;32m---> 85\u001b[0;31m                    training=False, optimizer=None, quantizer=quantizer)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cbe724eb105c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(data_loader, model, criterion, epoch, training, optimizer, quantizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# quantization before computing output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_and_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_quant_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_float_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/quantop.py\u001b[0m in \u001b[0;36mstore_and_quantize\u001b[0;34m(self, get_quant_params, update_float_params)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;31m# defold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mweight_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_defold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_quant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;31m# copy back into floating point model, not needed if not defold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/quantop.py\u001b[0m in \u001b[0;36m_batch_defold\u001b[0;34m(self, weight_tensor, batch_layer)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_out_channel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_tensor_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mgamma_over_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma_over_sigma_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_out_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mgamma_over_sigma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mweight_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_out_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gamma over sigma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run validation\n",
    "val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion, 0, quantizer, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.2059,  5.9497,  1.0535, 10.2415, 28.1148, 12.4720,  3.4225,  2.6865,\n",
       "        11.5736,  1.9509,  5.7577,  3.0689,  5.4406,  1.8761, 16.6637,  0.1006,\n",
       "         7.2505, 17.0050,  3.0287, 10.2209,  2.5811,  2.2394, 10.0781, 19.4257,\n",
       "         0.2328,  6.2219,  0.2076,  7.0416,  3.6740, 17.6075,  9.2285,  0.3068],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model[0][1].running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0109,  0.0000, -0.0055],\n",
       "          [ 0.0109, -0.0000, -0.0073],\n",
       "          [ 0.0109,  0.0000, -0.0073]],\n",
       "\n",
       "         [[ 0.0000, -0.0036, -0.0036],\n",
       "          [ 0.0073,  0.0000, -0.0091],\n",
       "          [ 0.0128,  0.0055,  0.0000]],\n",
       "\n",
       "         [[-0.0273, -0.0292, -0.0255],\n",
       "          [-0.0219, -0.0219, -0.0292],\n",
       "          [-0.0201, -0.0164, -0.0164]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0055,  0.0729,  0.0474],\n",
       "          [-0.0182, -0.0018,  0.0036],\n",
       "          [-0.0456, -0.1130, -0.0547]],\n",
       "\n",
       "         [[ 0.1039,  0.1823,  0.1294],\n",
       "          [-0.0219, -0.0292,  0.0073],\n",
       "          [-0.1659, -0.2826, -0.1513]],\n",
       "\n",
       "         [[ 0.0018,  0.0547,  0.0401],\n",
       "          [-0.0036,  0.0073,  0.0146],\n",
       "          [-0.0438, -0.0674, -0.0182]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0201,  0.0438,  0.0346],\n",
       "          [ 0.0201,  0.0219, -0.0018],\n",
       "          [ 0.0000,  0.0164,  0.0055]],\n",
       "\n",
       "         [[-0.0109,  0.0128, -0.0073],\n",
       "          [-0.0036, -0.0109, -0.0255],\n",
       "          [-0.0164, -0.0292, -0.0365]],\n",
       "\n",
       "         [[-0.0273, -0.0292, -0.0310],\n",
       "          [-0.0292, -0.0255, -0.0237],\n",
       "          [-0.0219, -0.0073, -0.0255]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0602,  0.0529,  0.0456],\n",
       "          [ 0.0656,  0.0583,  0.0365],\n",
       "          [ 0.0401,  0.0310,  0.0146]],\n",
       "\n",
       "         [[-0.0456, -0.0620, -0.0766],\n",
       "          [-0.0820, -0.1094, -0.1276],\n",
       "          [-0.1203, -0.1331, -0.1349]],\n",
       "\n",
       "         [[ 0.0346,  0.0529,  0.0766],\n",
       "          [ 0.0820,  0.1112,  0.1021],\n",
       "          [ 0.0984,  0.1076,  0.0966]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0346,  0.1112,  0.1258],\n",
       "          [ 0.0820,  0.1604,  0.1659],\n",
       "          [ 0.0911,  0.1294,  0.1313]],\n",
       "\n",
       "         [[-0.0711, -0.0747, -0.0802],\n",
       "          [-0.0747, -0.0656, -0.0602],\n",
       "          [-0.0474, -0.0656, -0.0729]],\n",
       "\n",
       "         [[ 0.0237,  0.0000, -0.0055],\n",
       "          [-0.0091, -0.0328, -0.0292],\n",
       "          [-0.0346, -0.0620, -0.0620]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0146,  0.0456,  0.0456],\n",
       "          [ 0.0565,  0.0930,  0.0693],\n",
       "          [ 0.0711,  0.0948,  0.0711]],\n",
       "\n",
       "         [[-0.0292, -0.0237, -0.0146],\n",
       "          [ 0.0000,  0.0146, -0.0018],\n",
       "          [ 0.0292,  0.0474,  0.0073]],\n",
       "\n",
       "         [[-0.0255, -0.0219, -0.0273],\n",
       "          [-0.0109,  0.0273,  0.0164],\n",
       "          [ 0.0018,  0.0164, -0.0109]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0182,  0.0109, -0.0164],\n",
       "          [ 0.0018,  0.0565, -0.0018],\n",
       "          [-0.0164,  0.0529,  0.0164]],\n",
       "\n",
       "         [[ 0.0109,  0.0328,  0.0237],\n",
       "          [-0.0055,  0.0583,  0.0237],\n",
       "          [ 0.0036,  0.0930,  0.0711]],\n",
       "\n",
       "         [[-0.0036,  0.0182, -0.0456],\n",
       "          [-0.0091,  0.0529, -0.0383],\n",
       "          [ 0.0073,  0.0383, -0.0164]]],\n",
       "\n",
       "\n",
       "        [[[-0.0328, -0.0164, -0.0219],\n",
       "          [-0.0055, -0.0201, -0.0128],\n",
       "          [-0.0073, -0.0164, -0.0164]],\n",
       "\n",
       "         [[-0.0091,  0.0018, -0.0146],\n",
       "          [-0.0000,  0.0073,  0.0091],\n",
       "          [ 0.0146, -0.0073,  0.0055]],\n",
       "\n",
       "         [[-0.0073, -0.0182, -0.0201],\n",
       "          [-0.0091, -0.0182, -0.0219],\n",
       "          [ 0.0073, -0.0055, -0.0055]]],\n",
       "\n",
       "\n",
       "        [[[-0.0018, -0.0036, -0.0018],\n",
       "          [-0.0036, -0.0055, -0.0036],\n",
       "          [-0.0055, -0.0055, -0.0055]],\n",
       "\n",
       "         [[-0.0000, -0.0036,  0.0018],\n",
       "          [-0.0055, -0.0036, -0.0055],\n",
       "          [-0.0055, -0.0036, -0.0055]],\n",
       "\n",
       "         [[ 0.0018, -0.0018, -0.0018],\n",
       "          [-0.0018, -0.0036, -0.0036],\n",
       "          [-0.0018, -0.0055, -0.0073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018,  0.0000,  0.0036],\n",
       "          [-0.0018,  0.0018,  0.0036],\n",
       "          [-0.0036,  0.0055,  0.0073]],\n",
       "\n",
       "         [[ 0.0036, -0.0000,  0.0000],\n",
       "          [-0.0018,  0.0018,  0.0055],\n",
       "          [ 0.0000,  0.0073,  0.0055]],\n",
       "\n",
       "         [[ 0.0000,  0.0036,  0.0055],\n",
       "          [ 0.0036,  0.0036,  0.0055],\n",
       "          [ 0.0073,  0.0036,  0.0036]]],\n",
       "\n",
       "\n",
       "        [[[-0.0273, -0.0182, -0.0073],\n",
       "          [ 0.0055,  0.0146, -0.0055],\n",
       "          [ 0.0383,  0.0310, -0.0164]],\n",
       "\n",
       "         [[-0.0273, -0.0128,  0.0109],\n",
       "          [ 0.0164,  0.0273,  0.0091],\n",
       "          [ 0.0529,  0.0456, -0.0346]],\n",
       "\n",
       "         [[-0.0128,  0.0055,  0.0219],\n",
       "          [ 0.0091,  0.0383,  0.0055],\n",
       "          [ 0.0419,  0.0401, -0.0328]]],\n",
       "\n",
       "\n",
       "        [[[-0.0036,  0.0182,  0.0839],\n",
       "          [ 0.0346,  0.0146,  0.0620],\n",
       "          [ 0.0000,  0.0018,  0.0219]],\n",
       "\n",
       "         [[-0.0602, -0.0036,  0.0164],\n",
       "          [-0.0383, -0.0055,  0.0000],\n",
       "          [-0.0201, -0.0219, -0.0292]],\n",
       "\n",
       "         [[-0.0875, -0.0638,  0.0219],\n",
       "          [-0.0547, -0.0474, -0.0255],\n",
       "          [-0.0711, -0.0583, -0.0456]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018, -0.0091, -0.0018],\n",
       "          [-0.0091, -0.0000, -0.0073],\n",
       "          [-0.0091, -0.0091, -0.0128]],\n",
       "\n",
       "         [[ 0.0036, -0.0109, -0.0018],\n",
       "          [ 0.0036, -0.0073, -0.0109],\n",
       "          [-0.0018, -0.0036, -0.0091]],\n",
       "\n",
       "         [[-0.0036, -0.0055, -0.0164],\n",
       "          [-0.0146, -0.0164, -0.0164],\n",
       "          [-0.0146, -0.0128, -0.0219]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036,  0.0073,  0.0000],\n",
       "          [ 0.0018,  0.0073,  0.0000],\n",
       "          [-0.0000,  0.0073, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0055,  0.0018],\n",
       "          [ 0.0055,  0.0073, -0.0000],\n",
       "          [ 0.0018,  0.0055, -0.0000]],\n",
       "\n",
       "         [[ 0.0036,  0.0018, -0.0000],\n",
       "          [ 0.0000,  0.0018,  0.0073],\n",
       "          [ 0.0055,  0.0073,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0182, -0.0091, -0.0255],\n",
       "          [ 0.0255, -0.0164, -0.0784],\n",
       "          [ 0.0164, -0.0036, -0.0456]],\n",
       "\n",
       "         [[ 0.0419, -0.0146, -0.0383],\n",
       "          [ 0.0583, -0.0401, -0.1221],\n",
       "          [ 0.0383, -0.0237, -0.0784]],\n",
       "\n",
       "         [[ 0.0146, -0.0036, -0.0018],\n",
       "          [ 0.0182, -0.0036, -0.0365],\n",
       "          [ 0.0109,  0.0091, -0.0128]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0128,  0.0036,  0.0857],\n",
       "          [-0.0547,  0.0565,  0.0018],\n",
       "          [ 0.0346,  0.0529,  0.0620]],\n",
       "\n",
       "         [[ 0.0292,  0.0201, -0.0273],\n",
       "          [-0.0383,  0.0802, -0.0474],\n",
       "          [-0.0273,  0.0055, -0.0182]],\n",
       "\n",
       "         [[-0.0383,  0.0182, -0.0438],\n",
       "          [-0.0766, -0.0201, -0.0547],\n",
       "          [-0.0948, -0.0438, -0.0073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018,  0.0036,  0.0036],\n",
       "          [-0.0000, -0.0018,  0.0018],\n",
       "          [ 0.0055,  0.0055,  0.0055]],\n",
       "\n",
       "         [[ 0.0128,  0.0055,  0.0036],\n",
       "          [ 0.0073,  0.0073,  0.0109],\n",
       "          [ 0.0055,  0.0036,  0.0146]],\n",
       "\n",
       "         [[ 0.0055,  0.0091,  0.0073],\n",
       "          [ 0.0091,  0.0073,  0.0128],\n",
       "          [ 0.0128,  0.0128,  0.0164]]],\n",
       "\n",
       "\n",
       "        [[[-0.0365, -0.0273, -0.0255],\n",
       "          [-0.0565, -0.0438, -0.0273],\n",
       "          [-0.0383, -0.0747, -0.0729]],\n",
       "\n",
       "         [[ 0.0091,  0.0492,  0.0674],\n",
       "          [-0.0036,  0.0565,  0.0948],\n",
       "          [ 0.0292,  0.0365,  0.0456]],\n",
       "\n",
       "         [[-0.0036,  0.0438,  0.0383],\n",
       "          [ 0.0055,  0.0674,  0.0729],\n",
       "          [ 0.0237,  0.0474,  0.0474]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0128,  0.0073],\n",
       "          [ 0.0073,  0.0018,  0.0109],\n",
       "          [-0.0055, -0.0091,  0.0018]],\n",
       "\n",
       "         [[ 0.0073,  0.0201,  0.0219],\n",
       "          [-0.0073,  0.0128,  0.0201],\n",
       "          [ 0.0182,  0.0000,  0.0073]],\n",
       "\n",
       "         [[-0.0036,  0.0328,  0.0310],\n",
       "          [ 0.0018,  0.0109,  0.0219],\n",
       "          [ 0.0182,  0.0000,  0.0164]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0383,  0.0492,  0.0510],\n",
       "          [ 0.0310,  0.0237,  0.0365],\n",
       "          [ 0.0237,  0.0237,  0.0419]],\n",
       "\n",
       "         [[-0.1003, -0.0875, -0.0602],\n",
       "          [-0.1513, -0.1385, -0.1076],\n",
       "          [-0.1604, -0.1513, -0.0966]],\n",
       "\n",
       "         [[ 0.0036, -0.0018, -0.0073],\n",
       "          [ 0.0820,  0.0711,  0.0529],\n",
       "          [ 0.1130,  0.1076,  0.0839]]],\n",
       "\n",
       "\n",
       "        [[[-0.0036,  0.0091, -0.0091],\n",
       "          [ 0.0055,  0.0055, -0.0109],\n",
       "          [ 0.0091, -0.0055, -0.0146]],\n",
       "\n",
       "         [[-0.0146, -0.0365, -0.0292],\n",
       "          [-0.0201, -0.0310, -0.0164],\n",
       "          [-0.0237, -0.0000, -0.0219]],\n",
       "\n",
       "         [[-0.0292, -0.0036,  0.0000],\n",
       "          [-0.0164, -0.0255, -0.0219],\n",
       "          [ 0.0091, -0.0128, -0.0036]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0018,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0018,  0.0000],\n",
       "          [ 0.0018,  0.0018,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0018,  0.0000,  0.0018],\n",
       "          [ 0.0000,  0.0000,  0.0018]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0036,  0.0018,  0.0018],\n",
       "          [ 0.0000,  0.0018, -0.0018]],\n",
       "\n",
       "         [[ 0.0036,  0.0036,  0.0055],\n",
       "          [ 0.0036,  0.0036,  0.0055],\n",
       "          [ 0.0055,  0.0055,  0.0055]],\n",
       "\n",
       "         [[ 0.0036,  0.0036,  0.0036],\n",
       "          [ 0.0036,  0.0036,  0.0036],\n",
       "          [ 0.0073,  0.0036,  0.0055]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0164,  0.0128,  0.0109],\n",
       "          [ 0.0109,  0.0146,  0.0073],\n",
       "          [ 0.0109,  0.0164,  0.0146]],\n",
       "\n",
       "         [[ 0.0091,  0.0109,  0.0036],\n",
       "          [ 0.0073,  0.0036,  0.0036],\n",
       "          [ 0.0055,  0.0109,  0.0036]],\n",
       "\n",
       "         [[ 0.0109,  0.0128,  0.0055],\n",
       "          [ 0.0146,  0.0109,  0.0055],\n",
       "          [ 0.0091,  0.0055,  0.0055]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036, -0.0000, -0.0018],\n",
       "          [-0.0018, -0.0000,  0.0018],\n",
       "          [-0.0000, -0.0036, -0.0018]],\n",
       "\n",
       "         [[-0.0018, -0.0036, -0.0018],\n",
       "          [-0.0018, -0.0018, -0.0018],\n",
       "          [ 0.0018, -0.0018, -0.0018]],\n",
       "\n",
       "         [[-0.0036, -0.0018, -0.0036],\n",
       "          [ 0.0018, -0.0018, -0.0018],\n",
       "          [ 0.0018,  0.0018,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036, -0.0036,  0.0109],\n",
       "          [-0.0128, -0.0201,  0.0091],\n",
       "          [-0.0091, -0.0091,  0.0164]],\n",
       "\n",
       "         [[-0.0000, -0.0109,  0.0164],\n",
       "          [-0.0255, -0.0310, -0.0000],\n",
       "          [-0.0146, -0.0091,  0.0109]],\n",
       "\n",
       "         [[-0.0073, -0.0073,  0.0146],\n",
       "          [-0.0219, -0.0255, -0.0036],\n",
       "          [-0.0164, -0.0128,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0438, -0.0273,  0.0383],\n",
       "          [-0.0273, -0.0164,  0.0292],\n",
       "          [ 0.0510, -0.0365, -0.0456]],\n",
       "\n",
       "         [[ 0.0146,  0.0656,  0.0091],\n",
       "          [-0.0000,  0.0656, -0.0474],\n",
       "          [ 0.0602,  0.0529,  0.0255]],\n",
       "\n",
       "         [[-0.0201,  0.0438,  0.0620],\n",
       "          [ 0.0310,  0.0328,  0.0510],\n",
       "          [-0.0492, -0.0292, -0.0164]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0747,  0.0456,  0.0474],\n",
       "          [ 0.0328, -0.1130, -0.1112],\n",
       "          [ 0.0802, -0.0839, -0.0310]],\n",
       "\n",
       "         [[ 0.0128, -0.0164, -0.0237],\n",
       "          [ 0.0091, -0.1622, -0.1586],\n",
       "          [ 0.0237, -0.1094, -0.0857]],\n",
       "\n",
       "         [[ 0.0255,  0.0401,  0.0602],\n",
       "          [ 0.0328, -0.0911, -0.0674],\n",
       "          [ 0.0109, -0.0802, -0.0273]]],\n",
       "\n",
       "\n",
       "        [[[-0.0693, -0.0346, -0.0073],\n",
       "          [-0.0820, -0.0547, -0.0383],\n",
       "          [-0.0602, -0.0055,  0.0128]],\n",
       "\n",
       "         [[-0.0109,  0.0273,  0.0510],\n",
       "          [-0.0201,  0.0091,  0.0547],\n",
       "          [ 0.0310,  0.0565,  0.0784]],\n",
       "\n",
       "         [[ 0.0164,  0.0383,  0.0729],\n",
       "          [ 0.0201,  0.0401,  0.0474],\n",
       "          [ 0.0201,  0.0438,  0.0583]]],\n",
       "\n",
       "\n",
       "        [[[-0.0365, -0.1039, -0.0638],\n",
       "          [-0.1185, -0.1805, -0.1148],\n",
       "          [-0.0857, -0.1422, -0.0510]],\n",
       "\n",
       "         [[ 0.0492, -0.0146,  0.0055],\n",
       "          [ 0.0182, -0.0602, -0.0055],\n",
       "          [ 0.0018, -0.0419,  0.0328]],\n",
       "\n",
       "         [[ 0.0456,  0.0109,  0.0328],\n",
       "          [ 0.0182, -0.0346,  0.0383],\n",
       "          [ 0.0128, -0.0237,  0.0438]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0073,  0.0000,  0.0018],\n",
       "          [-0.0036, -0.0091,  0.0018],\n",
       "          [-0.0036, -0.0036, -0.0018]],\n",
       "\n",
       "         [[ 0.0109,  0.0073,  0.0073],\n",
       "          [-0.0036, -0.0018,  0.0036],\n",
       "          [-0.0036, -0.0036, -0.0036]],\n",
       "\n",
       "         [[-0.0128, -0.0146, -0.0201],\n",
       "          [-0.0201, -0.0273, -0.0201],\n",
       "          [-0.0237, -0.0201, -0.0219]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036,  0.0073, -0.0073],\n",
       "          [-0.0055, -0.0091, -0.0055],\n",
       "          [-0.0055, -0.0018,  0.0000]],\n",
       "\n",
       "         [[ 0.0073, -0.0036, -0.0018],\n",
       "          [ 0.0036, -0.0128, -0.0036],\n",
       "          [-0.0109, -0.0128, -0.0128]],\n",
       "\n",
       "         [[ 0.0091,  0.0109,  0.0073],\n",
       "          [ 0.0018, -0.0036, -0.0036],\n",
       "          [-0.0018,  0.0073,  0.0018]]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "quantizer.deployment_model.model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/782]\tTime 5.019 (5.019)\tData 3.639 (3.639)\tLoss 8.2795 (8.2795)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][10/782]\tTime 0.965 (1.337)\tData 0.000 (0.331)\tLoss 8.7712 (9.9373)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][20/782]\tTime 0.964 (1.161)\tData 0.000 (0.174)\tLoss 10.0164 (9.0190)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][30/782]\tTime 0.952 (1.098)\tData 0.000 (0.118)\tLoss 8.3803 (9.0293)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][40/782]\tTime 0.968 (1.065)\tData 0.000 (0.089)\tLoss 7.8637 (9.0348)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][50/782]\tTime 0.975 (1.043)\tData 0.000 (0.072)\tLoss 8.8594 (9.0806)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][60/782]\tTime 0.970 (1.029)\tData 0.000 (0.060)\tLoss 8.8920 (9.0228)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "EVALUATING - Epoch: [0][70/782]\tTime 0.964 (1.020)\tData 0.000 (0.052)\tLoss 6.5390 (8.8604)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.022)\n",
      "EVALUATING - Epoch: [0][80/782]\tTime 0.979 (1.012)\tData 0.000 (0.045)\tLoss 9.0713 (8.7113)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.965)\n",
      "EVALUATING - Epoch: [0][90/782]\tTime 0.984 (1.006)\tData 0.000 (0.040)\tLoss 8.4570 (8.7363)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.859)\n",
      "EVALUATING - Epoch: [0][100/782]\tTime 0.984 (1.002)\tData 0.000 (0.037)\tLoss 6.0219 (8.6701)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.774)\n",
      "EVALUATING - Epoch: [0][110/782]\tTime 0.963 (0.999)\tData 0.000 (0.033)\tLoss 10.2985 (8.6976)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.704)\n",
      "EVALUATING - Epoch: [0][120/782]\tTime 0.965 (0.994)\tData 0.000 (0.031)\tLoss 7.6817 (8.6362)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.646)\n",
      "EVALUATING - Epoch: [0][130/782]\tTime 0.952 (0.991)\tData 0.000 (0.028)\tLoss 8.5773 (8.5547)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.596)\n",
      "EVALUATING - Epoch: [0][140/782]\tTime 0.958 (0.990)\tData 0.000 (0.026)\tLoss 7.9203 (8.5285)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.554)\n",
      "EVALUATING - Epoch: [0][150/782]\tTime 0.989 (0.986)\tData 0.000 (0.025)\tLoss 8.4802 (8.4927)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.517)\n",
      "EVALUATING - Epoch: [0][160/782]\tTime 0.947 (0.985)\tData 0.000 (0.023)\tLoss 8.0160 (8.4403)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.485)\n",
      "EVALUATING - Epoch: [0][170/782]\tTime 0.959 (0.984)\tData 0.000 (0.022)\tLoss 8.0115 (8.3631)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.457)\n",
      "EVALUATING - Epoch: [0][180/782]\tTime 0.955 (0.982)\tData 0.000 (0.021)\tLoss 7.4375 (8.3259)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.432)\n",
      "EVALUATING - Epoch: [0][190/782]\tTime 0.965 (0.981)\tData 0.000 (0.020)\tLoss 8.8382 (8.2868)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.409)\n",
      "EVALUATING - Epoch: [0][200/782]\tTime 0.983 (0.980)\tData 0.000 (0.019)\tLoss 7.5416 (8.2407)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.389)\n",
      "EVALUATING - Epoch: [0][210/782]\tTime 0.960 (0.978)\tData 0.000 (0.018)\tLoss 7.8985 (8.1980)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.370)\n",
      "EVALUATING - Epoch: [0][220/782]\tTime 0.956 (0.978)\tData 0.000 (0.017)\tLoss 6.3647 (8.2327)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.354)\n",
      "EVALUATING - Epoch: [0][230/782]\tTime 0.963 (0.977)\tData 0.000 (0.016)\tLoss 8.4014 (8.2697)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.338)\n",
      "EVALUATING - Epoch: [0][240/782]\tTime 0.978 (0.976)\tData 0.000 (0.016)\tLoss 7.7805 (8.2895)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.324)\n",
      "EVALUATING - Epoch: [0][250/782]\tTime 0.979 (0.975)\tData 0.000 (0.015)\tLoss 11.5204 (8.2900)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.311)\n",
      "EVALUATING - Epoch: [0][260/782]\tTime 0.983 (0.975)\tData 0.001 (0.014)\tLoss 8.0245 (8.3403)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.299)\n",
      "EVALUATING - Epoch: [0][270/782]\tTime 0.973 (0.975)\tData 0.000 (0.014)\tLoss 7.3110 (8.3539)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.288)\n",
      "EVALUATING - Epoch: [0][280/782]\tTime 0.897 (0.974)\tData 0.000 (0.013)\tLoss 8.5295 (8.3346)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.278)\n",
      "EVALUATING - Epoch: [0][290/782]\tTime 0.982 (0.974)\tData 0.000 (0.013)\tLoss 8.6974 (8.3433)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.268)\n",
      "EVALUATING - Epoch: [0][300/782]\tTime 0.980 (0.973)\tData 0.000 (0.013)\tLoss 8.3680 (8.3466)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.260)\n",
      "EVALUATING - Epoch: [0][310/782]\tTime 0.974 (0.973)\tData 0.000 (0.012)\tLoss 7.5611 (8.3284)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.251)\n",
      "EVALUATING - Epoch: [0][320/782]\tTime 0.965 (0.972)\tData 0.000 (0.012)\tLoss 7.0433 (8.3075)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.243)\n",
      "EVALUATING - Epoch: [0][330/782]\tTime 0.937 (0.972)\tData 0.000 (0.012)\tLoss 7.8173 (8.2711)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.236)\n",
      "EVALUATING - Epoch: [0][340/782]\tTime 0.968 (0.972)\tData 0.000 (0.011)\tLoss 6.1415 (8.2422)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.229)\n",
      "EVALUATING - Epoch: [0][350/782]\tTime 0.969 (0.971)\tData 0.000 (0.011)\tLoss 6.1353 (8.2056)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.223)\n",
      "EVALUATING - Epoch: [0][360/782]\tTime 0.971 (0.971)\tData 0.000 (0.011)\tLoss 6.6858 (8.1787)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.216)\n",
      "EVALUATING - Epoch: [0][370/782]\tTime 0.968 (0.970)\tData 0.000 (0.010)\tLoss 4.7814 (8.1504)\tPrec@1 31.250 (0.211)\tPrec@5 31.250 (0.421)\n",
      "EVALUATING - Epoch: [0][380/782]\tTime 0.967 (0.970)\tData 0.000 (0.010)\tLoss 7.5536 (8.1284)\tPrec@1 0.000 (0.205)\tPrec@5 0.000 (0.410)\n",
      "EVALUATING - Epoch: [0][390/782]\tTime 0.948 (0.970)\tData 0.000 (0.010)\tLoss 7.9802 (8.1136)\tPrec@1 0.000 (0.200)\tPrec@5 0.000 (0.400)\n",
      "EVALUATING - Epoch: [0][400/782]\tTime 0.955 (0.969)\tData 0.000 (0.010)\tLoss 5.6896 (8.0823)\tPrec@1 0.000 (0.195)\tPrec@5 0.000 (0.390)\n",
      "EVALUATING - Epoch: [0][410/782]\tTime 0.960 (0.969)\tData 0.000 (0.009)\tLoss 9.1765 (8.0617)\tPrec@1 0.000 (0.190)\tPrec@5 15.625 (0.570)\n",
      "EVALUATING - Epoch: [0][420/782]\tTime 0.981 (0.969)\tData 0.000 (0.009)\tLoss 7.5616 (8.0485)\tPrec@1 0.000 (0.186)\tPrec@5 0.000 (0.557)\n",
      "EVALUATING - Epoch: [0][430/782]\tTime 0.976 (0.969)\tData 0.000 (0.009)\tLoss 6.6741 (8.0393)\tPrec@1 0.000 (0.181)\tPrec@5 0.000 (0.544)\n",
      "EVALUATING - Epoch: [0][440/782]\tTime 0.970 (0.969)\tData 0.000 (0.009)\tLoss 6.6410 (8.0271)\tPrec@1 0.000 (0.177)\tPrec@5 0.000 (0.531)\n",
      "EVALUATING - Epoch: [0][450/782]\tTime 0.998 (0.968)\tData 0.000 (0.009)\tLoss 8.5591 (8.0068)\tPrec@1 0.000 (0.173)\tPrec@5 0.000 (0.662)\n",
      "EVALUATING - Epoch: [0][460/782]\tTime 0.994 (0.968)\tData 0.000 (0.008)\tLoss 7.0639 (7.9915)\tPrec@1 0.000 (0.169)\tPrec@5 0.000 (0.647)\n",
      "EVALUATING - Epoch: [0][470/782]\tTime 0.963 (0.968)\tData 0.000 (0.008)\tLoss 6.4562 (7.9615)\tPrec@1 0.000 (0.166)\tPrec@5 0.000 (0.634)\n",
      "EVALUATING - Epoch: [0][480/782]\tTime 0.973 (0.968)\tData 0.000 (0.008)\tLoss 6.0982 (7.9456)\tPrec@1 0.000 (0.162)\tPrec@5 0.000 (0.620)\n",
      "EVALUATING - Epoch: [0][490/782]\tTime 0.936 (0.968)\tData 0.000 (0.008)\tLoss 8.1502 (7.9357)\tPrec@1 0.000 (0.159)\tPrec@5 0.000 (0.608)\n",
      "EVALUATING - Epoch: [0][500/782]\tTime 0.956 (0.968)\tData 0.000 (0.008)\tLoss 11.9681 (7.9225)\tPrec@1 0.000 (0.156)\tPrec@5 0.000 (0.596)\n",
      "EVALUATING - Epoch: [0][510/782]\tTime 0.950 (0.968)\tData 0.000 (0.008)\tLoss 8.0935 (7.9170)\tPrec@1 0.000 (0.153)\tPrec@5 0.000 (0.584)\n",
      "EVALUATING - Epoch: [0][520/782]\tTime 0.978 (0.967)\tData 0.000 (0.007)\tLoss 6.5244 (7.9036)\tPrec@1 0.000 (0.150)\tPrec@5 0.000 (0.573)\n",
      "EVALUATING - Epoch: [0][530/782]\tTime 0.974 (0.967)\tData 0.000 (0.007)\tLoss 6.3035 (7.8902)\tPrec@1 0.000 (0.147)\tPrec@5 0.000 (0.562)\n",
      "EVALUATING - Epoch: [0][540/782]\tTime 0.964 (0.967)\tData 0.000 (0.007)\tLoss 7.2725 (7.8823)\tPrec@1 0.000 (0.144)\tPrec@5 0.000 (0.552)\n",
      "EVALUATING - Epoch: [0][550/782]\tTime 0.977 (0.967)\tData 0.000 (0.007)\tLoss 7.7163 (7.8736)\tPrec@1 0.000 (0.142)\tPrec@5 0.000 (0.542)\n",
      "EVALUATING - Epoch: [0][560/782]\tTime 0.993 (0.967)\tData 0.001 (0.007)\tLoss 6.8248 (7.8597)\tPrec@1 0.000 (0.139)\tPrec@5 0.000 (0.663)\n",
      "EVALUATING - Epoch: [0][570/782]\tTime 0.986 (0.967)\tData 0.000 (0.007)\tLoss 9.1024 (7.8551)\tPrec@1 0.000 (0.137)\tPrec@5 0.000 (0.651)\n",
      "EVALUATING - Epoch: [0][580/782]\tTime 0.943 (0.967)\tData 0.000 (0.007)\tLoss 7.7637 (7.8511)\tPrec@1 0.000 (0.134)\tPrec@5 0.000 (0.640)\n",
      "EVALUATING - Epoch: [0][590/782]\tTime 0.941 (0.966)\tData 0.000 (0.007)\tLoss 8.3272 (7.8422)\tPrec@1 0.000 (0.132)\tPrec@5 0.000 (0.629)\n",
      "EVALUATING - Epoch: [0][600/782]\tTime 0.972 (0.966)\tData 0.000 (0.007)\tLoss 7.0134 (7.8276)\tPrec@1 0.000 (0.130)\tPrec@5 0.000 (0.619)\n",
      "EVALUATING - Epoch: [0][610/782]\tTime 0.952 (0.966)\tData 0.000 (0.006)\tLoss 7.9760 (7.8119)\tPrec@1 0.000 (0.128)\tPrec@5 0.000 (0.609)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][620/782]\tTime 0.993 (0.966)\tData 0.000 (0.006)\tLoss 7.8771 (7.7980)\tPrec@1 0.000 (0.126)\tPrec@5 0.000 (0.599)\n",
      "EVALUATING - Epoch: [0][630/782]\tTime 0.971 (0.965)\tData 0.000 (0.006)\tLoss 7.7878 (7.7941)\tPrec@1 0.000 (0.124)\tPrec@5 0.000 (0.589)\n",
      "EVALUATING - Epoch: [0][640/782]\tTime 0.976 (0.965)\tData 0.000 (0.006)\tLoss 6.3292 (7.7890)\tPrec@1 0.000 (0.122)\tPrec@5 0.000 (0.580)\n",
      "EVALUATING - Epoch: [0][650/782]\tTime 0.962 (0.965)\tData 0.000 (0.006)\tLoss 8.6536 (7.7830)\tPrec@1 0.000 (0.120)\tPrec@5 0.000 (0.595)\n",
      "EVALUATING - Epoch: [0][660/782]\tTime 0.961 (0.965)\tData 0.000 (0.006)\tLoss 5.8704 (7.7698)\tPrec@1 0.000 (0.118)\tPrec@5 0.000 (0.586)\n",
      "EVALUATING - Epoch: [0][670/782]\tTime 0.951 (0.965)\tData 0.000 (0.006)\tLoss 7.5885 (7.7697)\tPrec@1 0.000 (0.116)\tPrec@5 0.000 (0.577)\n",
      "EVALUATING - Epoch: [0][680/782]\tTime 0.848 (0.965)\tData 0.000 (0.006)\tLoss 8.5749 (7.7752)\tPrec@1 0.000 (0.115)\tPrec@5 0.000 (0.569)\n",
      "EVALUATING - Epoch: [0][690/782]\tTime 0.965 (0.965)\tData 0.000 (0.006)\tLoss 6.9612 (7.7677)\tPrec@1 0.000 (0.113)\tPrec@5 0.000 (0.561)\n",
      "EVALUATING - Epoch: [0][700/782]\tTime 0.981 (0.965)\tData 0.000 (0.006)\tLoss 7.0836 (7.7589)\tPrec@1 0.000 (0.111)\tPrec@5 0.000 (0.553)\n",
      "EVALUATING - Epoch: [0][710/782]\tTime 0.967 (0.965)\tData 0.000 (0.006)\tLoss 7.8591 (7.7499)\tPrec@1 0.000 (0.110)\tPrec@5 0.000 (0.545)\n",
      "EVALUATING - Epoch: [0][720/782]\tTime 0.956 (0.965)\tData 0.000 (0.006)\tLoss 9.9092 (7.7424)\tPrec@1 0.000 (0.108)\tPrec@5 0.000 (0.537)\n",
      "EVALUATING - Epoch: [0][730/782]\tTime 0.968 (0.965)\tData 0.000 (0.005)\tLoss 7.8352 (7.7622)\tPrec@1 0.000 (0.107)\tPrec@5 0.000 (0.530)\n",
      "EVALUATING - Epoch: [0][740/782]\tTime 0.969 (0.965)\tData 0.000 (0.005)\tLoss 9.1154 (7.7767)\tPrec@1 0.000 (0.105)\tPrec@5 0.000 (0.523)\n",
      "EVALUATING - Epoch: [0][750/782]\tTime 0.959 (0.965)\tData 0.000 (0.005)\tLoss 9.8188 (7.7907)\tPrec@1 0.000 (0.104)\tPrec@5 0.000 (0.516)\n",
      "EVALUATING - Epoch: [0][760/782]\tTime 0.984 (0.965)\tData 0.000 (0.005)\tLoss 8.8584 (7.8005)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.509)\n",
      "EVALUATING - Epoch: [0][770/782]\tTime 0.958 (0.965)\tData 0.000 (0.005)\tLoss 7.8593 (7.8218)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.503)\n",
      "EVALUATING - Epoch: [0][780/782]\tTime 0.960 (0.964)\tData 0.000 (0.005)\tLoss 8.7775 (7.8398)\tPrec@1 0.000 (0.100)\tPrec@5 0.000 (0.496)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.839918360290527 0.1 0.496\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, quantizer.deployment_model, criterion, 0, quantizer, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.0\n",
      "model.1.0\n",
      "model.1.3\n",
      "model.2.0\n",
      "model.2.3\n",
      "model.3.0\n",
      "model.3.3\n",
      "model.4.0\n",
      "model.4.3\n",
      "model.5.0\n",
      "model.5.3\n",
      "model.6.0\n",
      "model.6.3\n",
      "model.7.0\n",
      "model.7.3\n",
      "model.8.0\n",
      "model.8.3\n",
      "model.9.0\n",
      "model.9.3\n",
      "model.10.0\n",
      "model.10.3\n",
      "model.11.0\n",
      "model.11.3\n",
      "model.12.0\n",
      "model.12.3\n",
      "model.13.0\n",
      "model.13.3\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "for module_full_name, module in model.named_modules():\n",
    "    if has_children(module) is False:\n",
    "        if type(module) in [nn.Conv2d, nn.Linear]:\n",
    "            print(module_full_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other module type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mobilenet_real(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Conv2d (64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d (64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d (128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): ReLU(inplace)\n",
       "    (16): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): ReLU(inplace)\n",
       "    (20): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): ReLU(inplace)\n",
       "    (22): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): ReLU(inplace)\n",
       "    (24): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): ReLU(inplace)\n",
       "    (32): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): ReLU(inplace)\n",
       "    (34): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): ReLU(inplace)\n",
       "    (36): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): ReLU(inplace)\n",
       "    (38): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): ReLU(inplace)\n",
       "    (40): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): ReLU(inplace)\n",
       "    (42): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): ReLU(inplace)\n",
       "    (44): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): ReLU(inplace)\n",
       "    (46): Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): ReLU(inplace)\n",
       "    (48): Conv2d (512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): ReLU(inplace)\n",
       "    (50): Conv2d (1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): ReLU(inplace)\n",
       "    (52): Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): ReLU(inplace)\n",
       "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment_model = None\n",
    "deployment_modules = []\n",
    "conv_module = []\n",
    "batch_module = []\n",
    "act_module = []\n",
    "\n",
    "\n",
    "def batch_fold(conv_layer, batch_layer):\n",
    "    weight_tensor = conv_layer.weight.data\n",
    "    weight_tensor_size = weight_tensor.size()\n",
    "\n",
    "    eps = batch_layer.eps\n",
    "    gamma_tensor = batch_layer.weight.data \n",
    "    beta_tensor = batch_layer.bias.data \n",
    "    mu_tensor = batch_layer.running_mean\n",
    "    var_tensor = batch_layer.running_var\n",
    "    \n",
    "    if conv_layer.bias is None:\n",
    "        bias_tensor = - mu_tensor \n",
    "    else:\n",
    "        bias_tensor = conv_layer.bias.data - mu_tensor \n",
    "\n",
    "    #folded weight\n",
    "    for n_out_channel in range(weight_tensor_size[0]):\n",
    "        gamma = gamma_tensor[n_out_channel]\n",
    "        sigma = math.sqrt(var_tensor[n_out_channel] + eps)\n",
    "        beta = beta_tensor[n_out_channel]\n",
    "        \n",
    "        weight_per_channel = weight_tensor[n_out_channel]\n",
    "        weight_per_channel =(weight_per_channel*gamma)/sigma\n",
    "        weight_tensor[n_out_channel] =  weight_per_channel\n",
    "        \n",
    "        bias = bias_tensor[n_out_channel]\n",
    "        bias = ((bias*gamma)/sigma)+beta\n",
    "        bias_tensor[n_out_channel] = bias\n",
    "        \n",
    "    conv_layer.bias = nn.Parameter(bias_tensor)\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "def find_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            find_layers(child) \n",
    "        elif isinstance(child, nn.Conv2d):\n",
    "            conv_module.append(child)\n",
    "        elif isinstance(child, nn.BatchNorm2d):\n",
    "            batch_module.append(child)\n",
    "        elif isinstance(child, nn.ReLU):\n",
    "            act_module.append(child)\n",
    "        else:\n",
    "            print('Other module type')\n",
    "            deployment_modules.append(child)\n",
    "\n",
    "            \n",
    "        #print(child.__class__.__name__)\n",
    "        if len(conv_module) > 0 and len(batch_module) > 0 and len(act_module) > 0 :\n",
    "            conv_layer = copy.deepcopy(conv_module.pop())\n",
    "            batch_layer = copy.deepcopy(batch_module.pop() )\n",
    "            act_layer = copy.deepcopy(act_module.pop() )\n",
    "            conv_layer = batch_fold(conv_layer, batch_layer)\n",
    "            deployment_modules.append(conv_layer )\n",
    "            #deployment_modules.append(batch_layer )\n",
    "            deployment_modules.append( act_layer )\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "find_layers(model.model)\n",
    "deployment_model = copy.deepcopy(model)\n",
    "deployment_model.model = nn.Sequential(*deployment_modules)    \n",
    "deployment_model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/391]\tTime 9.409 (9.409)\tData 2.844 (2.844)\tLoss 0.6119 (0.6119)\tPrec@1 82.812 (82.812)\tPrec@5 96.094 (96.094)\n",
      "EVALUATING - Epoch: [0][10/391]\tTime 0.047 (0.900)\tData 0.000 (0.259)\tLoss 1.1026 (0.6558)\tPrec@1 72.656 (82.812)\tPrec@5 91.406 (95.455)\n",
      "EVALUATING - Epoch: [0][20/391]\tTime 0.050 (0.564)\tData 0.000 (0.206)\tLoss 0.9791 (0.9172)\tPrec@1 78.906 (76.228)\tPrec@5 92.188 (93.192)\n",
      "EVALUATING - Epoch: [0][30/391]\tTime 0.046 (0.455)\tData 0.000 (0.197)\tLoss 1.5945 (1.0715)\tPrec@1 73.438 (72.681)\tPrec@5 86.719 (92.011)\n",
      "EVALUATING - Epoch: [0][40/391]\tTime 0.049 (0.395)\tData 0.000 (0.188)\tLoss 0.9665 (0.9725)\tPrec@1 76.562 (75.248)\tPrec@5 92.969 (92.683)\n",
      "EVALUATING - Epoch: [0][50/391]\tTime 0.300 (0.395)\tData 0.243 (0.219)\tLoss 0.3795 (0.9922)\tPrec@1 85.938 (75.107)\tPrec@5 99.219 (92.341)\n",
      "EVALUATING - Epoch: [0][60/391]\tTime 0.180 (0.366)\tData 0.131 (0.211)\tLoss 1.0166 (0.9407)\tPrec@1 75.000 (76.422)\tPrec@5 91.406 (92.649)\n",
      "EVALUATING - Epoch: [0][70/391]\tTime 0.047 (0.346)\tData 0.000 (0.206)\tLoss 1.1106 (0.9732)\tPrec@1 64.844 (75.209)\tPrec@5 92.969 (92.628)\n",
      "EVALUATING - Epoch: [0][80/391]\tTime 0.060 (0.331)\tData 0.010 (0.198)\tLoss 1.0620 (0.9957)\tPrec@1 70.312 (74.277)\tPrec@5 92.188 (92.554)\n",
      "EVALUATING - Epoch: [0][90/391]\tTime 0.144 (0.330)\tData 0.093 (0.206)\tLoss 1.0373 (1.0032)\tPrec@1 65.625 (74.021)\tPrec@5 93.750 (92.591)\n",
      "EVALUATING - Epoch: [0][100/391]\tTime 0.048 (0.319)\tData 0.000 (0.203)\tLoss 0.8347 (0.9944)\tPrec@1 80.469 (73.948)\tPrec@5 93.750 (92.837)\n",
      "EVALUATING - Epoch: [0][110/391]\tTime 0.344 (0.312)\tData 0.297 (0.202)\tLoss 1.2768 (0.9970)\tPrec@1 58.594 (73.705)\tPrec@5 93.750 (92.934)\n",
      "EVALUATING - Epoch: [0][120/391]\tTime 0.048 (0.303)\tData 0.000 (0.199)\tLoss 0.8282 (0.9894)\tPrec@1 78.125 (74.115)\tPrec@5 94.531 (93.046)\n",
      "EVALUATING - Epoch: [0][130/391]\tTime 0.445 (0.306)\tData 0.395 (0.205)\tLoss 0.4969 (0.9820)\tPrec@1 88.281 (74.416)\tPrec@5 96.875 (93.112)\n",
      "EVALUATING - Epoch: [0][140/391]\tTime 0.261 (0.300)\tData 0.210 (0.203)\tLoss 1.1970 (0.9811)\tPrec@1 60.938 (74.330)\tPrec@5 94.531 (93.146)\n",
      "EVALUATING - Epoch: [0][150/391]\tTime 0.261 (0.298)\tData 0.209 (0.205)\tLoss 0.9869 (0.9895)\tPrec@1 61.719 (74.064)\tPrec@5 94.531 (93.031)\n",
      "EVALUATING - Epoch: [0][160/391]\tTime 0.808 (0.296)\tData 0.756 (0.205)\tLoss 1.1045 (1.0027)\tPrec@1 69.531 (73.942)\tPrec@5 89.062 (92.833)\n",
      "EVALUATING - Epoch: [0][170/391]\tTime 0.047 (0.292)\tData 0.000 (0.204)\tLoss 1.8531 (1.0406)\tPrec@1 58.594 (73.195)\tPrec@5 85.156 (92.329)\n",
      "EVALUATING - Epoch: [0][180/391]\tTime 0.152 (0.288)\tData 0.103 (0.202)\tLoss 2.4478 (1.0762)\tPrec@1 47.656 (72.514)\tPrec@5 75.781 (91.790)\n",
      "EVALUATING - Epoch: [0][190/391]\tTime 0.086 (0.286)\tData 0.033 (0.202)\tLoss 2.2388 (1.1103)\tPrec@1 49.219 (71.850)\tPrec@5 80.469 (91.320)\n",
      "EVALUATING - Epoch: [0][200/391]\tTime 0.050 (0.286)\tData 0.001 (0.203)\tLoss 1.3808 (1.1454)\tPrec@1 57.812 (71.140)\tPrec@5 88.281 (90.827)\n",
      "EVALUATING - Epoch: [0][210/391]\tTime 0.050 (0.284)\tData 0.000 (0.203)\tLoss 1.4181 (1.1672)\tPrec@1 67.188 (70.812)\tPrec@5 87.500 (90.510)\n",
      "EVALUATING - Epoch: [0][220/391]\tTime 0.052 (0.286)\tData 0.000 (0.206)\tLoss 0.9056 (1.1795)\tPrec@1 80.469 (70.687)\tPrec@5 89.062 (90.229)\n",
      "EVALUATING - Epoch: [0][230/391]\tTime 0.047 (0.283)\tData 0.000 (0.204)\tLoss 2.1866 (1.1949)\tPrec@1 50.000 (70.417)\tPrec@5 75.000 (90.006)\n",
      "EVALUATING - Epoch: [0][240/391]\tTime 0.197 (0.281)\tData 0.151 (0.204)\tLoss 1.4121 (1.2047)\tPrec@1 67.188 (70.348)\tPrec@5 87.500 (89.818)\n",
      "EVALUATING - Epoch: [0][250/391]\tTime 0.469 (0.280)\tData 0.418 (0.204)\tLoss 1.1659 (1.2288)\tPrec@1 72.656 (69.814)\tPrec@5 86.719 (89.486)\n",
      "EVALUATING - Epoch: [0][260/391]\tTime 0.594 (0.279)\tData 0.544 (0.204)\tLoss 1.6096 (1.2474)\tPrec@1 60.938 (69.453)\tPrec@5 87.500 (89.278)\n",
      "EVALUATING - Epoch: [0][270/391]\tTime 0.166 (0.278)\tData 0.115 (0.204)\tLoss 2.1620 (1.2598)\tPrec@1 50.000 (69.194)\tPrec@5 76.562 (89.097)\n",
      "EVALUATING - Epoch: [0][280/391]\tTime 0.048 (0.276)\tData 0.000 (0.203)\tLoss 1.4343 (1.2680)\tPrec@1 61.719 (69.073)\tPrec@5 89.062 (89.012)\n",
      "EVALUATING - Epoch: [0][290/391]\tTime 0.049 (0.273)\tData 0.000 (0.201)\tLoss 2.0138 (1.2839)\tPrec@1 43.750 (68.798)\tPrec@5 83.594 (88.754)\n",
      "EVALUATING - Epoch: [0][300/391]\tTime 0.605 (0.273)\tData 0.553 (0.202)\tLoss 1.3191 (1.2958)\tPrec@1 71.875 (68.620)\tPrec@5 85.938 (88.543)\n",
      "EVALUATING - Epoch: [0][310/391]\tTime 0.048 (0.272)\tData 0.000 (0.202)\tLoss 1.6978 (1.3065)\tPrec@1 64.844 (68.474)\tPrec@5 79.688 (88.374)\n",
      "EVALUATING - Epoch: [0][320/391]\tTime 0.194 (0.272)\tData 0.139 (0.202)\tLoss 0.8506 (1.3148)\tPrec@1 81.250 (68.322)\tPrec@5 92.188 (88.264)\n",
      "EVALUATING - Epoch: [0][330/391]\tTime 0.050 (0.270)\tData 0.000 (0.201)\tLoss 2.0751 (1.3347)\tPrec@1 49.219 (67.917)\tPrec@5 77.344 (87.981)\n",
      "EVALUATING - Epoch: [0][340/391]\tTime 0.050 (0.270)\tData 0.000 (0.201)\tLoss 1.2242 (1.3409)\tPrec@1 63.281 (67.753)\tPrec@5 90.625 (87.901)\n",
      "EVALUATING - Epoch: [0][350/391]\tTime 0.050 (0.269)\tData 0.000 (0.200)\tLoss 1.2724 (1.3474)\tPrec@1 72.656 (67.642)\tPrec@5 88.281 (87.812)\n",
      "EVALUATING - Epoch: [0][360/391]\tTime 0.048 (0.268)\tData 0.000 (0.200)\tLoss 1.6824 (1.3569)\tPrec@1 57.031 (67.458)\tPrec@5 86.719 (87.719)\n",
      "EVALUATING - Epoch: [0][370/391]\tTime 0.260 (0.268)\tData 0.212 (0.200)\tLoss 1.1868 (1.3508)\tPrec@1 63.281 (67.571)\tPrec@5 92.188 (87.807)\n",
      "EVALUATING - Epoch: [0][380/391]\tTime 0.372 (0.269)\tData 0.319 (0.202)\tLoss 1.0566 (1.3527)\tPrec@1 71.875 (67.538)\tPrec@5 93.750 (87.787)\n",
      "EVALUATING - Epoch: [0][390/391]\tTime 0.390 (0.268)\tData 0.000 (0.201)\tLoss 2.6144 (1.3458)\tPrec@1 37.500 (67.652)\tPrec@5 72.500 (87.878)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3458160171508788 67.652 87.878\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, deployment_model.cuda(), criterion, 0, None, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Quantization of weights as\n",
    "https://github.com/ARM-software/ML-KWS-for-MCU/blob/master/quant_test.py\n",
    "\n",
    "https://github.com/ARM-software/ML-KWS-for-MCU/blob/master/quant_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_point_linear_quant(min_value, max_value, n_bits, signed=True):\n",
    "    range_values = max_value- min_value\n",
    "\n",
    "    max_range = max(abs(min_value),abs(max_value))\n",
    "    int_bits = int(np.ceil(np.log2(max_range)))\n",
    "    \n",
    "    if int_bits < 0:\n",
    "        int_bits = 0\n",
    "    elif int_bits>(n_bits-1):\n",
    "        int_bits = (n_bits-1)\n",
    "        \n",
    "    if signed:\n",
    "        frac_bits = n_bits-1-int_bits #remaining bits are fractional bits (1-bit for sign)\n",
    "    else:\n",
    "        frac_bits = n_bits-int_bits\n",
    "        \n",
    "    return int_bits, frac_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558258593082428 -0.8026384711265564 1.3608970642089844 0 7\n",
      "4.027159690856934 -4.618956565856934 8.646116256713867 3 4\n",
      "1.2754887342453003 -1.4489389657974243 2.7244277000427246 1 6\n",
      "5.171591758728027 -3.461690664291382 8.63328242301941 3 4\n",
      "0.8815251588821411 -0.831538736820221 1.713063895702362 0 7\n",
      "4.645492076873779 -4.746695041656494 9.392187118530273 3 4\n",
      "0.9018105864524841 -0.8606885075569153 1.7624990940093994 0 7\n",
      "2.1124305725097656 -2.2831485271453857 4.395579099655151 2 5\n",
      "0.571179986000061 -0.4470636546611786 1.0182436406612396 0 7\n",
      "6.458991527557373 -5.2962141036987305 11.755205631256104 3 4\n",
      "0.5835903286933899 -0.5659104585647583 1.1495007872581482 0 7\n",
      "22.159364700317383 -2.932857036590576 25.09222173690796 5 2\n",
      "0.3646238148212433 -0.40404146909713745 0.7686652839183807 0 7\n",
      "19.831918716430664 -6.3695831298828125 26.201501846313477 5 2\n",
      "0.45581403374671936 -0.3711283206939697 0.8269423544406891 0 7\n",
      "9.509058952331543 -5.71013879776001 15.219197750091553 4 3\n",
      "0.401159405708313 -0.3822166621685028 0.7833760678768158 0 7\n",
      "4.265459060668945 -5.392614364624023 9.658073425292969 3 4\n",
      "0.4583653509616852 -0.3846471905708313 0.8430125415325165 0 7\n",
      "5.717052936553955 -4.4894795417785645 10.20653247833252 3 4\n",
      "0.4761676490306854 -0.4442507326602936 0.920418381690979 0 7\n",
      "2.4391751289367676 -3.9615747928619385 6.400749921798706 2 5\n",
      "0.37422895431518555 -0.4431194067001343 0.8173483610153198 0 7\n",
      "2.249077081680298 -2.4493234157562256 4.698400497436523 2 5\n",
      "0.500937819480896 -0.4916362166404724 0.9925740361213684 0 7\n",
      "12.539684295654297 -14.799212455749512 27.33889675140381 4 3\n",
      "8.922979354858398 -7.055124282836914 15.978103637695312 4 3\n",
      "0.9389712810516357 -0.3607997000217438 1.2997709810733795 0 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.558258593082428, -0.8026384711265564, 1.3608970642089844, 0, 7],\n",
       " [4.027159690856934, -4.618956565856934, 8.646116256713867, 3, 4],\n",
       " [1.2754887342453003, -1.4489389657974243, 2.7244277000427246, 1, 6],\n",
       " [5.171591758728027, -3.461690664291382, 8.63328242301941, 3, 4],\n",
       " [0.8815251588821411, -0.831538736820221, 1.713063895702362, 0, 7],\n",
       " [4.645492076873779, -4.746695041656494, 9.392187118530273, 3, 4],\n",
       " [0.9018105864524841, -0.8606885075569153, 1.7624990940093994, 0, 7],\n",
       " [2.1124305725097656, -2.2831485271453857, 4.395579099655151, 2, 5],\n",
       " [0.571179986000061, -0.4470636546611786, 1.0182436406612396, 0, 7],\n",
       " [6.458991527557373, -5.2962141036987305, 11.755205631256104, 3, 4],\n",
       " [0.5835903286933899, -0.5659104585647583, 1.1495007872581482, 0, 7],\n",
       " [22.159364700317383, -2.932857036590576, 25.09222173690796, 5, 2],\n",
       " [0.3646238148212433, -0.40404146909713745, 0.7686652839183807, 0, 7],\n",
       " [19.831918716430664, -6.3695831298828125, 26.201501846313477, 5, 2],\n",
       " [0.45581403374671936, -0.3711283206939697, 0.8269423544406891, 0, 7],\n",
       " [9.509058952331543, -5.71013879776001, 15.219197750091553, 4, 3],\n",
       " [0.401159405708313, -0.3822166621685028, 0.7833760678768158, 0, 7],\n",
       " [4.265459060668945, -5.392614364624023, 9.658073425292969, 3, 4],\n",
       " [0.4583653509616852, -0.3846471905708313, 0.8430125415325165, 0, 7],\n",
       " [5.717052936553955, -4.4894795417785645, 10.20653247833252, 3, 4],\n",
       " [0.4761676490306854, -0.4442507326602936, 0.920418381690979, 0, 7],\n",
       " [2.4391751289367676, -3.9615747928619385, 6.400749921798706, 2, 5],\n",
       " [0.37422895431518555, -0.4431194067001343, 0.8173483610153198, 0, 7],\n",
       " [2.249077081680298, -2.4493234157562256, 4.698400497436523, 2, 5],\n",
       " [0.500937819480896, -0.4916362166404724, 0.9925740361213684, 0, 7],\n",
       " [12.539684295654297, -14.799212455749512, 27.33889675140381, 4, 3],\n",
       " [8.922979354858398, -7.055124282836914, 15.978103637695312, 4, 3],\n",
       " [0.9389712810516357, -0.3607997000217438, 1.2997709810733795, 0, 7]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_conv2d = []\n",
    "N_BITS = 8\n",
    "def f_quant_weight(model, symmetric_range=True):\n",
    "    if symmetric_range is not True: # asymmetric to be implemented\n",
    "        print('Error')\n",
    "        return -1\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            f_quant_weight(child, symmetric_range) \n",
    "        elif isinstance(child, nn.Conv2d) or isinstance(child, nn.Linear):\n",
    "            max_weights = child.weight.data.max()\n",
    "            min_weights = child.weight.data.min()\n",
    "            range_weights = max_weights- min_weights\n",
    "            int_bits, frac_bits = fixed_point_linear_quant(min_weights, max_weights, N_BITS, signed=True)\n",
    "            \n",
    "#            max_range = max(abs(min_weights),abs(max_weights))\n",
    "#            int_bits = int(np.ceil(np.log2(max_range)))\n",
    "#            if int_bits<0:\n",
    "#                int_bits = 0\n",
    "#            elif int_bits>(N_BITS-1):\n",
    "#                int_bits = (N_BITS-1)\n",
    "#            frac_bits = N_BITS-1-int_bits #remaining bits are fractional bits (1-bit for sign)           \n",
    "            \n",
    "            stats_conv2d.append([max_weights, min_weights, range_weights, int_bits, frac_bits])\n",
    "            print(max_weights, min_weights, range_weights, int_bits, frac_bits)\n",
    "            if int_bits > 0:\n",
    "                max_value = (2**(int_bits-1))+1-2**(-frac_bits)\n",
    "            else:\n",
    "                max_value = 1-2**(-frac_bits)\n",
    "            #print(int_bits, -2**int_bits, (2**int_bits)-1, max_value, 2**(-frac_bits))\n",
    "            clipped_weight = child.weight.data.clamp( -max_value, max_value)\n",
    "            quant_weight = clipped_weight.mul(2**frac_bits).round().div(2**frac_bits)\n",
    "            #print(quant_weight)\n",
    "            child.weight.data = quant_weight\n",
    "            \n",
    "            \n",
    "f_quant_weight(deployment_model, symmetric_range=True)\n",
    "stats_conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Quantization of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.1 ReLU(inplace)\n",
      "model.3 ReLU(inplace)\n",
      "model.5 ReLU(inplace)\n",
      "model.7 ReLU(inplace)\n",
      "model.9 ReLU(inplace)\n",
      "model.11 ReLU(inplace)\n",
      "model.13 ReLU(inplace)\n",
      "model.15 ReLU(inplace)\n",
      "model.17 ReLU(inplace)\n",
      "model.19 ReLU(inplace)\n",
      "model.21 ReLU(inplace)\n",
      "model.23 ReLU(inplace)\n",
      "model.25 ReLU(inplace)\n",
      "model.27 ReLU(inplace)\n",
      "model.29 ReLU(inplace)\n",
      "model.31 ReLU(inplace)\n",
      "model.33 ReLU(inplace)\n",
      "model.35 ReLU(inplace)\n",
      "model.37 ReLU(inplace)\n",
      "model.39 ReLU(inplace)\n",
      "model.41 ReLU(inplace)\n",
      "model.43 ReLU(inplace)\n",
      "model.45 ReLU(inplace)\n",
      "model.47 ReLU(inplace)\n",
      "model.49 ReLU(inplace)\n",
      "model.51 ReLU(inplace)\n",
      "model.53 ReLU(inplace)\n",
      "5.781166076660156 0.0 0.3834649622440338 0.4645973742008209\n",
      "11.030117988586426 0.0 0.4068128168582916 0.5873093605041504\n",
      "15.173482894897461 0.0 0.29163435101509094 0.34446993470191956\n",
      "9.82027530670166 0.0 0.38282033801078796 0.4403855800628662\n",
      "7.053549289703369 0.0 0.24418221414089203 0.2565935552120209\n",
      "8.497363090515137 0.0 0.31330856680870056 0.41559818387031555\n",
      "6.900217056274414 0.0 0.16909408569335938 0.25600773096084595\n",
      "5.829596996307373 0.0 0.36365532875061035 0.40665629506111145\n",
      "4.300732612609863 0.0 0.23592238128185272 0.22172459959983826\n",
      "6.044707775115967 0.0 0.2059078961610794 0.3030555248260498\n",
      "5.0792388916015625 0.0 0.12782810628414154 0.1951996237039566\n",
      "5.930695533752441 0.0 0.25347980856895447 0.33783596754074097\n",
      "2.746802568435669 0.0 0.17686402797698975 0.19882994890213013\n",
      "5.156436920166016 0.0 0.16627545654773712 0.24499720335006714\n",
      "2.577873468399048 0.0 0.11860686540603638 0.16618749499320984\n",
      "5.190664768218994 0.0 0.1889183670282364 0.2715107798576355\n",
      "2.6644299030303955 0.0 0.09938447177410126 0.1590992659330368\n",
      "6.229292869567871 0.0 0.1911730319261551 0.2737148404121399\n",
      "3.028366804122925 0.0 0.09505829215049744 0.16001670062541962\n",
      "9.23070240020752 0.0 0.21005034446716309 0.285238116979599\n",
      "3.0311944484710693 0.0 0.0819159746170044 0.15400920808315277\n",
      "8.21544361114502 0.0 0.21989963948726654 0.2998955249786377\n",
      "3.0023880004882812 0.0 0.08230133354663849 0.14688849449157715\n",
      "5.690117359161377 0.0 0.24297726154327393 0.317352831363678\n",
      "2.495088577270508 0.0 0.03746156021952629 0.10500804334878922\n",
      "5.377908229827881 0.0 0.19883793592453003 0.26117825508117676\n",
      "24.16253662109375 0.0 0.5525522828102112 1.0916551351547241\n",
      "6.664950370788574 0.0 0.3872512876987457 0.4662327170372009\n",
      "10.988555908203125 0.0 0.4079631567001343 0.5896613597869873\n",
      "16.35980987548828 0.0 0.29030683636665344 0.3429514169692993\n",
      "11.518256187438965 0.0 0.3817547857761383 0.4368402361869812\n",
      "7.814406871795654 0.0 0.24423319101333618 0.25419312715530396\n",
      "10.22216796875 0.0 0.31167978048324585 0.4136788845062256\n",
      "9.11392879486084 0.0 0.16812920570373535 0.2537979483604431\n",
      "6.719604015350342 0.0 0.3632097542285919 0.4052547514438629\n",
      "4.476060390472412 0.0 0.23494665324687958 0.22002099454402924\n",
      "6.0306315422058105 0.0 0.20608185231685638 0.3025068938732147\n",
      "4.231258392333984 0.0 0.1275200992822647 0.19453935325145721\n",
      "6.0801310539245605 0.0 0.2530222535133362 0.33593520522117615\n",
      "2.443739891052246 0.0 0.17674794793128967 0.1979517638683319\n",
      "4.307379245758057 0.0 0.1660032868385315 0.24437858164310455\n",
      "3.3671813011169434 0.0 0.11883580684661865 0.16617974638938904\n",
      "6.2998504638671875 0.0 0.18906453251838684 0.27161580324172974\n",
      "3.031012535095215 0.0 0.09959936141967773 0.15934967994689941\n",
      "8.671957969665527 0.0 0.1915571093559265 0.274165540933609\n",
      "3.3287761211395264 0.0 0.09549186378717422 0.16049756109714508\n",
      "8.809027671813965 0.0 0.2100968211889267 0.28544867038726807\n",
      "3.3182079792022705 0.0 0.08226322382688522 0.15440939366817474\n",
      "6.460849285125732 0.0 0.22035157680511475 0.30053311586380005\n",
      "2.7975456714630127 0.0 0.08244195580482483 0.14737065136432648\n",
      "6.1071696281433105 0.0 0.24390345811843872 0.3191078007221222\n",
      "2.2566850185394287 0.0 0.037500228732824326 0.10552773624658585\n",
      "4.138452529907227 0.0 0.19875772297382355 0.26104578375816345\n",
      "31.04730224609375 0.0 0.5449451208114624 1.077134370803833\n",
      "5.767683506011963 0.0 0.38724377751350403 0.46766579151153564\n",
      "10.204055786132812 0.0 0.4079686403274536 0.5865800976753235\n",
      "15.62868881225586 0.0 0.2880960702896118 0.3386620879173279\n",
      "10.739806175231934 0.0 0.38048693537712097 0.4351388216018677\n",
      "6.5554046630859375 0.0 0.24347269535064697 0.25357621908187866\n",
      "8.813451766967773 0.0 0.3105151653289795 0.41203972697257996\n",
      "7.263717174530029 0.0 0.16673660278320312 0.25254738330841064\n",
      "6.283617973327637 0.0 0.36254173517227173 0.4054209291934967\n",
      "4.251223564147949 0.0 0.23401430249214172 0.2201586365699768\n",
      "5.846908092498779 0.0 0.20581437647342682 0.303046315908432\n",
      "4.30446195602417 0.0 0.12711821496486664 0.19467449188232422\n",
      "6.6130828857421875 0.0 0.2527199685573578 0.3365621864795685\n",
      "3.2688236236572266 0.0 0.17593637108802795 0.19782163202762604\n",
      "4.782138347625732 0.0 0.16580620408058167 0.24443469941616058\n",
      "2.7686214447021484 0.0 0.11852842569351196 0.1658501774072647\n",
      "6.857495307922363 0.0 0.18915903568267822 0.2719874978065491\n",
      "2.988759994506836 0.0 0.09957278519868851 0.1596129685640335\n",
      "9.359879493713379 0.0 0.19146034121513367 0.2744625210762024\n",
      "3.228980302810669 0.0 0.0953095331788063 0.1607336401939392\n",
      "10.603116035461426 0.0 0.21037551760673523 0.28620612621307373\n",
      "3.723951578140259 0.0 0.08191580325365067 0.1547054797410965\n",
      "7.182102203369141 0.0 0.22057069838047028 0.30119332671165466\n",
      "3.1340975761413574 0.0 0.08221153169870377 0.14758746325969696\n",
      "7.306548118591309 0.0 0.24456804990768433 0.31971198320388794\n",
      "2.1038150787353516 0.0 0.037643976509571075 0.1064058244228363\n",
      "7.007523536682129 0.0 0.19876748323440552 0.2614665627479553\n",
      "29.268627166748047 0.0 0.5484708547592163 1.0891704559326172\n",
      "5.969430446624756 0.0 0.3853878378868103 0.46774816513061523\n",
      "10.100621223449707 0.0 0.40873637795448303 0.5886038541793823\n",
      "15.736034393310547 0.0 0.28963354229927063 0.3423806130886078\n",
      "10.070354461669922 0.0 0.38071736693382263 0.4358086585998535\n",
      "6.405888080596924 0.0 0.24355089664459229 0.2547966241836548\n",
      "9.56824779510498 0.0 0.31269964575767517 0.4142134189605713\n",
      "5.946002960205078 0.0 0.16803981363773346 0.2537473738193512\n",
      "5.691372871398926 0.0 0.3611658811569214 0.402597576379776\n",
      "4.861944198608398 0.0 0.23492030799388885 0.2192559391260147\n",
      "5.838006973266602 0.0 0.204440638422966 0.30015093088150024\n",
      "4.067049026489258 0.0 0.12686938047409058 0.1933504194021225\n",
      "5.871992588043213 0.0 0.25232771039009094 0.3348012864589691\n",
      "2.654642105102539 0.0 0.1762882024049759 0.19789248704910278\n",
      "4.488038539886475 0.0 0.1650201976299286 0.24246379733085632\n",
      "3.945125102996826 0.0 0.11806107312440872 0.16488268971443176\n",
      "6.540256977081299 0.0 0.18830972909927368 0.26961803436279297\n",
      "2.728891611099243 0.0 0.09893817454576492 0.15833094716072083\n",
      "8.544553756713867 0.0 0.19086843729019165 0.2726295292377472\n",
      "3.9182915687561035 0.0 0.09499049931764603 0.15995067358016968\n",
      "9.709424018859863 0.0 0.21043376624584198 0.2853914201259613\n",
      "3.2372515201568604 0.0 0.08146795630455017 0.15375357866287231\n",
      "7.010584831237793 0.0 0.22057704627513885 0.3008340895175934\n",
      "2.7842295169830322 0.0 0.081698477268219 0.1469714343547821\n",
      "6.114715099334717 0.0 0.24381721019744873 0.318882018327713\n",
      "2.728534460067749 0.0 0.037461057305336 0.10566697269678116\n",
      "4.378849983215332 0.0 0.19856387376785278 0.26135730743408203\n",
      "25.820993423461914 0.0 0.5418849587440491 1.076187014579773\n",
      "5.7389349937438965 0.0 0.38931119441986084 0.4707522988319397\n",
      "11.5618314743042 0.0 0.4057389199733734 0.5872696042060852\n",
      "16.483959197998047 0.0 0.2896914482116699 0.3430219292640686\n",
      "9.122922897338867 0.0 0.38277867436408997 0.43825194239616394\n",
      "6.949329853057861 0.0 0.24493840336799622 0.25608962774276733\n",
      "8.746262550354004 0.0 0.31168118119239807 0.41390132904052734\n",
      "6.758269786834717 0.0 0.16735686361789703 0.25363337993621826\n",
      "5.6861138343811035 0.0 0.36337682604789734 0.40586450695991516\n",
      "3.9971959590911865 0.0 0.23508593440055847 0.2206311672925949\n",
      "5.638060092926025 0.0 0.20561550557613373 0.3022677004337311\n",
      "4.120280742645264 0.0 0.12708967924118042 0.19449345767498016\n",
      "5.966485977172852 0.0 0.2541050314903259 0.3378106355667114\n",
      "2.514887809753418 0.0 0.17635010182857513 0.1987503618001938\n",
      "4.84092903137207 0.0 0.1660451740026474 0.24406737089157104\n",
      "2.702420949935913 0.0 0.11838028579950333 0.1656891405582428\n",
      "5.249781608581543 0.0 0.1886080652475357 0.2705487012863159\n",
      "2.4960405826568604 0.0 0.09880774468183517 0.1583433896303177\n",
      "7.493415355682373 0.0 0.1908179670572281 0.27272945642471313\n",
      "3.2901570796966553 0.0 0.09478703141212463 0.15926629304885864\n",
      "10.087891578674316 0.0 0.21032480895519257 0.28544890880584717\n",
      "3.4092931747436523 0.0 0.08149620145559311 0.15359991788864136\n",
      "7.027744770050049 0.0 0.22086268663406372 0.30092427134513855\n",
      "2.683464288711548 0.0 0.0816037729382515 0.14693336188793182\n",
      "6.2671613693237305 0.0 0.24425023794174194 0.31950780749320984\n",
      "2.4426631927490234 0.0 0.037558719515800476 0.10625260323286057\n",
      "4.369529724121094 0.0 0.19925929605960846 0.2618064880371094\n",
      "33.942787170410156 0.0 0.5526525378227234 1.097456932067871\n",
      "6.265329360961914 0.0 0.38849449157714844 0.47099149227142334\n",
      "13.286332130432129 0.0 0.4057871699333191 0.5854570865631104\n",
      "15.5068359375 0.0 0.28933316469192505 0.33952683210372925\n",
      "10.981025695800781 0.0 0.3819543421268463 0.4368765652179718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.011906147003174 0.0 0.24467183649539948 0.25533583760261536\n",
      "9.05483627319336 0.0 0.31126174330711365 0.41254761815071106\n",
      "7.086294651031494 0.0 0.16814205050468445 0.2536892890930176\n",
      "7.003238201141357 0.0 0.36242273449897766 0.40484511852264404\n",
      "4.015329837799072 0.0 0.2347562611103058 0.22057360410690308\n",
      "5.846207618713379 0.0 0.2059193253517151 0.3024962544441223\n",
      "3.634432792663574 0.0 0.1273060292005539 0.194541797041893\n",
      "7.402890205383301 0.0 0.25346988439559937 0.3365992605686188\n",
      "2.5735719203948975 0.0 0.17581391334533691 0.19792959094047546\n",
      "5.480377674102783 0.0 0.16607365012168884 0.24425289034843445\n",
      "4.021682262420654 0.0 0.11848624795675278 0.16565629839897156\n",
      "6.608579158782959 0.0 0.18902117013931274 0.27147623896598816\n",
      "2.855058431625366 0.0 0.09935259073972702 0.15912334620952606\n",
      "7.009454250335693 0.0 0.1912030577659607 0.2737687826156616\n",
      "2.9869165420532227 0.0 0.0953017920255661 0.16059397161006927\n",
      "11.047274589538574 0.0 0.21059156954288483 0.2868184745311737\n",
      "3.5444271564483643 0.0 0.08188551664352417 0.15471000969409943\n",
      "7.714627742767334 0.0 0.22077952325344086 0.3013904094696045\n",
      "5.036674499511719 0.0 0.08229091018438339 0.14796766638755798\n",
      "6.056582450866699 0.0 0.2442735731601715 0.3198987543582916\n",
      "2.178928852081299 0.0 0.03823716193437576 0.10761307924985886\n",
      "3.861360549926758 0.0 0.19887512922286987 0.2615945041179657\n",
      "25.35779571533203 0.0 0.5559198260307312 1.10194993019104\n",
      "5.909363746643066 0.0 0.38511136174201965 0.46720510721206665\n",
      "9.986863136291504 0.0 0.41103655099868774 0.5956870317459106\n",
      "14.219680786132812 0.0 0.2910098135471344 0.3457166254520416\n",
      "9.084871292114258 0.0 0.38353943824768066 0.44226545095443726\n",
      "6.149734973907471 0.0 0.24452978372573853 0.25634828209877014\n",
      "8.0183744430542 0.0 0.31228107213974 0.4153136610984802\n",
      "6.236149787902832 0.0 0.16891181468963623 0.2558910846710205\n",
      "5.157678604125977 0.0 0.36269229650497437 0.4061799645423889\n",
      "3.896655321121216 0.0 0.2347375899553299 0.2209833562374115\n",
      "5.807025909423828 0.0 0.20568059384822845 0.30356329679489136\n",
      "4.648247241973877 0.0 0.1277662068605423 0.1951795071363449\n",
      "6.688112735748291 0.0 0.2529548704624176 0.3365786075592041\n",
      "3.044114112854004 0.0 0.17582735419273376 0.19758908450603485\n",
      "5.706474304199219 0.0 0.1662878841161728 0.2451874315738678\n",
      "2.9484164714813232 0.0 0.11877602338790894 0.16623805463314056\n",
      "5.21544075012207 0.0 0.18956446647644043 0.2723267376422882\n",
      "3.2019569873809814 0.0 0.09948976337909698 0.15952633321285248\n",
      "5.80686616897583 0.0 0.1913485825061798 0.27431702613830566\n",
      "4.049004077911377 0.0 0.09562313556671143 0.16108274459838867\n",
      "7.922708034515381 0.0 0.21048852801322937 0.2866356670856476\n",
      "3.8861351013183594 0.0 0.08186722546815872 0.15460219979286194\n",
      "8.371297836303711 0.0 0.2213996797800064 0.301884263753891\n",
      "4.070579528808594 0.0 0.08236993849277496 0.14801351726055145\n",
      "6.0577497482299805 0.0 0.24404741823673248 0.31924161314964294\n",
      "2.967348098754883 0.0 0.03833664208650589 0.1072695404291153\n",
      "6.595295429229736 0.0 0.19844798743724823 0.26100262999534607\n",
      "25.294723510742188 0.0 0.5586786270141602 1.1038320064544678\n",
      "5.844448566436768 0.0 0.38782280683517456 0.4676472842693329\n",
      "10.842126846313477 0.0 0.4053933322429657 0.5890145897865295\n",
      "15.13398551940918 0.0 0.2910056412220001 0.3447411358356476\n",
      "9.586301803588867 0.0 0.3840760290622711 0.4424612820148468\n",
      "6.090952396392822 0.0 0.24587245285511017 0.2579963803291321\n",
      "7.612849235534668 0.0 0.31376948952674866 0.4178102910518646\n",
      "6.108648300170898 0.0 0.1708398163318634 0.25804853439331055\n",
      "5.3065338134765625 0.0 0.36518990993499756 0.40755191445350647\n",
      "3.794229030609131 0.0 0.23693619668483734 0.22251306474208832\n",
      "5.042410850524902 0.0 0.20725716650485992 0.3051253855228424\n",
      "4.707599639892578 0.0 0.1294940710067749 0.19677385687828064\n",
      "6.290925979614258 0.0 0.2539892792701721 0.3371850848197937\n",
      "2.5567727088928223 0.0 0.1778959035873413 0.19902528822422028\n",
      "4.233060359954834 0.0 0.16710779070854187 0.24634882807731628\n",
      "3.4775238037109375 0.0 0.1196117103099823 0.16729220747947693\n",
      "6.907040119171143 0.0 0.18967729806900024 0.2728407382965088\n",
      "2.871995210647583 0.0 0.10007896274328232 0.160182923078537\n",
      "7.614438056945801 0.0 0.1913563311100006 0.2749495208263397\n",
      "3.5761263370513916 0.0 0.09597920626401901 0.16122552752494812\n",
      "8.064541816711426 0.0 0.2103610783815384 0.2868732213973999\n",
      "3.0820233821868896 0.0 0.0824316143989563 0.1551540046930313\n",
      "6.225557327270508 0.0 0.22038784623146057 0.30094924569129944\n",
      "2.542156934738159 0.0 0.08218783140182495 0.14731299877166748\n",
      "5.841694355010986 0.0 0.24311351776123047 0.3176997900009155\n",
      "2.1115713119506836 0.0 0.037893228232860565 0.1061650738120079\n",
      "3.9283928871154785 0.0 0.19840826094150543 0.2609136700630188\n",
      "33.53582000732422 0.0 0.5502418279647827 1.0851693153381348\n",
      "5.661237716674805 0.0 0.38766470551490784 0.46748095750808716\n",
      "11.83582878112793 0.0 0.40471288561820984 0.5828987956047058\n",
      "18.945629119873047 0.0 0.2876698076725006 0.33761027455329895\n",
      "11.47564697265625 0.0 0.3793642818927765 0.43342089653015137\n",
      "7.71937370300293 0.0 0.2436412274837494 0.2539370059967041\n",
      "9.750901222229004 0.0 0.31142452359199524 0.41369080543518066\n",
      "6.5113067626953125 0.0 0.16786344349384308 0.25310781598091125\n",
      "5.4035964012146 0.0 0.3627488613128662 0.403978168964386\n",
      "4.314685344696045 0.0 0.2343718260526657 0.22016407549381256\n",
      "6.167637348175049 0.0 0.20551256835460663 0.3012770116329193\n",
      "4.46516752243042 0.0 0.1277286261320114 0.19425225257873535\n",
      "6.783105850219727 0.0 0.2520386576652527 0.33454737067222595\n",
      "2.6339664459228516 0.0 0.17695751786231995 0.19807764887809753\n",
      "6.2744317054748535 0.0 0.16583004593849182 0.24447733163833618\n",
      "3.18927264213562 0.0 0.1190718337893486 0.1661561131477356\n",
      "4.920074939727783 0.0 0.1890660524368286 0.2712991237640381\n",
      "3.1828625202178955 0.0 0.09982894361019135 0.15934446454048157\n",
      "6.603978633880615 0.0 0.190804123878479 0.2733577489852905\n",
      "3.2678122520446777 0.0 0.0954243540763855 0.16013915836811066\n",
      "7.181917190551758 0.0 0.21004055440425873 0.2856161892414093\n",
      "3.390014410018921 0.0 0.08220672607421875 0.15428784489631653\n",
      "7.203535556793213 0.0 0.21989592909812927 0.2999536693096161\n",
      "2.8746843338012695 0.0 0.08216226100921631 0.1467757374048233\n",
      "6.375483989715576 0.0 0.2433827519416809 0.3176104426383972\n",
      "2.3035173416137695 0.0 0.03762286901473999 0.10550283640623093\n",
      "5.104689121246338 0.0 0.19909590482711792 0.26091015338897705\n",
      "38.55585861206055 0.0 0.548650324344635 1.0798920392990112\n",
      "5.623178958892822 0.0 0.3876851499080658 0.4708155691623688\n",
      "11.642446517944336 0.0 0.40537595748901367 0.5885428190231323\n",
      "16.603351593017578 0.0 0.2918592691421509 0.34699547290802\n",
      "10.775076866149902 0.0 0.38571760058403015 0.4464004337787628\n",
      "7.286225318908691 0.0 0.24639467895030975 0.2599391043186188\n",
      "8.626455307006836 0.0 0.3143174648284912 0.41913512349128723\n",
      "7.169100284576416 0.0 0.16993610560894012 0.2582634687423706\n",
      "7.056463241577148 0.0 0.36465081572532654 0.40853261947631836\n",
      "4.7752275466918945 0.0 0.23735776543617249 0.2227226048707962\n",
      "6.119517803192139 0.0 0.20717722177505493 0.30522724986076355\n",
      "4.928838729858398 0.0 0.12840601801872253 0.19644229114055634\n",
      "6.761790752410889 0.0 0.25502175092697144 0.34057721495628357\n",
      "2.741333484649658 0.0 0.17732666432857513 0.20103879272937775\n",
      "5.1860222816467285 0.0 0.16713808476924896 0.24702005088329315\n",
      "3.5990800857543945 0.0 0.11914175748825073 0.1670798510313034\n",
      "5.807942867279053 0.0 0.18985402584075928 0.27293410897254944\n",
      "2.7619049549102783 0.0 0.09972768276929855 0.15977682173252106\n",
      "8.922192573547363 0.0 0.19165560603141785 0.2745321989059448\n",
      "3.461829423904419 0.0 0.09555225074291229 0.1607576310634613\n",
      "8.8489351272583 0.0 0.21130219101905823 0.28714466094970703\n",
      "3.866086006164551 0.0 0.08193142712116241 0.154922753572464\n",
      "7.371682643890381 0.0 0.22095640003681183 0.3017602562904358\n",
      "3.02559494972229 0.0 0.08188841491937637 0.14749696850776672\n",
      "5.711442470550537 0.0 0.24450066685676575 0.31926727294921875\n",
      "2.248440980911255 0.0 0.03789600729942322 0.10675227642059326\n",
      "5.4514312744140625 0.0 0.19899679720401764 0.2618192434310913\n",
      "42.723777770996094 0.0 0.5535233020782471 1.1020148992538452\n",
      "Variable containing:\n",
      " 3.3336e+00 -3.3298e-02 -2.7162e+00  ...   3.2468e+00  4.5482e+00 -9.0792e-02\n",
      "-5.6986e+00 -3.3322e+00 -5.6047e-01  ...  -5.8076e+00  2.2899e-01  5.8702e+00\n",
      " 4.7371e+00  1.0636e+00  6.9245e-02  ...   7.0995e+00  3.4443e+00  3.0970e+00\n",
      "                ...                                      ...                \n",
      " 1.4816e+00 -2.5806e+00 -4.4353e+00  ...   4.9790e+00  4.5386e+00 -9.4509e-02\n",
      "-4.2827e-01 -1.0509e+00 -1.0286e-01  ...  -5.0183e+00  1.6008e+00  7.5282e+00\n",
      " 5.8482e-01  1.2717e+00 -2.5426e+00  ...  -2.1912e+00  3.4187e+00  6.0799e+00\n",
      "[torch.cuda.FloatTensor of size 128x1000 (GPU 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_range_min =  []\n",
    "output_range_max =  []\n",
    "output_range_mean =  []\n",
    "output_range_std =  []\n",
    "output_range_values = []\n",
    "def get_io_shape_3(self, input, output):\n",
    "    output_range_min[self.mark].append(output.data.min())\n",
    "    output_range_max[self.mark].append(output.data.max())\n",
    "    output_range_mean[self.mark].append(output.data.mean())\n",
    "    output_range_std[self.mark].append(output.data.std())\n",
    "    print(output.data.max(),output.data.min(),output.data.mean(),output.data.std())\n",
    "\n",
    "    \n",
    "    \n",
    "def calibrate(data_loader, model, n_batch, gpus):\n",
    "    training = False\n",
    "    \n",
    "    counter = 0\n",
    "    for n,l in model.named_modules():\n",
    "        if isinstance(l,nn.ReLU):\n",
    "            print(n,l)\n",
    "            l.mark = counter\n",
    "            output_range_min.append([])\n",
    "            output_range_max.append([])\n",
    "            output_range_std.append([])\n",
    "            output_range_mean.append([])\n",
    "            counter += 1 \n",
    "            l.register_forward_hook(get_io_shape_3)\n",
    "        \n",
    "    #v = model.model[1].register_forward_hook(get_layer8)\n",
    "\n",
    "    #if gpus and len(gpus) > 1:\n",
    "    #    model = torch.nn.DataParallel(model, gpus)\n",
    "    model.eval()\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "            \n",
    "        input_var = Variable(inputs.type(ttype), volatile=not training)\n",
    "        target_var = Variable(target)\n",
    "        \n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        \n",
    "        #print(v) \n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Run validation\n",
    "output = calibrate(train_loader, deployment_model.cuda(), 10, gpus)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1d861d683a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_range_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "mean(output_range_min[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine precision per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_act_layer = len(output_range_min)\n",
    "precision_layer = [[] for x in range(n_act_layer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7\n",
      "1 7\n",
      "1 7\n",
      "1 7\n",
      "0 8\n",
      "1 7\n",
      "0 8\n",
      "1 7\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "1 7\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "0 8\n",
      "2 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_act_layer):\n",
    "    min_value = min(output_range_min[i])\n",
    "    max_value = max(output_range_max[i])\n",
    "    std_value = max(output_range_std[i])\n",
    "    int_bits, frac_bits = fixed_point_linear_quant(min_value, 3*std_value, N_BITS, signed=False)\n",
    "    print(int_bits, frac_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantizeFixed(real_activation, numBits=8, min_value=0, max_value=6, int_bits=1):\n",
    "    tensor = real_activation.clamp( 0, max_value )\n",
    "    eps = (max_value)/((2**numBits)-1)\n",
    "    shift_int = 2**int_bits\n",
    "    tensor=tensor.div(eps).floor().mul(eps)\n",
    "    #tensor = tensor.div(shift_int)\n",
    "    #print(eps,shift_int )\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class QuantizationReLU(torch.autograd.Function):\n",
    "    '''\n",
    "    Quantize the input activations and calculate the mean across channel dimension.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bits, min_value, max_value, int_bits):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = QuantizeFixed(input, numBits=bits, min_value=min_value, max_value=max_value, int_bits=int_bits)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        grad_input[input.le(0)] = 0\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "class QuantReLU(nn.Module):\n",
    "    def __init__(self, min_value, max_value, num_of_bits, int_bits):\n",
    "        super(QuantReLU,self).__init__()\n",
    "        self.bits = num_of_bits\n",
    "        self.max_value = max_value\n",
    "        self.min_value = min_value\n",
    "        self.int_bits = int_bits\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return QuantizationReLU.apply(input, self.bits, self.min_value,self.max_value, self.int_bits )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2969404458999634\n",
      "4.1698092222213745\n",
      "2.42896831035614\n",
      "3.1248030364513397\n",
      "1.8195737302303314\n",
      "2.9339458644390106\n",
      "1.8078442811965942\n",
      "2.8597283363342285\n",
      "1.5590582340955734\n",
      "2.136590749025345\n",
      "1.3774169981479645\n",
      "2.384040504693985\n",
      "1.4072715491056442\n",
      "1.729140356183052\n",
      "1.1710454523563385\n",
      "1.910538762807846\n",
      "1.121280461549759\n",
      "1.924646645784378\n",
      "1.1285786926746368\n",
      "2.010012626647949\n",
      "1.0860780328512192\n",
      "2.113189846277237\n",
      "1.0360946208238602\n",
      "2.2392912805080414\n",
      "0.753291554749012\n",
      "1.8327347040176392\n",
      "7.726824045181274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mobilenet_real(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): QuantReLU(\n",
       "    )\n",
       "    (2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "    (3): QuantReLU(\n",
       "    )\n",
       "    (4): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (5): QuantReLU(\n",
       "    )\n",
       "    (6): Conv2d (64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "    (7): QuantReLU(\n",
       "    )\n",
       "    (8): Conv2d (64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (9): QuantReLU(\n",
       "    )\n",
       "    (10): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "    (11): QuantReLU(\n",
       "    )\n",
       "    (12): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (13): QuantReLU(\n",
       "    )\n",
       "    (14): Conv2d (128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "    (15): QuantReLU(\n",
       "    )\n",
       "    (16): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (17): QuantReLU(\n",
       "    )\n",
       "    (18): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (19): QuantReLU(\n",
       "    )\n",
       "    (20): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): QuantReLU(\n",
       "    )\n",
       "    (22): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "    (23): QuantReLU(\n",
       "    )\n",
       "    (24): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (25): QuantReLU(\n",
       "    )\n",
       "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (27): QuantReLU(\n",
       "    )\n",
       "    (28): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): QuantReLU(\n",
       "    )\n",
       "    (30): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (31): QuantReLU(\n",
       "    )\n",
       "    (32): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (33): QuantReLU(\n",
       "    )\n",
       "    (34): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (35): QuantReLU(\n",
       "    )\n",
       "    (36): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (37): QuantReLU(\n",
       "    )\n",
       "    (38): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (39): QuantReLU(\n",
       "    )\n",
       "    (40): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (41): QuantReLU(\n",
       "    )\n",
       "    (42): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "    (43): QuantReLU(\n",
       "    )\n",
       "    (44): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (45): QuantReLU(\n",
       "    )\n",
       "    (46): Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "    (47): QuantReLU(\n",
       "    )\n",
       "    (48): Conv2d (512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): QuantReLU(\n",
       "    )\n",
       "    (50): Conv2d (1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "    (51): QuantReLU(\n",
       "    )\n",
       "    (52): Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (53): QuantReLU(\n",
       "    )\n",
       "    (54): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model = None\n",
    "quantized_modules = []\n",
    "counter = 0\n",
    "\n",
    "def find_activations_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            find_activations_layers(child) \n",
    "        elif isinstance(child, nn.ReLU):\n",
    "            global counter\n",
    "            min_value = min(output_range_min[counter])\n",
    "            max_value = max(output_range_max[counter])\n",
    "            std_value = max(output_range_std[counter])\n",
    "            counter += 1\n",
    "            max_value = 7*std_value\n",
    "            print(max_value)\n",
    "            int_bits, frac_bits = fixed_point_linear_quant(min_value, max_value, N_BITS, signed=False)\n",
    "            quantized_modules.append(QuantReLU(min_value, max_value, N_BITS, int_bits ) )\n",
    "        else:\n",
    "            quantized_modules.append(child)\n",
    "            \n",
    "            \n",
    "find_activations_layers(deployment_model.model)\n",
    "quantized_model = copy.deepcopy(deployment_model)\n",
    "quantized_model.model = nn.Sequential(*quantized_modules)    \n",
    "quantized_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING - Epoch: [0][0/391]\tTime 3.287 (3.287)\tData 3.211 (3.211)\tLoss 0.6898 (0.6898)\tPrec@1 83.594 (83.594)\tPrec@5 92.188 (92.188)\n",
      "EVALUATING - Epoch: [0][10/391]\tTime 0.542 (0.509)\tData 0.473 (0.440)\tLoss 1.3616 (0.9342)\tPrec@1 64.062 (76.989)\tPrec@5 85.938 (91.051)\n",
      "EVALUATING - Epoch: [0][20/391]\tTime 0.213 (0.357)\tData 0.143 (0.288)\tLoss 1.5703 (1.1213)\tPrec@1 64.844 (71.429)\tPrec@5 82.812 (90.141)\n",
      "EVALUATING - Epoch: [0][30/391]\tTime 0.067 (0.327)\tData 0.000 (0.257)\tLoss 1.8310 (1.2872)\tPrec@1 66.406 (67.288)\tPrec@5 82.031 (88.659)\n",
      "EVALUATING - Epoch: [0][40/391]\tTime 0.232 (0.303)\tData 0.160 (0.234)\tLoss 1.0482 (1.1945)\tPrec@1 74.219 (69.798)\tPrec@5 89.844 (89.348)\n",
      "EVALUATING - Epoch: [0][50/391]\tTime 0.070 (0.300)\tData 0.000 (0.230)\tLoss 0.6157 (1.2641)\tPrec@1 82.812 (68.903)\tPrec@5 94.531 (88.251)\n",
      "EVALUATING - Epoch: [0][60/391]\tTime 0.310 (0.288)\tData 0.240 (0.219)\tLoss 1.2655 (1.2117)\tPrec@1 69.531 (70.338)\tPrec@5 87.500 (88.781)\n",
      "EVALUATING - Epoch: [0][70/391]\tTime 0.144 (0.283)\tData 0.073 (0.213)\tLoss 1.5444 (1.2282)\tPrec@1 53.906 (69.443)\tPrec@5 90.625 (88.974)\n",
      "EVALUATING - Epoch: [0][80/391]\tTime 0.068 (0.280)\tData 0.001 (0.210)\tLoss 1.3246 (1.2529)\tPrec@1 61.719 (68.470)\tPrec@5 89.844 (88.860)\n",
      "EVALUATING - Epoch: [0][90/391]\tTime 0.349 (0.277)\tData 0.280 (0.208)\tLoss 1.2517 (1.2611)\tPrec@1 59.375 (68.218)\tPrec@5 91.406 (88.856)\n",
      "EVALUATING - Epoch: [0][100/391]\tTime 0.068 (0.270)\tData 0.000 (0.200)\tLoss 1.1961 (1.2477)\tPrec@1 71.875 (68.201)\tPrec@5 88.281 (89.233)\n",
      "EVALUATING - Epoch: [0][110/391]\tTime 0.264 (0.275)\tData 0.194 (0.206)\tLoss 1.4441 (1.2443)\tPrec@1 54.688 (68.039)\tPrec@5 89.062 (89.372)\n",
      "EVALUATING - Epoch: [0][120/391]\tTime 0.069 (0.270)\tData 0.001 (0.201)\tLoss 1.1183 (1.2308)\tPrec@1 71.875 (68.472)\tPrec@5 90.625 (89.573)\n",
      "EVALUATING - Epoch: [0][130/391]\tTime 0.293 (0.273)\tData 0.224 (0.203)\tLoss 0.5791 (1.2285)\tPrec@1 85.156 (68.726)\tPrec@5 95.312 (89.522)\n",
      "EVALUATING - Epoch: [0][140/391]\tTime 0.411 (0.271)\tData 0.344 (0.201)\tLoss 1.5002 (1.2259)\tPrec@1 51.562 (68.761)\tPrec@5 89.062 (89.633)\n",
      "EVALUATING - Epoch: [0][150/391]\tTime 0.067 (0.271)\tData 0.001 (0.201)\tLoss 1.1172 (1.2278)\tPrec@1 64.062 (68.703)\tPrec@5 91.406 (89.637)\n",
      "EVALUATING - Epoch: [0][160/391]\tTime 0.168 (0.267)\tData 0.095 (0.197)\tLoss 1.4304 (1.2376)\tPrec@1 60.938 (68.668)\tPrec@5 85.156 (89.431)\n",
      "EVALUATING - Epoch: [0][170/391]\tTime 0.068 (0.267)\tData 0.001 (0.197)\tLoss 1.9976 (1.2756)\tPrec@1 55.469 (67.978)\tPrec@5 81.250 (88.916)\n",
      "EVALUATING - Epoch: [0][180/391]\tTime 0.069 (0.265)\tData 0.000 (0.195)\tLoss 2.8174 (1.3121)\tPrec@1 37.500 (67.218)\tPrec@5 71.094 (88.445)\n",
      "EVALUATING - Epoch: [0][190/391]\tTime 0.067 (0.263)\tData 0.000 (0.193)\tLoss 2.4280 (1.3375)\tPrec@1 47.656 (66.754)\tPrec@5 72.656 (88.040)\n",
      "EVALUATING - Epoch: [0][200/391]\tTime 0.069 (0.262)\tData 0.000 (0.192)\tLoss 2.2488 (1.3761)\tPrec@1 44.531 (65.990)\tPrec@5 71.875 (87.496)\n",
      "EVALUATING - Epoch: [0][210/391]\tTime 0.151 (0.262)\tData 0.080 (0.192)\tLoss 1.9601 (1.4034)\tPrec@1 59.375 (65.558)\tPrec@5 76.562 (87.089)\n",
      "EVALUATING - Epoch: [0][220/391]\tTime 0.069 (0.262)\tData 0.000 (0.192)\tLoss 1.2940 (1.4229)\tPrec@1 68.750 (65.222)\tPrec@5 84.375 (86.740)\n",
      "EVALUATING - Epoch: [0][230/391]\tTime 0.068 (0.261)\tData 0.000 (0.191)\tLoss 2.3712 (1.4393)\tPrec@1 47.656 (65.013)\tPrec@5 72.656 (86.462)\n",
      "EVALUATING - Epoch: [0][240/391]\tTime 0.333 (0.259)\tData 0.261 (0.189)\tLoss 1.6326 (1.4488)\tPrec@1 67.969 (64.931)\tPrec@5 84.375 (86.268)\n",
      "EVALUATING - Epoch: [0][250/391]\tTime 0.377 (0.257)\tData 0.301 (0.187)\tLoss 1.5558 (1.4702)\tPrec@1 66.406 (64.436)\tPrec@5 80.469 (86.000)\n",
      "EVALUATING - Epoch: [0][260/391]\tTime 0.074 (0.259)\tData 0.000 (0.189)\tLoss 2.0896 (1.4882)\tPrec@1 51.562 (64.071)\tPrec@5 78.906 (85.794)\n",
      "EVALUATING - Epoch: [0][270/391]\tTime 0.067 (0.257)\tData 0.000 (0.187)\tLoss 2.3109 (1.5007)\tPrec@1 46.875 (63.855)\tPrec@5 72.656 (85.606)\n",
      "EVALUATING - Epoch: [0][280/391]\tTime 0.067 (0.256)\tData 0.000 (0.186)\tLoss 1.5986 (1.5099)\tPrec@1 57.812 (63.648)\tPrec@5 86.719 (85.504)\n",
      "EVALUATING - Epoch: [0][290/391]\tTime 0.067 (0.254)\tData 0.000 (0.184)\tLoss 2.0242 (1.5240)\tPrec@1 42.969 (63.418)\tPrec@5 87.500 (85.285)\n",
      "EVALUATING - Epoch: [0][300/391]\tTime 0.434 (0.255)\tData 0.363 (0.185)\tLoss 1.6379 (1.5377)\tPrec@1 68.750 (63.276)\tPrec@5 80.469 (85.068)\n",
      "EVALUATING - Epoch: [0][310/391]\tTime 0.149 (0.254)\tData 0.074 (0.184)\tLoss 1.5625 (1.5535)\tPrec@1 63.281 (63.010)\tPrec@5 88.281 (84.870)\n",
      "EVALUATING - Epoch: [0][320/391]\tTime 0.069 (0.254)\tData 0.000 (0.184)\tLoss 1.3095 (1.5610)\tPrec@1 71.094 (62.902)\tPrec@5 87.500 (84.760)\n",
      "EVALUATING - Epoch: [0][330/391]\tTime 0.069 (0.253)\tData 0.000 (0.183)\tLoss 2.2447 (1.5824)\tPrec@1 49.219 (62.488)\tPrec@5 77.344 (84.467)\n",
      "EVALUATING - Epoch: [0][340/391]\tTime 0.266 (0.254)\tData 0.200 (0.183)\tLoss 1.5371 (1.5893)\tPrec@1 67.969 (62.314)\tPrec@5 85.938 (84.386)\n",
      "EVALUATING - Epoch: [0][350/391]\tTime 0.068 (0.253)\tData 0.000 (0.183)\tLoss 1.4917 (1.5978)\tPrec@1 63.281 (62.148)\tPrec@5 85.938 (84.235)\n",
      "EVALUATING - Epoch: [0][360/391]\tTime 0.355 (0.253)\tData 0.285 (0.182)\tLoss 1.7082 (1.6024)\tPrec@1 57.812 (62.059)\tPrec@5 82.031 (84.191)\n",
      "EVALUATING - Epoch: [0][370/391]\tTime 0.387 (0.253)\tData 0.312 (0.182)\tLoss 1.4691 (1.5977)\tPrec@1 53.906 (62.089)\tPrec@5 87.500 (84.272)\n",
      "EVALUATING - Epoch: [0][380/391]\tTime 0.242 (0.252)\tData 0.168 (0.182)\tLoss 1.3470 (1.6002)\tPrec@1 67.188 (62.071)\tPrec@5 88.281 (84.240)\n",
      "EVALUATING - Epoch: [0][390/391]\tTime 0.044 (0.252)\tData 0.000 (0.181)\tLoss 2.2472 (1.5935)\tPrec@1 45.000 (62.202)\tPrec@5 73.750 (84.306)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5934641359710693 62.202 84.306\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion, 0, None, gpus)\n",
    "print(val_loss, val_prec1, val_prec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- Extra ----------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3250339 ,  0.5       ,  0.21670543,  0.23417938,  0.5       ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(5)\n",
    "np.clip(a,0,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3])\n",
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 1, 3, 3])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "torch.Size([128, 1, 3, 3])\n",
      "torch.Size([128, 128, 1, 1])\n",
      "torch.Size([128, 1, 3, 3])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256, 1, 3, 3])\n",
      "torch.Size([256, 256, 1, 1])\n",
      "torch.Size([256, 1, 3, 3])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 1, 3, 3])\n",
      "torch.Size([1024, 512, 1, 1])\n",
      "torch.Size([1024, 1, 3, 3])\n",
      "torch.Size([1024, 1024, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_fold_vector = [\n",
    "['module.model.0.0','module.model.0.1'],\n",
    "['module.model.1.0','module.model.1.1'],\n",
    "['module.model.1.3','module.model.1.4'],\n",
    "['module.model.2.0','module.model.2.1'],\n",
    "['module.model.2.3','module.model.2.4'],\n",
    "['module.model.3.0','module.model.3.1'],\n",
    "['module.model.3.3','module.model.3.4'],\n",
    "['module.model.4.0','module.model.4.1'],\n",
    "['module.model.4.3','module.model.4.4'],\n",
    "['module.model.5.0','module.model.5.1'],\n",
    "['module.model.5.3','module.model.5.4'],\n",
    "['module.model.6.0','module.model.6.1'],\n",
    "['module.model.6.3','module.model.6.4'],\n",
    "['module.model.7.0','module.model.7.1'],\n",
    "['module.model.7.3','module.model.7.4'],\n",
    "['module.model.8.0','module.model.8.1'],\n",
    "['module.model.8.3','module.model.8.4'],\n",
    "['module.model.9.0','module.model.9.1'],\n",
    "['module.model.9.3','module.model.9.4'],\n",
    "['module.model.10.0','module.model.10.1'],\n",
    "['module.model.10.3','module.model.10.4'],\n",
    "['module.model.11.0','module.model.11.1'],\n",
    "['module.model.11.3','module.model.11.4'],\n",
    "['module.model.12.0','module.model.12.1'],\n",
    "['module.model.12.3','module.model.12.4'],\n",
    "['module.model.13.0','module.model.13.1'],\n",
    "['module.model.13.3','module.model.13.4']\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tensor_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index =  0\n",
      "Index =  1\n",
      "Index =  2\n",
      "Conv module  2 quantized!\n",
      "Index =  3\n",
      "Index =  4\n",
      "Index =  5\n",
      "Index =  6\n",
      "Index =  7\n",
      "Conv module  7 quantized!\n",
      "Index =  8\n",
      "Index =  9\n",
      "Index =  10\n",
      "Index =  11\n",
      "Index =  12\n",
      "Conv module  12 quantized!\n",
      "Index =  13\n",
      "Index =  14\n",
      "Index =  15\n",
      "Index =  16\n",
      "Conv module  16 quantized!\n",
      "Index =  17\n",
      "Index =  18\n",
      "Index =  19\n",
      "Index =  20\n",
      "Conv module  20 quantized!\n",
      "Index =  21\n",
      "Index =  22\n",
      "Index =  23\n",
      "Index =  24\n",
      "Index =  25\n",
      "Linear module:  25  quantized!\n",
      "Index =  26\n",
      "Index =  27\n",
      "Index =  28\n",
      "Linear module:  28  quantized!\n",
      "Index =  29\n",
      "Index =  30\n",
      "Index =  31\n",
      "Linear module:  31  quantized!\n",
      "Index =  32\n",
      "************************\n",
      "Tot to quantized:  8\n"
     ]
    }
   ],
   "source": [
    "index = -1\n",
    "index_conv2d = []\n",
    "index_lin = []\n",
    "\n",
    "for idx,m in enumerate(model.modules()):\n",
    "    print('Index = ', idx)\n",
    "    weight_quant = getattr(m, 'quant_weight', None)\n",
    "    if weight_quant != None:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            index = index + 1\n",
    "            index_conv2d.append(index)\n",
    "            print('Conv module ', idx, 'quantized!')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            index = index + 1\n",
    "            index_lin.append(index)\n",
    "            print('Linear module: ', idx, ' quantized!')\n",
    "        else: \n",
    "            print('Module: ', idx, ' NOT quantized!')\n",
    "        \n",
    "print('************************')\n",
    "print('Tot to quantized: ', len(index_conv2d) + len(index_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaxPool2d' object has no attribute 'quant_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d4f503ecf526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 366\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaxPool2d' object has no attribute 'quant_weight'"
     ]
    }
   ],
   "source": [
    "model.features[1].quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trianing \n",
    "epoch = 0\n",
    "model.train()\n",
    "optimizer = adjust_optimizer(optimizer, epoch, regime)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.adam.Adam at 0x7f13cb465128>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = Variable(inputs, volatile=False)\n",
    "target_var = Variable(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.binarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_var)\n",
    "loss = criterion(output, target_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(quantizer.num_of_params):\n",
    "    quantizer.target_modules[index].data.copy_(quantizer.saved_params[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.updateBinaryGradWeight2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.8127e-02  1.5027e-03  3.2594e-03 -1.2902e-02  1.4510e-02\n",
       "  7.1123e-03  1.9530e-02 -1.0884e-02 -2.8592e-03 -1.3259e-02\n",
       "  2.1212e-02 -2.1252e-02 -8.8512e-03  5.5863e-03  2.5938e-02\n",
       "  2.4559e-02 -1.9748e-02  2.2135e-02  1.5002e-03 -1.0777e-02\n",
       " -1.0942e-02  3.0251e-03 -8.2488e-03 -2.0687e-02 -3.1814e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -2.3760e-03 -2.6540e-03 -1.1723e-02 -1.5072e-02 -1.2241e-03\n",
       " -1.3689e-02 -2.1758e-02  1.1396e-02  2.5437e-02 -8.7456e-04\n",
       "  2.2533e-03 -1.3677e-02 -5.6864e-03  2.1138e-02  4.9958e-03\n",
       "  1.5956e-03 -3.7952e-03 -6.2803e-03  1.3870e-02  1.3949e-02\n",
       "  1.2288e-02  1.0312e-02 -8.1034e-04  1.1017e-02 -1.1564e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -9.1109e-03 -9.9038e-04  6.6380e-03 -2.5215e-02 -1.5876e-03\n",
       " -1.5147e-02 -3.6442e-03  1.6910e-02  4.4561e-04 -1.8616e-02\n",
       " -2.3990e-02  2.0563e-03 -2.4307e-02 -8.7227e-03  1.0326e-02\n",
       "  2.3016e-02 -2.1399e-02  2.1827e-02  1.4622e-02 -1.6246e-02\n",
       " -1.4855e-02 -4.9985e-03 -1.8892e-02 -1.4951e-03  4.6154e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -7.0762e-03  4.7802e-03  7.1773e-03  2.0799e-02 -1.8840e-03\n",
       " -1.0092e-02 -1.4496e-02  3.5899e-03 -2.2817e-02 -1.8218e-02\n",
       "  2.5225e-02 -2.6470e-02 -1.2509e-02  9.9451e-03 -2.1741e-02\n",
       "  9.1894e-03 -4.6840e-03 -1.1146e-02  2.0051e-02  2.2290e-02\n",
       " -2.1867e-02 -1.3247e-02  1.2491e-02 -1.2545e-02 -1.1102e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  3.1433e-03 -2.2180e-02 -2.0657e-02  4.6139e-03 -1.0855e-02\n",
       " -2.1155e-02 -1.9463e-02  1.2641e-02  6.9894e-03  1.3142e-02\n",
       " -1.1386e-03 -1.1071e-02 -2.8952e-03  1.7722e-02  2.3034e-02\n",
       "  4.7652e-03 -1.6204e-02 -1.7498e-02 -9.6184e-03 -1.8215e-02\n",
       " -4.4003e-03 -1.5078e-02  1.8350e-02 -3.9738e-03 -7.5132e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -8.4719e-03 -1.8920e-02  2.5539e-02 -6.3212e-03 -1.1142e-02\n",
       " -1.6250e-02  1.4741e-02 -1.1612e-02  1.6739e-03 -1.6748e-02\n",
       "  1.5673e-03 -2.2873e-02 -6.9813e-03  6.2617e-05  2.1669e-02\n",
       " -7.5196e-03  1.4789e-02 -1.1447e-02  2.1471e-02 -3.5458e-03\n",
       " -2.7225e-03 -1.6787e-02 -2.1236e-02 -2.1042e-02  7.3566e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1.9096e-02 -1.7414e-02  2.2140e-02 -1.5287e-02  1.4455e-02\n",
       "  2.0397e-03 -1.4509e-02 -1.6617e-03  1.5224e-02  1.9241e-02\n",
       "  1.5679e-02  2.1376e-02  5.1866e-03  1.3742e-03  5.6530e-03\n",
       " -1.3192e-02  3.4915e-03  2.0605e-02  1.9053e-02 -5.1003e-03\n",
       " -9.3828e-03 -3.5060e-03  6.4312e-03  2.0911e-02  7.4230e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.4834e-02  1.5733e-02 -1.7126e-02 -2.3076e-02 -8.4893e-03\n",
       "  2.3981e-02 -1.9572e-02  3.4260e-03  2.5698e-03 -1.1628e-02\n",
       "  1.5161e-03 -1.6879e-02  6.2190e-03  4.4115e-03  1.4936e-02\n",
       "  1.1733e-02 -1.6696e-02 -1.1506e-02  1.5677e-02  2.2522e-02\n",
       "  6.7536e-03 -6.0812e-03 -2.2076e-02 -1.1564e-02  1.1740e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.9174e-02  1.9563e-02  6.8029e-04  9.0326e-03  2.4305e-02\n",
       " -2.0872e-02  1.5029e-02  1.8903e-02 -2.7500e-03 -2.4397e-02\n",
       " -3.5883e-03 -2.4260e-03  5.9608e-03 -9.6108e-03 -1.9486e-02\n",
       " -6.4282e-04  1.3236e-02 -6.3805e-03  4.3614e-03 -1.9481e-02\n",
       "  2.5316e-03  6.9512e-03 -5.6784e-03  2.0049e-02 -1.2546e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  2.1178e-02  9.2475e-03 -1.3356e-02  2.3237e-02  1.7591e-02\n",
       " -7.8461e-04  2.3762e-02 -1.1373e-04  2.5877e-02  1.9244e-02\n",
       " -1.7386e-02  8.1954e-03  2.3153e-02 -1.1749e-02  2.2543e-02\n",
       " -5.1184e-03 -1.2765e-02 -2.7481e-03 -1.5597e-02  4.5053e-03\n",
       " -2.3091e-03  3.5104e-03  1.6740e-02  2.0176e-02  4.9362e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.1149e-02  1.2407e-02  1.5939e-02  2.5849e-03  1.1430e-02\n",
       "  9.7003e-03 -1.8499e-02 -1.8851e-02 -1.0847e-02  3.4258e-03\n",
       " -1.7196e-02 -5.4654e-03 -6.4304e-03 -5.8260e-03 -3.4130e-03\n",
       " -1.9647e-02  3.5472e-03 -1.9833e-02 -1.9377e-02  6.4316e-03\n",
       "  1.1275e-02 -1.9409e-02  5.7234e-03  2.1985e-02  9.7657e-03\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1.6058e-03  3.7541e-03 -1.6566e-02  2.3257e-02 -1.4757e-02\n",
       " -9.1012e-03 -1.0380e-02 -1.7606e-02 -1.4127e-02  4.5094e-03\n",
       "  1.0568e-02  1.3536e-02  2.3153e-02 -1.5670e-03  1.9681e-02\n",
       " -2.2910e-02 -1.8209e-02 -2.3108e-02 -1.3555e-02 -7.6443e-03\n",
       " -1.2070e-02 -2.2582e-02  2.2177e-02 -9.1846e-04  1.0176e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1.1585e-02 -7.6666e-03 -9.1613e-03 -1.5271e-02  1.6936e-02\n",
       " -7.5146e-03  1.2522e-02 -1.4715e-02 -2.2066e-02  9.0287e-03\n",
       "  1.7321e-02  2.1467e-02 -8.5637e-03  9.4318e-03 -2.1875e-02\n",
       " -6.8519e-03 -1.8958e-02 -1.4568e-02 -2.4060e-02 -7.4957e-03\n",
       "  2.2308e-02  1.8909e-02  1.5447e-02 -4.4835e-03  5.2158e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  9.7157e-03  3.0865e-04  1.1476e-02  5.8029e-03 -1.4323e-02\n",
       " -1.5687e-02 -2.2458e-02 -1.3379e-02 -1.4305e-02  7.6678e-03\n",
       "  7.1823e-03  1.2023e-02  2.5851e-02  1.8209e-02  8.7983e-03\n",
       "  2.5959e-02  1.5203e-02 -9.4223e-03  1.1792e-02 -1.5215e-02\n",
       "  1.4800e-02 -6.8187e-03  5.4734e-03  7.9059e-03  1.0551e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.6353e-02 -5.7692e-03  4.3811e-03 -8.4465e-03 -1.5122e-02\n",
       " -2.7007e-03 -1.5615e-02 -1.8432e-02 -9.0274e-03 -2.0761e-02\n",
       "  2.3247e-02 -2.4744e-02  1.1459e-02  5.7309e-03 -1.4396e-02\n",
       " -1.8722e-02 -5.6219e-04  7.1841e-03  1.6912e-02  1.7856e-02\n",
       "  1.5941e-02  9.0543e-03  1.3205e-02  1.0449e-02 -2.0351e-03\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.8643e-02 -6.6896e-04 -9.5753e-03  3.6254e-03  2.1763e-02\n",
       "  1.0189e-02 -1.1745e-04  1.2482e-02  1.2609e-02 -1.1771e-02\n",
       " -2.9821e-03 -1.0488e-02 -1.3929e-02  2.3947e-02  1.9473e-02\n",
       " -1.4677e-02 -1.5780e-02 -2.3211e-02 -6.2103e-03  7.4194e-03\n",
       " -9.8016e-03 -1.4882e-02 -2.0435e-02 -1.9943e-02 -7.5823e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.8786e-02  1.0936e-02  2.0851e-02  1.5438e-02 -1.1014e-02\n",
       "  1.3392e-02 -1.8632e-02  2.1880e-02  2.6382e-02 -1.0817e-02\n",
       "  1.9406e-02  5.6681e-03  1.6735e-02 -1.0116e-02  5.9607e-03\n",
       " -3.0752e-03  9.2660e-03  5.0495e-03  1.4882e-02  1.7354e-02\n",
       "  5.3769e-03  2.0343e-02 -1.1393e-04 -1.6952e-02  1.1258e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.4544e-02 -2.3295e-02  6.3029e-03  1.4665e-02 -1.8535e-02\n",
       " -2.2719e-02  1.7705e-02  9.2022e-03  1.7612e-02  4.5667e-04\n",
       " -5.9746e-03  2.1752e-02 -5.1201e-03 -2.1248e-02  1.7188e-02\n",
       "  2.3872e-02 -2.0435e-02 -1.7739e-02  2.2524e-02 -1.0802e-02\n",
       "  2.3962e-02  2.4012e-02 -1.1178e-02 -8.2167e-03 -7.4922e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.7364e-02 -2.3632e-02 -1.6631e-02 -5.5243e-03 -1.5761e-02\n",
       "  1.7652e-02  1.1065e-02 -8.6269e-03 -1.9450e-02  2.0392e-02\n",
       "  2.1468e-02 -8.9295e-03  1.5543e-02  2.0302e-02  8.3006e-03\n",
       " -2.1280e-03 -1.2588e-02  7.7211e-04 -9.8156e-03  2.0207e-02\n",
       "  4.6332e-03  1.4387e-02 -1.1070e-02 -1.6499e-02  1.0041e-02\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.4068e-02  1.2109e-02  7.8176e-03  1.5914e-02  3.0466e-03\n",
       "  1.8128e-02  6.7775e-03 -1.1536e-02 -1.4469e-02  1.4715e-02\n",
       "  1.0304e-02  1.7005e-02 -1.1996e-02 -1.4766e-03  4.8024e-04\n",
       "  8.6332e-03 -8.7581e-03 -1.3530e-02 -1.2634e-02  2.0204e-02\n",
       "  5.3275e-04  1.5249e-02 -1.9053e-02  6.2446e-03  1.6458e-03\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.3839e-02 -6.6901e-03 -1.7259e-02  1.0888e-02 -1.1756e-02\n",
       " -2.0550e-02 -2.0470e-02 -6.7177e-03  2.7262e-03  2.3528e-02\n",
       " -3.3724e-03 -2.0359e-02  8.7641e-03  1.4151e-02  2.4517e-02\n",
       "  7.7960e-03  1.9392e-02  2.2572e-02 -2.0742e-02 -1.0075e-02\n",
       " -1.9978e-03 -1.3593e-02  4.3226e-03  1.9941e-02 -1.2358e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1.0589e-02 -6.6963e-03  2.4210e-02  1.1408e-02 -2.1294e-03\n",
       "  7.7296e-03 -9.2406e-03  8.1714e-03  1.9367e-02 -7.2199e-03\n",
       " -2.1214e-02  1.8880e-02 -7.4127e-03  1.9251e-03 -1.1957e-02\n",
       " -2.2092e-02 -8.0984e-03  1.9220e-02  1.9847e-02 -2.2047e-02\n",
       "  2.0403e-02 -3.5536e-03  2.4226e-02 -2.4024e-02 -2.4730e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -4.2391e-03 -5.3030e-03  1.0658e-02 -1.2108e-02 -5.4902e-03\n",
       "  9.7204e-04  1.3947e-02  2.1941e-02 -5.2715e-03 -7.0353e-03\n",
       "  1.2066e-02 -2.0121e-02 -1.4200e-04 -5.7222e-03  2.5184e-02\n",
       " -7.6840e-03 -8.9304e-03 -8.6936e-03 -1.9946e-02 -2.1174e-02\n",
       "  9.4660e-03  1.3340e-02 -2.2505e-02  2.5334e-02  1.3314e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.2388e-02 -1.2473e-02  1.0232e-02  4.0885e-03  1.9520e-02\n",
       "  2.5037e-02 -1.7132e-02 -2.0942e-02 -3.6316e-03 -1.2958e-02\n",
       " -2.1506e-02  6.6534e-03 -1.0212e-02  2.1433e-02 -1.0499e-02\n",
       " -9.0928e-03  1.5628e-02  1.1886e-02  9.6382e-03 -1.8932e-02\n",
       "  6.7430e-03 -1.7735e-02  1.2968e-02  1.3950e-02 -1.2412e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -7.3033e-03 -9.6375e-03 -1.6765e-02  5.9203e-03  1.2102e-02\n",
       " -1.8605e-02  2.5505e-02  9.5010e-03 -3.6182e-03 -2.1026e-02\n",
       "  6.9440e-03 -1.4352e-02 -1.9271e-02 -1.1019e-04  2.0428e-03\n",
       "  1.9763e-02 -4.9082e-03  1.3405e-03  6.9979e-04 -2.0806e-02\n",
       "  1.1378e-02  2.3315e-02 -1.7659e-02 -1.8258e-03 -4.3315e-04\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.7995e-02 -4.9583e-03  6.3116e-03 -1.3536e-02  1.4433e-02\n",
       "  2.4653e-02  5.6016e-03  1.9553e-02 -2.0848e-02 -7.4392e-03\n",
       " -1.8006e-02  1.7567e-02  5.3860e-03 -1.9548e-02  1.9358e-02\n",
       " -1.9094e-02 -5.6246e-03 -1.7039e-02 -5.1723e-03 -1.0268e-02\n",
       "  2.3830e-02 -1.2603e-02 -1.2014e-02  8.4320e-03 -2.1335e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  3.3651e-03 -2.9374e-03  5.6431e-03  9.1055e-03  1.8099e-02\n",
       "  2.3847e-02  1.1458e-02 -9.1567e-03 -1.6221e-02  1.7251e-02\n",
       " -1.3793e-02  2.4402e-02  5.0026e-03  1.2061e-02  2.2677e-02\n",
       " -1.3772e-03  2.0489e-02  4.4757e-04  1.3686e-03 -1.0282e-02\n",
       " -6.7459e-03 -4.5834e-03 -1.8638e-02 -1.6804e-02  1.6340e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  2.2726e-03 -1.7781e-02 -1.6517e-02  9.7338e-03 -1.2821e-03\n",
       " -2.3312e-02 -1.7351e-02  1.9172e-02  1.7839e-02  2.2680e-02\n",
       " -1.0755e-02 -2.0241e-02 -2.3159e-02 -2.1281e-02  1.6821e-02\n",
       " -1.0286e-02 -1.8717e-03 -1.3905e-02  1.1224e-02  1.8342e-02\n",
       " -5.6575e-03 -2.4588e-02 -1.1709e-02  1.7231e-02 -5.8803e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  8.9722e-03  7.6994e-03 -6.1710e-03 -1.7346e-02 -1.0553e-02\n",
       " -1.9065e-02 -4.7555e-03  1.8818e-03  2.3298e-03  4.8624e-03\n",
       " -5.3030e-03  1.5726e-02  1.9602e-02 -1.0429e-02  8.7251e-03\n",
       " -1.5885e-02  1.5797e-02  7.0807e-03  6.3033e-03  1.9082e-03\n",
       "  3.0532e-03 -1.6404e-02  1.7278e-02 -2.3992e-02 -1.4893e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  2.2734e-03  8.6821e-03  1.0706e-02  1.2268e-02 -4.1850e-03\n",
       " -1.8091e-02 -8.4180e-03 -7.9240e-03  5.9634e-03 -1.7016e-02\n",
       " -9.0636e-03  1.9540e-02  2.0140e-02  5.4666e-03 -2.4706e-02\n",
       "  8.3691e-03 -1.5574e-02  2.3885e-02  2.2704e-02  1.8427e-02\n",
       " -2.0067e-02  4.3661e-04 -4.2112e-03  8.2481e-03 -2.2887e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.4308e-02 -9.5898e-03  2.3750e-02  2.3004e-02 -1.6307e-02\n",
       "  1.5765e-02  2.3169e-02  1.0679e-02 -1.1085e-02  2.0628e-02\n",
       "  8.4566e-03 -2.1378e-02 -1.1902e-02 -6.9342e-03 -2.2068e-02\n",
       " -6.3314e-03 -2.2011e-02  5.2397e-03  1.3538e-02  1.3860e-02\n",
       "  7.8030e-03 -2.3762e-02  1.4535e-02  5.1957e-03 -2.1936e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  3.7074e-03  1.6220e-02  2.4980e-02 -1.4291e-02  5.7010e-03\n",
       " -1.4965e-02 -3.0771e-03  4.3778e-03 -2.4586e-02  6.5280e-03\n",
       " -1.4934e-02 -2.1475e-02  2.0766e-02  1.1532e-02  5.2004e-03\n",
       "  1.4769e-02  3.5629e-03  1.7762e-02 -2.9013e-03  5.2643e-04\n",
       " -1.6940e-02 -1.3351e-02 -6.3491e-03  2.3796e-02 -2.1506e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.2541e-02  1.1167e-02  1.1739e-02  2.3201e-02 -3.5539e-03\n",
       " -2.0052e-02 -7.7975e-03  1.3294e-02  2.2005e-02  9.3777e-03\n",
       "  1.1195e-02  7.4881e-03 -1.7157e-02  9.1117e-03  2.6419e-02\n",
       " -1.3660e-02  1.3795e-02 -8.1635e-03 -2.2401e-03  1.7745e-02\n",
       " -5.8353e-03  5.6632e-03  1.7998e-02 -2.0141e-02  2.5952e-03\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  7.4122e-04  9.2812e-03 -7.9120e-03 -2.4929e-02 -1.9314e-02\n",
       " -2.1690e-02 -1.9924e-02  1.6077e-02  1.4468e-03  2.0931e-02\n",
       " -4.9944e-03 -1.2125e-02 -1.2787e-02  1.9119e-02  2.0771e-02\n",
       " -4.4511e-03  1.4258e-02  1.6398e-02 -1.6357e-02  4.7478e-03\n",
       " -2.0676e-02  1.5762e-02  2.8027e-03  1.5717e-02  6.5052e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.6821e-02 -1.9790e-03  9.8683e-04  2.4800e-03 -9.8744e-03\n",
       " -1.9654e-02  6.3028e-03  8.2439e-03 -1.9898e-03  7.1746e-03\n",
       "  2.2315e-02 -2.4642e-02  6.8330e-03 -1.5569e-02  2.3157e-02\n",
       "  3.2500e-04  4.6957e-03 -2.0091e-02  6.0148e-03  2.4852e-03\n",
       "  4.3062e-03  2.1994e-02  1.4914e-02 -1.3887e-02  1.3293e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.9145e-02 -1.2170e-02  1.3510e-02 -1.9991e-02  5.8897e-03\n",
       "  1.9570e-02  2.5780e-02 -5.0493e-04 -2.4282e-02 -2.0446e-02\n",
       " -1.4590e-02 -2.9095e-03  3.5906e-03  1.8663e-02 -1.6835e-02\n",
       " -6.7832e-03  8.4137e-03  5.5342e-03 -4.4324e-03 -8.0609e-03\n",
       "  7.0907e-03  1.4839e-02 -1.4372e-02 -5.1883e-03  6.1437e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       " -0.0305 -0.0396 -0.0279 -0.0134 -0.0152\n",
       " -0.0114 -0.0266 -0.0116 -0.0244 -0.0115\n",
       " -0.0183 -0.0288 -0.0244 -0.0335 -0.0331\n",
       "  0.0048  0.0138 -0.0234 -0.0050 -0.0150\n",
       " -0.0089  0.0039  0.0133  0.0012 -0.0098\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -0.0149 -0.0453 -0.0115  0.0112  0.0002\n",
       " -0.0263 -0.0172 -0.0210  0.0020 -0.0010\n",
       " -0.0262 -0.0166 -0.0302 -0.0196 -0.0029\n",
       " -0.0282 -0.0147 -0.0169 -0.0244 -0.0162\n",
       " -0.0065 -0.0139 -0.0212  0.0036  0.0018\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -0.0039 -0.0094 -0.0027  0.0085  0.0162\n",
       " -0.0215 -0.0199  0.0074 -0.0156  0.0015\n",
       " -0.0125 -0.0276 -0.0038 -0.0135 -0.0058\n",
       "  0.0034  0.0195 -0.0195  0.0120 -0.0050\n",
       " -0.0144  0.0106  0.0295  0.0133  0.0012\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -0.0010 -0.0220 -0.0113 -0.0068 -0.0008\n",
       " -0.0075 -0.0106 -0.0159  0.0005 -0.0021\n",
       " -0.0197 -0.0172 -0.0132 -0.0122  0.0000\n",
       " -0.0088 -0.0091 -0.0031 -0.0062  0.0006\n",
       "  0.0066 -0.0009 -0.0075 -0.0113 -0.0007\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  0.0064  0.0128 -0.0052 -0.0077 -0.0012\n",
       " -0.0130 -0.0098 -0.0072  0.0094 -0.0054\n",
       " -0.0076  0.0050  0.0007  0.0026 -0.0190\n",
       " -0.0067 -0.0046  0.0013 -0.0050  0.0063\n",
       " -0.0083 -0.0124  0.0210  0.0165  0.0036\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -0.0107 -0.0242 -0.0173 -0.0013 -0.0090\n",
       " -0.0099 -0.0190 -0.0328 -0.0113 -0.0145\n",
       " -0.0035 -0.0110 -0.0057  0.0008 -0.0148\n",
       " -0.0205 -0.0231  0.0059  0.0081 -0.0180\n",
       " -0.0194 -0.0010 -0.0043 -0.0024 -0.0125\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       " -0.0095 -0.0099 -0.0243 -0.0168  0.0040\n",
       " -0.0139 -0.0295 -0.0125 -0.0115 -0.0106\n",
       " -0.0140 -0.0079 -0.0269 -0.0076 -0.0144\n",
       " -0.0062 -0.0131  0.0047 -0.0059  0.0010\n",
       " -0.0068  0.0152 -0.0129 -0.0195 -0.0056\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       " -0.0216 -0.0113 -0.0087  0.0076 -0.0025\n",
       " -0.0283  0.0046 -0.0094  0.0076  0.0188\n",
       " -0.0136  0.0188 -0.0007 -0.0031 -0.0182\n",
       "  0.0018  0.0314 -0.0090 -0.0072  0.0076\n",
       " -0.0030  0.0034 -0.0110 -0.0048 -0.0024\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -0.0227 -0.0091  0.0084  0.0208  0.0105\n",
       " -0.0350 -0.0281 -0.0081 -0.0046 -0.0056\n",
       " -0.0109 -0.0315 -0.0129  0.0072  0.0007\n",
       " -0.0118 -0.0172 -0.0207 -0.0025  0.0148\n",
       "  0.0003 -0.0152 -0.0295  0.0014  0.0118\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  0.0150  0.0138  0.0137  0.0085  0.0069\n",
       "  0.0074  0.0108  0.0012  0.0017  0.0070\n",
       "  0.0091  0.0124  0.0074  0.0040  0.0055\n",
       " -0.0005  0.0061  0.0042  0.0014  0.0003\n",
       "  0.0038  0.0020  0.0042  0.0003 -0.0033\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       " -0.0180 -0.0027 -0.0069 -0.0228  0.0115\n",
       "  0.0087 -0.0145 -0.0168 -0.0021 -0.0084\n",
       "  0.0018 -0.0103 -0.0002  0.0095  0.0118\n",
       " -0.0029 -0.0120 -0.0177 -0.0152 -0.0119\n",
       " -0.0194 -0.0045 -0.0158 -0.0178 -0.0224\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -0.0099  0.0030  0.0054  0.0022 -0.0061\n",
       "  0.0051 -0.0039 -0.0092 -0.0068 -0.0031\n",
       "  0.0180  0.0273 -0.0091 -0.0150  0.0015\n",
       "  0.0094  0.0249 -0.0108 -0.0161  0.0055\n",
       " -0.0011 -0.0022 -0.0090  0.0026 -0.0254\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  0.0026  0.0023  0.0018 -0.0022  0.0005\n",
       "  0.0004 -0.0002  0.0026  0.0010  0.0011\n",
       " -0.0002  0.0002  0.0017  0.0025 -0.0012\n",
       " -0.0009  0.0018  0.0000 -0.0003 -0.0005\n",
       "  0.0008  0.0029  0.0022 -0.0017  0.0003\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       " -0.0011 -0.0023  0.0002 -0.0002 -0.0013\n",
       " -0.0012 -0.0015  0.0009 -0.0022 -0.0058\n",
       " -0.0032 -0.0008 -0.0016 -0.0021 -0.0014\n",
       "  0.0002 -0.0036 -0.0021 -0.0035 -0.0016\n",
       "  0.0005 -0.0021 -0.0020 -0.0028 -0.0032\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  0.0037  0.0019 -0.0008  0.0009  0.0032\n",
       "  0.0038  0.0032  0.0044  0.0030  0.0000\n",
       "  0.0025  0.0011  0.0005 -0.0009 -0.0035\n",
       "  0.0034  0.0051  0.0004 -0.0009  0.0006\n",
       "  0.0031  0.0066  0.0034  0.0001 -0.0002\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -0.0009 -0.0017 -0.0015 -0.0003 -0.0003\n",
       " -0.0028 -0.0014 -0.0011 -0.0009 -0.0005\n",
       " -0.0014 -0.0018  0.0005 -0.0010 -0.0007\n",
       " -0.0008 -0.0007 -0.0013 -0.0019 -0.0022\n",
       " -0.0007 -0.0021 -0.0008 -0.0013 -0.0001\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  0.0038  0.0055  0.0049  0.0017  0.0031\n",
       "  0.0050  0.0057  0.0023  0.0011  0.0010\n",
       "  0.0007  0.0008 -0.0007 -0.0018 -0.0036\n",
       "  0.0007  0.0052  0.0036  0.0047 -0.0001\n",
       "  0.0008  0.0014  0.0032  0.0013  0.0016\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  0.0027  0.0029  0.0038  0.0009 -0.0003\n",
       "  0.0011  0.0010 -0.0001 -0.0024 -0.0004\n",
       "  0.0011 -0.0008 -0.0031 -0.0028 -0.0005\n",
       " -0.0018 -0.0023 -0.0008  0.0022  0.0017\n",
       " -0.0027  0.0001 -0.0015  0.0023  0.0025\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       " -0.0163 -0.0056 -0.0087 -0.0157 -0.0132\n",
       " -0.0167 -0.0109 -0.0019 -0.0096 -0.0097\n",
       " -0.0118  0.0006 -0.0084 -0.0043 -0.0098\n",
       " -0.0103 -0.0084  0.0035  0.0042 -0.0070\n",
       " -0.0058  0.0004 -0.0045 -0.0039 -0.0049\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -0.0216 -0.0011 -0.0153 -0.0093 -0.0117\n",
       " -0.0232 -0.0043 -0.0020 -0.0031 -0.0139\n",
       " -0.0223 -0.0087 -0.0116 -0.0103 -0.0130\n",
       " -0.0130 -0.0003 -0.0036 -0.0162 -0.0122\n",
       " -0.0174 -0.0087  0.0022 -0.0063 -0.0168\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -0.0167 -0.0004 -0.0128 -0.0140 -0.0139\n",
       " -0.0225 -0.0131 -0.0103 -0.0056 -0.0137\n",
       " -0.0190  0.0029 -0.0124 -0.0210 -0.0194\n",
       " -0.0134  0.0006  0.0072 -0.0039 -0.0050\n",
       "  0.0061  0.0110  0.0039  0.0159  0.0136\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       " -0.0140 -0.0096 -0.0188 -0.0156 -0.0106\n",
       " -0.0113 -0.0050 -0.0059 -0.0090 -0.0152\n",
       " -0.0134 -0.0095 -0.0072 -0.0129 -0.0064\n",
       " -0.0120 -0.0013 -0.0003 -0.0133 -0.0108\n",
       " -0.0179 -0.0065 -0.0036 -0.0113 -0.0050\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -0.0127 -0.0078 -0.0087 -0.0068 -0.0012\n",
       " -0.0050 -0.0189 -0.0180 -0.0200 -0.0100\n",
       " -0.0083 -0.0118  0.0047 -0.0121 -0.0110\n",
       " -0.0054 -0.0008  0.0021  0.0105  0.0021\n",
       " -0.0168 -0.0005  0.0002 -0.0129 -0.0028\n",
       "\n",
       "(189,63 ,.,.) = \n",
       " -0.0254 -0.0152 -0.0041 -0.0163 -0.0121\n",
       " -0.0153 -0.0093 -0.0092 -0.0117 -0.0158\n",
       " -0.0104 -0.0079 -0.0071 -0.0092 -0.0093\n",
       " -0.0151 -0.0076 -0.0159 -0.0040 -0.0010\n",
       " -0.0037 -0.0088 -0.0148 -0.0088 -0.0037\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       "  0.0080  0.0084  0.0001 -0.0073 -0.0227\n",
       " -0.0082  0.0150 -0.0007 -0.0198 -0.0045\n",
       " -0.0079 -0.0206 -0.0097 -0.0123 -0.0168\n",
       " -0.0215 -0.0164 -0.0147 -0.0176 -0.0266\n",
       " -0.0285 -0.0160 -0.0233 -0.0117 -0.0205\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       " -0.0383 -0.0143  0.0003 -0.0203  0.0038\n",
       " -0.0167 -0.0216 -0.0185 -0.0238 -0.0055\n",
       " -0.0110 -0.0153 -0.0276 -0.0214 -0.0086\n",
       " -0.0012 -0.0302 -0.0133 -0.0066  0.0071\n",
       " -0.0146 -0.0169 -0.0106 -0.0047 -0.0007\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  0.0144  0.0065 -0.0132 -0.0053 -0.0029\n",
       " -0.0003  0.0124  0.0006 -0.0038 -0.0175\n",
       " -0.0096 -0.0132 -0.0102 -0.0072 -0.0036\n",
       " -0.0132 -0.0090 -0.0089 -0.0114  0.0026\n",
       " -0.0129 -0.0201 -0.0066  0.0013 -0.0142\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       " -0.0182 -0.0152 -0.0062 -0.0067 -0.0067\n",
       " -0.0042 -0.0014 -0.0004 -0.0053 -0.0104\n",
       " -0.0066 -0.0060 -0.0006  0.0002 -0.0076\n",
       " -0.0120 -0.0026 -0.0041 -0.0040 -0.0099\n",
       " -0.0118 -0.0069 -0.0012 -0.0055 -0.0093\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  0.0006  0.0098 -0.0210 -0.0128 -0.0169\n",
       "  0.0021 -0.0011  0.0040 -0.0211 -0.0226\n",
       "  0.0013  0.0028  0.0104 -0.0033 -0.0133\n",
       " -0.0198 -0.0072 -0.0112 -0.0087 -0.0255\n",
       " -0.0121  0.0040  0.0095  0.0027 -0.0146\n",
       "\n",
       "(190,63 ,.,.) = \n",
       " -0.0103  0.0020 -0.0055 -0.0109 -0.0099\n",
       " -0.0212 -0.0080 -0.0146 -0.0088 -0.0177\n",
       "  0.0007 -0.0037 -0.0117 -0.0048 -0.0104\n",
       "  0.0022 -0.0024 -0.0130  0.0136 -0.0030\n",
       "  0.0012 -0.0008  0.0102  0.0165 -0.0013\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -0.0130  0.0098 -0.0047 -0.0243 -0.0040\n",
       "  0.0308  0.0091 -0.0070 -0.0171  0.0026\n",
       "  0.0041  0.0054  0.0049 -0.0061 -0.0180\n",
       " -0.0194  0.0015 -0.0122 -0.0081 -0.0209\n",
       " -0.0033 -0.0173 -0.0211  0.0155 -0.0133\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  0.0016 -0.0458 -0.0290  0.0074 -0.0102\n",
       " -0.0196 -0.0453 -0.0218  0.0368  0.0228\n",
       " -0.0114 -0.0123 -0.0081 -0.0005  0.0057\n",
       " -0.0169  0.0045 -0.0193  0.0344  0.0030\n",
       " -0.0307 -0.0322  0.0290  0.0280  0.0128\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       " -0.0131 -0.0315 -0.0400 -0.0101 -0.0079\n",
       " -0.0121 -0.0402 -0.0180 -0.0532 -0.0330\n",
       " -0.0246 -0.0273 -0.0388 -0.0098 -0.0030\n",
       " -0.0222 -0.0063  0.0074 -0.0336 -0.0307\n",
       " -0.0198 -0.0204 -0.0155 -0.0343 -0.0071\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  0.0026  0.0041 -0.0108  0.0135  0.0073\n",
       " -0.0052  0.0062  0.0124  0.0251  0.0077\n",
       " -0.0026  0.0043  0.0019  0.0065  0.0156\n",
       " -0.0033 -0.0133  0.0058  0.0059  0.0115\n",
       " -0.0150 -0.0076  0.0024 -0.0008  0.0045\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  0.0088 -0.0082 -0.0180 -0.0274 -0.0209\n",
       " -0.0221 -0.0099 -0.0162 -0.0216 -0.0091\n",
       " -0.0189 -0.0123 -0.0014 -0.0367 -0.0449\n",
       " -0.0164 -0.0280  0.0198 -0.0062 -0.0368\n",
       " -0.0423 -0.0337 -0.0199 -0.0208 -0.0177\n",
       "\n",
       "(191,63 ,.,.) = \n",
       " -0.0267 -0.0395 -0.0140 -0.0011  0.0010\n",
       " -0.0123 -0.0253 -0.0245 -0.0256 -0.0057\n",
       "  0.0054 -0.0044 -0.0222 -0.0134 -0.0357\n",
       "  0.0078 -0.0066  0.0117 -0.0053 -0.0158\n",
       " -0.0101 -0.0183 -0.0315 -0.0158 -0.0442\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.8670e-02  2.2519e-03  4.0718e-03 -1.2053e-02  1.5346e-02\n",
       "  7.2466e-03  2.0200e-02 -1.0133e-02 -2.1329e-03 -1.2672e-02\n",
       "  2.1838e-02 -2.0518e-02 -8.0581e-03  6.3540e-03  2.6678e-02\n",
       "  2.4403e-02 -2.0051e-02  2.2867e-02  2.3579e-03 -9.9237e-03\n",
       " -1.0284e-02  2.7908e-03 -8.8978e-03 -2.0199e-02 -2.3274e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1.7704e-03 -1.8924e-03 -1.0922e-02 -1.5883e-02 -1.8224e-03\n",
       " -1.2829e-02 -2.0913e-02  1.2217e-02  2.5780e-02 -1.2984e-04\n",
       "  2.9267e-03 -1.2853e-02 -5.1088e-03  2.1482e-02  5.8533e-03\n",
       "  2.2906e-03 -3.0871e-03 -5.7004e-03  1.4502e-02  1.4483e-02\n",
       "  1.2501e-02  1.0770e-02 -1.9770e-04  1.1162e-02 -1.2295e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -8.3275e-03 -1.1672e-03  6.1495e-03 -2.6007e-02 -2.4467e-03\n",
       " -1.4519e-02 -3.3815e-03  1.6122e-02  8.5915e-04 -1.9005e-02\n",
       " -2.3674e-02  2.4657e-03 -2.3547e-02 -7.8640e-03  1.0768e-02\n",
       "  2.3381e-02 -2.2082e-02  2.2670e-02  1.4836e-02 -1.5474e-02\n",
       " -1.4576e-02 -5.6797e-03 -1.9072e-02 -2.1535e-03  4.0057e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -7.3399e-03  5.5056e-03  7.6906e-03  2.1133e-02 -2.3271e-03\n",
       " -9.2341e-03 -1.3682e-02  4.1928e-03 -2.3184e-02 -1.8525e-02\n",
       "  2.5907e-02 -2.5945e-02 -1.2084e-02  1.0320e-02 -2.2313e-02\n",
       "  9.4249e-03 -4.0526e-03 -1.1258e-02  2.0531e-02  2.1654e-02\n",
       " -2.2652e-02 -1.3649e-02  1.2735e-02 -1.2371e-02 -1.1598e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  2.2827e-03 -2.3022e-02 -2.1071e-02  4.2460e-03 -1.1342e-02\n",
       " -2.0824e-02 -1.9459e-02  1.2625e-02  6.2654e-03  1.3067e-02\n",
       " -1.3808e-03 -1.1724e-02 -3.0852e-03  1.6897e-02  2.2999e-02\n",
       "  5.5358e-03 -1.5501e-02 -1.7133e-02 -8.7850e-03 -1.8876e-02\n",
       " -3.9984e-03 -1.4232e-02  1.7678e-02 -4.6650e-03 -7.2793e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -8.2504e-03 -1.8230e-02  2.6378e-02 -5.5046e-03 -1.0436e-02\n",
       " -1.5695e-02  1.5343e-02 -1.0780e-02  2.4873e-03 -1.5936e-02\n",
       "  2.3924e-03 -2.2019e-02 -6.1866e-03  6.0553e-04  2.2464e-02\n",
       " -6.9490e-03  1.5363e-02 -1.1436e-02  2.1486e-02 -2.7997e-03\n",
       " -2.4099e-03 -1.7311e-02 -2.1271e-02 -2.0207e-02  7.5329e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1.9105e-02 -1.6569e-02  2.2941e-02 -1.4427e-02  1.3617e-02\n",
       "  2.1528e-03 -1.3728e-02 -8.1956e-04  1.6026e-02  1.9993e-02\n",
       "  1.5530e-02  2.2236e-02  5.8993e-03  2.2328e-03  6.4849e-03\n",
       " -1.3503e-02  3.4639e-03  1.9793e-02  1.9670e-02 -4.5792e-03\n",
       " -9.5303e-03 -4.2561e-03  6.6372e-03  2.1478e-02  7.4338e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.5559e-02  1.6592e-02 -1.6284e-02 -2.3023e-02 -7.6557e-03\n",
       "  2.4712e-02 -1.9364e-02  4.1592e-03  2.0739e-03 -1.2089e-02\n",
       "  2.0948e-03 -1.7468e-02  6.8909e-03  5.2689e-03  1.5606e-02\n",
       "  1.2102e-02 -1.7167e-02 -1.0661e-02  1.6532e-02  2.2462e-02\n",
       "  7.5405e-03 -5.8228e-03 -2.1221e-02 -1.0767e-02  1.2513e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.8566e-02  1.9733e-02 -1.7857e-04  8.4009e-03  2.3603e-02\n",
       " -2.0286e-02  1.5572e-02  1.9252e-02 -2.5934e-03 -2.4064e-02\n",
       " -3.3300e-03 -1.9621e-03  6.8216e-03 -1.0468e-02 -2.0141e-02\n",
       " -8.0692e-04  1.3554e-02 -5.5379e-03  5.0553e-03 -1.9428e-02\n",
       "  1.9428e-03  7.6142e-03 -4.9818e-03  2.0506e-02 -1.2991e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  2.0637e-02  8.5368e-03 -1.3934e-02  2.2987e-02  1.7503e-02\n",
       " -1.1677e-03  2.3216e-02  3.2018e-04  2.6247e-02  1.9151e-02\n",
       " -1.8164e-02  7.4293e-03  2.2689e-02 -1.1687e-02  2.2509e-02\n",
       " -5.6037e-03 -1.3010e-02 -2.5217e-03 -1.5104e-02  5.0626e-03\n",
       " -2.1716e-03  2.8965e-03  1.6445e-02  2.0724e-02  5.6710e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.1546e-02  1.1953e-02  1.5979e-02  2.9715e-03  1.0598e-02\n",
       "  8.8745e-03 -1.8374e-02 -1.8471e-02 -1.1304e-02  3.6013e-03\n",
       " -1.7834e-02 -5.2165e-03 -5.8118e-03 -6.3218e-03 -4.2718e-03\n",
       " -2.0055e-02  4.2288e-03 -1.9146e-02 -1.9066e-02  6.7234e-03\n",
       "  1.1631e-02 -1.8696e-02  6.5212e-03  2.2672e-02  1.0341e-02\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1.4405e-03  3.8690e-03 -1.6284e-02  2.3745e-02 -1.4042e-02\n",
       " -9.0710e-03 -9.6603e-03 -1.6747e-02 -1.3406e-02  5.2242e-03\n",
       "  9.7888e-03  1.2946e-02  2.4013e-02 -7.3156e-04  2.0200e-02\n",
       " -2.3753e-02 -1.8835e-02 -2.2833e-02 -1.2771e-02 -8.0316e-03\n",
       " -1.2512e-02 -2.1876e-02  2.2577e-02 -5.9588e-04  1.0818e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1.2320e-02 -8.2558e-03 -1.0018e-02 -1.4543e-02  1.7418e-02\n",
       " -7.1733e-03  1.3230e-02 -1.5497e-02 -2.1842e-02  9.3744e-03\n",
       "  1.7635e-02  2.1739e-02 -9.4136e-03  8.7187e-03 -2.1039e-02\n",
       " -6.6724e-03 -1.9121e-02 -1.5147e-02 -2.4555e-02 -7.7806e-03\n",
       "  2.1512e-02  1.8051e-02  1.4592e-02 -3.8636e-03  4.4773e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1.0419e-02  1.1175e-03  1.2022e-02  6.3997e-03 -1.3627e-02\n",
       " -1.4988e-02 -2.1716e-02 -1.2959e-02 -1.3515e-02  8.5271e-03\n",
       "  7.9967e-03  1.2694e-02  2.6667e-02  1.8991e-02  9.5414e-03\n",
       "  2.6516e-02  1.6057e-02 -8.6534e-03  1.2635e-02 -1.4491e-02\n",
       "  1.5318e-02 -6.0597e-03  6.2457e-03  8.7008e-03  1.1348e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.7211e-02 -6.5809e-03  4.3338e-03 -9.2861e-03 -1.5834e-02\n",
       " -3.5359e-03 -1.6427e-02 -1.9155e-02 -9.8790e-03 -2.1348e-02\n",
       "  2.2434e-02 -2.5540e-02  1.0802e-02  5.2642e-03 -1.4406e-02\n",
       " -1.9447e-02 -1.2026e-03  6.4859e-03  1.6601e-02  1.7098e-02\n",
       "  1.5083e-02  8.2247e-03  1.2362e-02  9.8584e-03 -2.5765e-03\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.7969e-02  1.0552e-04 -8.8331e-03  4.2374e-03  2.2378e-02\n",
       "  1.0969e-02  6.1506e-04  1.3202e-02  1.3283e-02 -1.1140e-02\n",
       " -2.2678e-03 -9.7112e-03 -1.3434e-02  2.4659e-02  2.0132e-02\n",
       " -1.4021e-02 -1.5115e-02 -2.2483e-02 -5.4310e-03  8.2147e-03\n",
       " -9.1615e-03 -1.4111e-02 -1.9775e-02 -1.9246e-02 -6.9944e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.8559e-02  1.0649e-02  2.0254e-02  1.5598e-02 -1.1228e-02\n",
       "  1.2857e-02 -1.8859e-02  2.1911e-02  2.6598e-02 -1.0640e-02\n",
       "  1.9808e-02  6.1613e-03  1.7403e-02 -9.2771e-03  6.8081e-03\n",
       " -2.6568e-03  9.1057e-03  5.0440e-03  1.4531e-02  1.7950e-02\n",
       "  5.8364e-03  2.0602e-02 -1.4418e-04 -1.6905e-02  1.1065e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.4518e-02 -2.3477e-02  6.0982e-03  1.4966e-02 -1.7902e-02\n",
       " -2.2894e-02  1.7831e-02  9.8696e-03  1.8387e-02  1.1258e-03\n",
       " -6.6692e-03  2.2610e-02 -4.6073e-03 -2.1262e-02  1.6892e-02\n",
       "  2.4691e-02 -1.9755e-02 -1.7883e-02  2.1821e-02 -1.1197e-02\n",
       "  2.4450e-02  2.3402e-02 -1.1416e-02 -9.0451e-03 -8.0720e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.8046e-02 -2.3537e-02 -1.6262e-02 -4.8368e-03 -1.5394e-02\n",
       "  1.8241e-02  1.1182e-02 -9.0376e-03 -1.8592e-02  2.1043e-02\n",
       "  2.1950e-02 -9.5889e-03  1.5639e-02  2.0315e-02  9.1451e-03\n",
       " -2.3267e-03 -1.2284e-02 -7.1756e-05 -1.0619e-02  2.0109e-02\n",
       "  4.4275e-03  1.3744e-02 -1.1429e-02 -1.6875e-02  9.7686e-03\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.3246e-02  1.2735e-02  8.6130e-03  1.6734e-02  3.9059e-03\n",
       "  1.8977e-02  7.5253e-03 -1.0851e-02 -1.3775e-02  1.5560e-02\n",
       "  1.1163e-02  1.7825e-02 -1.1147e-02 -6.2173e-04  1.3400e-03\n",
       "  9.4686e-03 -8.1568e-03 -1.2844e-02 -1.1887e-02  2.1063e-02\n",
       "  1.3916e-03  1.6072e-02 -1.8647e-02  7.0476e-03  2.5042e-03\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.3556e-02 -7.2452e-03 -1.7225e-02  1.1344e-02 -1.1336e-02\n",
       " -2.0284e-02 -2.0633e-02 -6.4459e-03  3.4400e-03  2.4381e-02\n",
       " -3.0923e-03 -2.0966e-02  8.9185e-03  1.4266e-02  2.4969e-02\n",
       "  7.9520e-03  1.8698e-02  2.1713e-02 -2.1202e-02 -1.0432e-02\n",
       " -2.7984e-03 -1.4452e-02  3.4756e-03  1.9380e-02 -1.2840e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1.1448e-02 -5.8377e-03  2.5046e-02  1.2264e-02 -1.3176e-03\n",
       "  8.5886e-03 -8.4665e-03  8.9383e-03  2.0220e-02 -6.3645e-03\n",
       " -2.0364e-02  1.9690e-02 -6.6556e-03  2.7742e-03 -1.1188e-02\n",
       " -2.1234e-02 -7.4628e-03  1.9806e-02  2.0683e-02 -2.1219e-02\n",
       "  2.1238e-02 -2.7510e-03  2.4952e-02 -2.3170e-02 -2.3968e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -3.4050e-03 -5.1940e-03  1.1263e-02 -1.1307e-02 -4.8074e-03\n",
       "  1.8097e-03  1.4678e-02  2.2424e-02 -4.6559e-03 -6.1757e-03\n",
       "  1.2877e-02 -1.9268e-02 -5.7676e-04 -5.7181e-03  2.6044e-02\n",
       " -6.9507e-03 -8.2867e-03 -8.2182e-03 -2.0289e-02 -2.0671e-02\n",
       "  1.0301e-02  1.3992e-02 -2.1944e-02  2.6177e-02  1.3970e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.2837e-02 -1.2078e-02  1.1061e-02  4.9365e-03  1.9722e-02\n",
       "  2.5484e-02 -1.6645e-02 -2.0138e-02 -3.0087e-03 -1.2823e-02\n",
       " -2.0875e-02  7.4963e-03 -9.3639e-03  2.2183e-02 -9.6396e-03\n",
       " -8.8622e-03  1.5942e-02  1.2628e-02  1.0482e-02 -1.8276e-02\n",
       "  7.5114e-03 -1.6877e-02  1.3392e-02  1.4190e-02 -1.1644e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -7.0010e-03 -9.3824e-03 -1.6193e-02  6.7654e-03  1.2865e-02\n",
       " -1.7797e-02  2.5439e-02  9.0229e-03 -3.1898e-03 -2.0862e-02\n",
       "  7.8012e-03 -1.3555e-02 -1.8953e-02 -2.6799e-04  2.6673e-03\n",
       "  2.0622e-02 -4.0580e-03  2.1546e-03  7.7087e-04 -2.0151e-02\n",
       "  1.2140e-02  2.4143e-02 -1.6828e-02 -9.6826e-04  3.7081e-04\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.8570e-02 -4.5321e-03  5.7150e-03 -1.2961e-02  1.3622e-02\n",
       "  2.4875e-02  6.0688e-03  2.0093e-02 -2.0456e-02 -7.5649e-03\n",
       " -1.7649e-02  1.8139e-02  6.0733e-03 -1.8994e-02  2.0107e-02\n",
       " -1.9482e-02 -5.0889e-03 -1.6822e-02 -5.0680e-03 -1.1071e-02\n",
       "  2.3988e-02 -1.2587e-02 -1.1856e-02  8.4337e-03 -2.1866e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  3.5241e-03 -2.7112e-03  6.2973e-03  9.7319e-03  1.8664e-02\n",
       "  2.4439e-02  1.0968e-02 -8.6145e-03 -1.5376e-02  1.7948e-02\n",
       " -1.2935e-02  2.5098e-02  5.8532e-03  1.2842e-02  2.3337e-02\n",
       " -5.3009e-04  2.1346e-02  1.2471e-03  2.2014e-03 -9.8304e-03\n",
       " -5.8872e-03 -3.7409e-03 -1.7845e-02 -1.7671e-02  1.7177e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  2.8432e-03 -1.7505e-02 -1.6710e-02  1.0037e-02 -1.3251e-03\n",
       " -2.3581e-02 -1.7777e-02  1.8641e-02  1.7586e-02  2.2702e-02\n",
       " -1.0578e-02 -2.0103e-02 -2.3598e-02 -2.1876e-02  1.6998e-02\n",
       " -9.6156e-03 -1.5975e-03 -1.3788e-02  1.1007e-02  1.8400e-02\n",
       " -5.2096e-03 -2.4455e-02 -1.1717e-02  1.7118e-02 -5.6230e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  9.5110e-03  7.4729e-03 -5.3286e-03 -1.6515e-02 -9.7323e-03\n",
       " -1.8635e-02 -4.1297e-03  2.0368e-03  3.0251e-03  5.5285e-03\n",
       " -4.8662e-03  1.6057e-02  1.9415e-02 -1.0821e-02  9.1630e-03\n",
       " -1.5349e-02  1.6564e-02  7.8934e-03  6.5254e-03  2.3483e-03\n",
       "  3.0808e-03 -1.6148e-02  1.6661e-02 -2.4807e-02 -1.4977e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  3.0798e-03  8.1739e-03  1.0655e-02  1.2138e-02 -3.9180e-03\n",
       " -1.7254e-02 -7.5638e-03 -7.3530e-03  6.7505e-03 -1.6334e-02\n",
       " -8.5179e-03  2.0351e-02  2.0164e-02  6.0206e-03 -2.4103e-02\n",
       "  7.5062e-03 -1.4729e-02  2.4534e-02  2.2018e-02  1.8050e-02\n",
       " -1.9543e-02  1.0441e-03 -4.0711e-03  8.2021e-03 -2.2020e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.3580e-02 -9.2796e-03  2.4614e-02  2.3861e-02 -1.5630e-02\n",
       "  1.5582e-02  2.2632e-02  1.1446e-02 -1.0233e-02  2.0752e-02\n",
       "  8.9332e-03 -2.0991e-02 -1.1641e-02 -6.2087e-03 -2.1310e-02\n",
       " -5.4981e-03 -2.1777e-02  5.7550e-03  1.4263e-02  1.4705e-02\n",
       "  8.4598e-03 -2.3380e-02  1.4696e-02  5.5077e-03 -2.1390e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  3.1032e-03  1.6355e-02  2.4870e-02 -1.4955e-02  5.3029e-03\n",
       " -1.5089e-02 -2.9769e-03  4.3013e-03 -2.5432e-02  5.7325e-03\n",
       " -1.5201e-02 -2.1835e-02  2.0351e-02  1.0961e-02  4.5630e-03\n",
       "  1.4695e-02  2.8762e-03  1.7635e-02 -3.7575e-03 -9.2043e-05\n",
       " -1.6739e-02 -1.3173e-02 -7.2054e-03  2.2961e-02 -2.8976e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.3310e-02  1.1999e-02  1.2588e-02  2.3936e-02 -2.8494e-03\n",
       " -1.9326e-02 -6.9395e-03  1.4118e-02  2.2814e-02  1.0237e-02\n",
       "  1.1986e-02  8.3442e-03 -1.6309e-02  9.8632e-03  2.7161e-02\n",
       " -1.2876e-02  1.4518e-02 -7.6771e-03 -1.3956e-03  1.8585e-02\n",
       " -4.9774e-03  6.4292e-03  1.8809e-02 -1.9363e-02  3.4013e-03\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1.2765e-04  8.6574e-03 -8.3671e-03 -2.5633e-02 -1.9983e-02\n",
       " -2.2185e-02 -2.0584e-02  1.5347e-02  6.2846e-04  2.0257e-02\n",
       " -5.5350e-03 -1.2754e-02 -1.3388e-02  1.8476e-02  2.0015e-02\n",
       " -4.9486e-03  1.3905e-02  1.5753e-02 -1.7002e-02  4.0364e-03\n",
       " -2.0980e-02  1.5303e-02  2.1979e-03  1.5153e-02  5.8710e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.6153e-02 -2.4644e-03  6.8616e-04  2.2691e-03 -1.0124e-02\n",
       " -1.9592e-02  5.9084e-03  8.0661e-03 -2.3547e-03  6.6889e-03\n",
       "  2.2338e-02 -2.5033e-02  6.2770e-03 -1.5753e-02  2.3221e-02\n",
       " -2.4655e-06  4.3782e-03 -2.0854e-02  5.5371e-03  2.3293e-03\n",
       "  4.5042e-03  2.1750e-02  1.4522e-02 -1.4266e-02  1.2989e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.9934e-02 -1.1313e-02  1.4127e-02 -2.0543e-02  6.3847e-03\n",
       "  1.9417e-02  2.6635e-02  3.5149e-04 -2.4509e-02 -2.0893e-02\n",
       " -1.4504e-02 -2.2117e-03  3.7037e-03  1.8773e-02 -1.6588e-02\n",
       " -7.6201e-03  8.2898e-03  4.6790e-03 -4.6200e-03 -8.1411e-03\n",
       "  7.9393e-03  1.4588e-02 -1.4134e-02 -5.0342e-03  6.5145e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.binarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       " -1  1 -1 -1 -1\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1  1 -1  1 -1\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -1  1  1  1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "  1 -1 -1  1  1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  1 -1 -1  1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1  1 -1  1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1 -1 -1  1  1\n",
       "  1  1  1  1  1\n",
       " -1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1 -1  1\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1  1  1  1  1\n",
       " -1  1  1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1  1 -1  1 -1\n",
       "  1  1 -1  1 -1\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  1  1 -1  1  1\n",
       " -1  1 -1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  1  1  1  1  1\n",
       "  1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1 -1  1\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1 -1 -1 -1  1\n",
       " -1  1 -1 -1  1\n",
       "  1  1 -1  1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1  1  1\n",
       "  1  1 -1  1 -1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1  1\n",
       "  1  1  1  1 -1\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1 -1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1 -1 -1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       " -1 -1  1 -1  1\n",
       "  1  1 -1 -1  1\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1  1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1  1 -1  1  1\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1 -1 -1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       "  1  1  1 -1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1 -1  1  1\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1  1 -1 -1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  1 -1 -1  1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       " -1 -1 -1  1 -1\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       "  1 -1  1  1 -1\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       " -1 -1  1 -1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "  1  1 -1  1  1\n",
       " -1  1 -1 -1  1\n",
       " -1  1  1 -1  1\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1 -1  1\n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1  1  1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       " -1  1  1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.8873e-02  7.5993e-04  2.5151e-03 -1.3646e-02  1.3772e-02\n",
       "  7.8605e-03  1.8820e-02 -1.1640e-02 -3.6067e-03 -1.2481e-02\n",
       "  2.2044e-02 -2.1994e-02 -9.5977e-03  4.8411e-03  2.5192e-02\n",
       "  2.3820e-02 -2.0491e-02  2.1387e-02  7.5171e-04 -1.1523e-02\n",
       " -1.1641e-02  2.2801e-03 -7.4655e-03 -2.1435e-02 -3.9254e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1.7088e-03 -3.4021e-03 -1.2472e-02 -1.4329e-02 -4.8686e-04\n",
       " -1.4436e-02 -2.2507e-02  1.0649e-02  2.4689e-02 -1.6210e-03\n",
       "  1.4775e-03 -1.4429e-02 -4.9556e-03  2.1881e-02  4.2458e-03\n",
       "  8.3148e-04 -4.5646e-03 -5.5588e-03  1.4497e-02  1.4675e-02\n",
       "  1.3019e-02  1.1046e-02 -9.1878e-05  1.0265e-02 -1.0822e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -9.8510e-03 -2.4725e-04  7.3819e-03 -2.4469e-02 -8.3953e-04\n",
       " -1.4348e-02 -2.9021e-03  1.7654e-02  1.1927e-03 -1.9339e-02\n",
       " -2.3239e-02  2.8028e-03 -2.5049e-02 -9.4663e-03  1.1083e-02\n",
       "  2.2274e-02 -2.0638e-02  2.1082e-02  1.3878e-02 -1.6996e-02\n",
       " -1.4111e-02 -4.2093e-03 -1.9636e-02 -7.8934e-04  3.7102e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -6.3411e-03  4.0243e-03  7.9108e-03  2.1537e-02 -1.1378e-03\n",
       " -1.0838e-02 -1.5243e-02  4.3175e-03 -2.3765e-02 -1.7476e-02\n",
       "  2.4465e-02 -2.5725e-02 -1.1770e-02  1.0681e-02 -2.1020e-02\n",
       "  9.9313e-03 -4.1354e-03 -1.0405e-02  2.0778e-02  2.3030e-02\n",
       " -2.1142e-02 -1.2516e-02  1.3223e-02 -1.1810e-02 -1.0359e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  3.8911e-03 -2.1435e-02 -1.9913e-02  5.3589e-03 -1.0109e-02\n",
       " -2.0404e-02 -1.8715e-02  1.3386e-02  7.7525e-03  1.3887e-02\n",
       " -3.9169e-04 -1.0079e-02 -3.6061e-03  1.8469e-02  2.3778e-02\n",
       "  4.0349e-03 -1.6948e-02 -1.8247e-02 -1.0364e-02 -1.7491e-02\n",
       " -3.6643e-03 -1.5824e-02  1.9101e-02 -3.2313e-03 -8.2563e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -7.7283e-03 -1.9673e-02  2.4791e-02 -7.1117e-03 -1.1897e-02\n",
       " -1.5557e-02  1.5443e-02 -1.2357e-02  9.2839e-04 -1.7495e-02\n",
       "  7.8128e-04 -2.3618e-02 -7.7252e-03 -6.8038e-04  2.0925e-02\n",
       " -6.7971e-03  1.5516e-02 -1.2196e-02  2.0723e-02 -4.2917e-03\n",
       " -1.9835e-03 -1.6046e-02 -2.0495e-02 -2.1791e-02  8.1017e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1.9844e-02 -1.8156e-02  2.1395e-02 -1.6033e-02  1.5197e-02\n",
       "  2.7841e-03 -1.5253e-02 -2.4095e-03  1.4480e-02  1.8496e-02\n",
       "  1.6425e-02  2.0631e-02  4.4349e-03  6.2910e-04  4.9091e-03\n",
       " -1.2448e-02  4.2352e-03  2.1354e-02  1.9868e-02 -5.8433e-03\n",
       " -8.6355e-03 -2.7518e-03  7.1805e-03  2.1675e-02  8.1735e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.4088e-02  1.4989e-02 -1.7872e-02 -2.3818e-02 -9.2239e-03\n",
       "  2.3243e-02 -2.0313e-02  2.6963e-03  1.8287e-03 -1.2372e-02\n",
       "  2.2804e-03 -1.7609e-02  5.4868e-03  3.6703e-03  1.4207e-02\n",
       "  1.0992e-02 -1.7439e-02 -1.2247e-02  1.4933e-02  2.1778e-02\n",
       "  6.0057e-03 -6.8270e-03 -2.2818e-02 -1.2310e-02  1.0997e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.8421e-02  2.0306e-02  1.4248e-03  8.2208e-03  2.5037e-02\n",
       " -2.0123e-02  1.5769e-02  1.9638e-02 -2.0141e-03 -2.3663e-02\n",
       " -2.8459e-03 -1.6847e-03  5.2134e-03 -8.8668e-03 -1.8751e-02\n",
       "  1.0144e-04  1.3982e-02 -7.1225e-03  3.6198e-03 -2.0225e-02\n",
       "  3.2762e-03  6.2192e-03 -6.4182e-03  1.9305e-02 -1.3289e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  2.0434e-02  9.9912e-03 -1.4121e-02  2.2485e-02  1.6849e-02\n",
       " -1.5295e-03  2.3024e-02 -8.5548e-04  2.5132e-02  1.8502e-02\n",
       " -1.6641e-02  8.9460e-03  2.2402e-02 -1.2497e-02  2.1799e-02\n",
       " -4.3779e-03 -1.3508e-02 -3.4915e-03 -1.6341e-02  3.7628e-03\n",
       " -3.0543e-03  2.8612e-03  1.6013e-02  1.9434e-02  4.1950e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.1895e-02  1.3150e-02  1.6681e-02  3.3286e-03  1.2173e-02\n",
       "  1.0443e-02 -1.7757e-02 -1.8114e-02 -1.0106e-02  4.1655e-03\n",
       " -1.6453e-02 -4.7237e-03 -7.1833e-03 -6.5593e-03 -2.6685e-03\n",
       " -1.8904e-02  2.8176e-03 -2.0575e-02 -1.8630e-02  7.1756e-03\n",
       "  1.2020e-02 -2.0156e-02  4.9785e-03  2.1261e-02  1.0514e-02\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -8.6376e-04  3.0061e-03 -1.7313e-02  2.2512e-02 -1.5496e-02\n",
       " -9.8457e-03 -1.1124e-02 -1.8353e-02 -1.4898e-02  3.7646e-03\n",
       "  1.1305e-02  1.2791e-02  2.2407e-02 -2.3119e-03  1.8938e-02\n",
       " -2.2167e-02 -1.8997e-02 -2.2362e-02 -1.4289e-02 -8.3647e-03\n",
       " -1.1326e-02 -2.3428e-02  2.2928e-02 -1.6584e-03  9.6283e-03\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1.0837e-02 -8.4032e-03 -8.4152e-03 -1.6011e-02  1.6192e-02\n",
       " -8.2568e-03  1.1776e-02 -1.3968e-02 -2.2808e-02  8.2840e-03\n",
       "  1.8076e-02  2.0721e-02 -7.8200e-03  1.0179e-02 -2.2619e-02\n",
       " -6.1042e-03 -1.9702e-02 -1.3823e-02 -2.3316e-02 -6.7515e-03\n",
       "  2.3047e-02  1.9652e-02  1.6191e-02 -3.8290e-03  5.9550e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  8.9714e-03 -4.3575e-04  1.0732e-02  5.0587e-03 -1.5067e-02\n",
       " -1.6431e-02 -2.3202e-02 -1.4124e-02 -1.5050e-02  6.9236e-03\n",
       "  6.4380e-03  1.1278e-02  2.5105e-02  1.7465e-02  8.0542e-03\n",
       "  2.5215e-02  1.4459e-02 -1.0167e-02  1.1048e-02 -1.5960e-02\n",
       "  1.4056e-02 -7.5631e-03  4.7301e-03  7.1624e-03  9.8070e-03\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.5609e-02 -5.0260e-03  5.1248e-03 -7.7080e-03 -1.4386e-02\n",
       " -1.9576e-03 -1.4872e-02 -1.7688e-02 -8.2844e-03 -2.0018e-02\n",
       "  2.3991e-02 -2.4003e-02  1.2204e-02  6.4748e-03 -1.3651e-02\n",
       " -1.7971e-02  3.1873e-04  7.9283e-03  1.7656e-02  1.8601e-02\n",
       "  1.6685e-02  9.7989e-03  1.3949e-02  1.1193e-02 -1.2905e-03\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.9387e-02 -1.4122e-03 -1.0318e-02  2.8818e-03  2.1019e-02\n",
       "  9.4458e-03 -8.6057e-04  1.1739e-02  1.1866e-02 -1.2514e-02\n",
       " -3.7262e-03 -1.1232e-02 -1.4673e-02  2.3204e-02  1.8730e-02\n",
       " -1.5420e-02 -1.6523e-02 -2.3954e-02 -6.9532e-03  6.6762e-03\n",
       " -1.0545e-02 -1.5625e-02 -2.1178e-02 -2.0686e-02 -8.3262e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.8041e-02  1.0192e-02  2.0092e-02  1.4693e-02 -1.1757e-02\n",
       "  1.2645e-02 -1.9376e-02  2.1135e-02  2.5638e-02 -1.1559e-02\n",
       "  1.8662e-02  4.9249e-03  1.5992e-02 -1.0858e-02  5.2174e-03\n",
       " -3.8190e-03  8.5228e-03  4.3064e-03  1.4139e-02  1.6611e-02\n",
       "  4.6332e-03  1.9598e-02 -8.5756e-04 -1.7695e-02  1.0517e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.3799e-02 -2.4039e-02  5.5593e-03  1.3921e-02 -1.9278e-02\n",
       " -2.3463e-02  1.6960e-02  8.4596e-03  1.6871e-02 -2.8590e-04\n",
       " -5.2175e-03  2.1009e-02 -4.3725e-03 -2.0503e-02  1.7933e-02\n",
       "  2.3126e-02 -2.1178e-02 -1.6994e-02  2.3269e-02 -1.1542e-02\n",
       "  2.4701e-02  2.4752e-02 -1.0434e-02 -7.4703e-03 -8.2170e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.6613e-02 -2.2886e-02 -1.5885e-02 -6.2669e-03 -1.5019e-02\n",
       "  1.8385e-02  1.1809e-02 -7.8820e-03 -2.0193e-02  1.9612e-02\n",
       "  2.2217e-02 -8.1810e-03  1.6292e-02  2.1053e-02  7.5604e-03\n",
       " -1.3824e-03 -1.1841e-02  1.5223e-03 -9.0697e-03  2.0951e-02\n",
       "  5.3791e-03  1.5134e-02 -1.0324e-02 -1.5756e-02  1.0784e-02\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.4812e-02  1.1365e-02  7.0708e-03  1.5169e-02  2.3029e-03\n",
       "  1.7384e-02  6.0333e-03 -1.2281e-02 -1.5213e-02  1.3971e-02\n",
       "  9.5597e-03  1.6260e-02 -1.2741e-02 -2.2209e-03 -2.6418e-04\n",
       "  7.8888e-03 -9.5033e-03 -1.4275e-02 -1.3385e-02  1.9459e-02\n",
       " -2.1158e-04  1.4506e-02 -1.9797e-02  5.5011e-03  9.0219e-04\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.3096e-02 -5.9467e-03 -1.6514e-02  1.1632e-02 -1.1013e-02\n",
       " -1.9806e-02 -1.9726e-02 -5.9735e-03  1.9682e-03  2.2786e-02\n",
       " -2.6277e-03 -2.0994e-02  9.5077e-03  1.4894e-02  2.5259e-02\n",
       "  8.5419e-03  2.0143e-02  2.3317e-02 -1.9999e-02 -9.3312e-03\n",
       " -1.2521e-03 -1.2848e-02  5.0678e-03  1.9196e-02 -1.3103e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  9.8438e-03 -7.4413e-03  2.3466e-02  1.0664e-02 -2.8734e-03\n",
       "  6.9859e-03 -9.9863e-03  7.4275e-03  1.8623e-02 -7.9618e-03\n",
       " -2.1956e-02  1.8136e-02 -8.1548e-03  1.1823e-03 -1.2700e-02\n",
       " -2.2834e-02 -8.8400e-03  1.8479e-02  1.9105e-02 -2.2790e-02\n",
       "  1.9659e-02 -4.2960e-03  2.3484e-02 -2.4766e-02 -2.5474e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -4.9824e-03 -4.5596e-03  1.1411e-02 -1.2851e-02 -6.2358e-03\n",
       "  2.3359e-04  1.3211e-02  2.2690e-02 -4.5226e-03 -7.7814e-03\n",
       "  1.1322e-02 -2.0863e-02 -8.8313e-04 -4.9782e-03  2.4438e-02\n",
       " -8.4262e-03 -9.6739e-03 -9.4382e-03 -2.0687e-02 -2.1918e-02\n",
       "  8.7239e-03  1.2602e-02 -2.3247e-02  2.4590e-02  1.2570e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.3130e-02 -1.1730e-02  9.4904e-03  3.3437e-03  2.0266e-02\n",
       "  2.5777e-02 -1.6381e-02 -2.1684e-02 -2.9011e-03 -1.2213e-02\n",
       " -2.0652e-02  5.9133e-03 -1.0956e-02  2.0686e-02 -1.1243e-02\n",
       " -8.3474e-03  1.6375e-02  1.1138e-02  8.8956e-03 -1.9677e-02\n",
       "  5.9986e-03 -1.8478e-02  1.3713e-02  1.4692e-02 -1.3158e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -8.0487e-03 -1.0383e-02 -1.7508e-02  5.1739e-03  1.1359e-02\n",
       " -1.9351e-02  2.4760e-02  1.0242e-02 -2.8771e-03 -2.0277e-02\n",
       "  6.2038e-03 -1.5092e-02 -1.8525e-02  6.3524e-04  2.8401e-03\n",
       "  1.9019e-02 -5.6514e-03  5.9684e-04  1.4458e-03 -2.1526e-02\n",
       "  1.0633e-02  2.2571e-02 -1.8400e-02 -2.5662e-03 -1.1753e-03\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.8742e-02 -4.2124e-03  7.0572e-03 -1.2788e-02  1.5176e-02\n",
       "  2.5394e-02  6.3467e-03  2.0304e-02 -2.0105e-02 -6.6959e-03\n",
       " -1.7265e-02  1.8306e-02  4.6403e-03 -1.8812e-02  1.8595e-02\n",
       " -1.8350e-02 -4.8846e-03 -1.6297e-02 -4.4348e-03 -9.5267e-03\n",
       "  2.4573e-02 -1.1860e-02 -1.1273e-02  9.1719e-03 -2.0593e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  2.6202e-03 -3.6839e-03  4.8600e-03  9.7666e-03  1.8810e-02\n",
       "  2.3102e-02  1.0707e-02 -9.9029e-03 -1.6966e-02  1.6515e-02\n",
       " -1.4534e-02  2.3663e-02  4.2590e-03  1.1319e-02  2.1933e-02\n",
       " -2.1198e-03  1.9748e-02 -2.9550e-04  6.2981e-04 -1.1024e-02\n",
       " -7.4889e-03 -5.3269e-03 -1.9380e-02 -1.6034e-02  1.5598e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  3.0331e-03 -1.7037e-02 -1.5772e-02  1.0483e-02 -5.3627e-04\n",
       " -2.2567e-02 -1.6605e-02  1.9917e-02  1.8586e-02  2.3428e-02\n",
       " -1.0008e-02 -1.9490e-02 -2.2409e-02 -2.0533e-02  1.7565e-02\n",
       " -1.0984e-02 -1.1160e-03 -1.3157e-02  1.1969e-02  1.9088e-02\n",
       " -4.9069e-03 -2.3842e-02 -1.0942e-02  1.7978e-02 -5.1324e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  8.2246e-03  6.9526e-03 -6.9166e-03 -1.8090e-02 -1.1295e-02\n",
       " -1.9810e-02 -5.5010e-03  1.1380e-03  1.5770e-03  4.1188e-03\n",
       " -6.0445e-03  1.4983e-02  1.8859e-02 -9.6841e-03  9.4751e-03\n",
       " -1.5141e-02  1.5054e-02  6.3377e-03  7.0526e-03  2.6551e-03\n",
       "  3.7987e-03 -1.7149e-02  1.6598e-02 -2.3244e-02 -1.4149e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  1.5287e-03  7.9543e-03  1.1450e-02  1.3012e-02 -3.4372e-03\n",
       " -1.8834e-02 -9.1617e-03 -7.1692e-03  5.2180e-03 -1.7775e-02\n",
       " -9.8081e-03  1.8798e-02  2.0886e-02  6.2479e-03 -2.3974e-02\n",
       "  9.1216e-03 -1.6312e-02  2.3242e-02  2.3447e-02  1.9169e-02\n",
       " -2.0810e-02 -3.0732e-04 -4.9543e-03  7.5026e-03 -2.3655e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.5051e-02 -1.0335e-02  2.2998e-02  2.2260e-02 -1.7052e-02\n",
       "  1.5021e-02  2.2429e-02  9.9356e-03 -1.1826e-02  1.9894e-02\n",
       "  7.7142e-03 -2.2123e-02 -1.2642e-02 -7.6788e-03 -2.2815e-02\n",
       " -7.0753e-03 -2.2778e-02  5.9357e-03  1.2794e-02  1.3117e-02\n",
       "  7.0572e-03 -2.3037e-02  1.5274e-02  4.4500e-03 -2.1206e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  4.4512e-03  1.6963e-02  2.5723e-02 -1.3546e-02  6.4457e-03\n",
       " -1.4223e-02 -2.3333e-03  5.1215e-03 -2.3841e-02  7.2729e-03\n",
       " -1.4190e-02 -2.0731e-02  2.1510e-02  1.2276e-02  5.9443e-03\n",
       "  1.5512e-02  4.3072e-03  1.8506e-02 -2.1582e-03  1.2703e-03\n",
       " -1.6196e-02 -1.2608e-02 -5.6047e-03  2.4539e-02 -1.4074e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.1795e-02  1.0422e-02  1.0993e-02  2.2456e-02 -4.2983e-03\n",
       " -2.0798e-02 -8.5427e-03  1.2548e-02  2.1261e-02  8.6331e-03\n",
       "  1.0449e-02  6.7424e-03 -1.7902e-02  8.3656e-03  2.5670e-02\n",
       " -1.4413e-02  1.3048e-02 -8.9081e-03 -2.9853e-03  1.6999e-02\n",
       " -6.5839e-03  4.9066e-03  1.7252e-02 -2.0893e-02  1.8474e-03\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1.4847e-03  1.0025e-02 -7.1674e-03 -2.4185e-02 -1.8570e-02\n",
       " -2.0945e-02 -1.9179e-02  1.6823e-02  2.1900e-03  2.1675e-02\n",
       " -4.2502e-03 -1.1380e-02 -1.2043e-02  1.9862e-02  2.1516e-02\n",
       " -3.7076e-03  1.5002e-02  1.7142e-02 -1.5614e-02  5.4898e-03\n",
       " -1.9932e-02  1.6506e-02  3.5464e-03  1.6460e-02  7.2486e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.7563e-02 -1.2354e-03  1.7307e-03  3.2241e-03 -9.1307e-03\n",
       " -1.8913e-02  7.0451e-03  8.9872e-03 -1.2460e-03  7.9187e-03\n",
       "  2.3057e-02 -2.3899e-02  7.5769e-03 -1.4825e-02  2.3900e-02\n",
       "  1.0676e-03  5.4388e-03 -1.9349e-02  6.7580e-03  3.2281e-03\n",
       "  5.0478e-03  2.2738e-02  1.5657e-02 -1.3144e-02  1.4036e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.8393e-02 -1.2916e-02  1.4268e-02 -1.9248e-02  5.1454e-03\n",
       "  2.0311e-02  2.5032e-02 -1.2503e-03 -2.3538e-02 -1.9703e-02\n",
       " -1.5343e-02 -3.6563e-03  4.3335e-03  1.9407e-02 -1.6091e-02\n",
       " -6.0439e-03  9.1570e-03  6.2726e-03 -3.6989e-03 -7.3185e-03\n",
       "  6.3458e-03  1.5584e-02 -1.3631e-02 -4.4525e-03  6.8851e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.weight.grad.data\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "  0  0  0  0  0\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> ('', VGG(\n",
      "  (first_feature): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  )\n",
      "  (last_layer): Sequential(\n",
      "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Linear(in_features=4096, out_features=1000)\n",
      "    (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (1): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (4): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): BinActive(\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=4096)\n",
      "    (1): BinConv(\n",
      "      (ConvLayer): Sequential(\n",
      "        (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): BinActive(\n",
      "        )\n",
      "        (2): Linear(in_features=4096, out_features=4096)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "))\n",
      "1 -> ('first_feature', Sequential(\n",
      "  (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "))\n",
      "2 -> ('first_feature.0', Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)))\n",
      "3 -> ('last_layer', Sequential(\n",
      "  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Linear(in_features=4096, out_features=1000)\n",
      "  (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
      "))\n",
      "4 -> ('last_layer.0', BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True))\n",
      "5 -> ('last_layer.1', BinActive(\n",
      "))\n",
      "6 -> ('last_layer.2', Linear(in_features=4096, out_features=1000))\n",
      "7 -> ('last_layer.3', BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True))\n",
      "8 -> ('features', Sequential(\n",
      "  (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (1): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (3): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (4): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (5): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "  (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (8): BinActive(\n",
      "  )\n",
      "))\n",
      "9 -> ('features.0', MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))\n",
      "10 -> ('features.1', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "))\n",
      "11 -> ('features.1.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "))\n",
      "12 -> ('features.1.ConvLayer.0', BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True))\n",
      "13 -> ('features.1.ConvLayer.1', BinActive(\n",
      "))\n",
      "14 -> ('features.1.ConvLayer.2', Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)))\n",
      "15 -> ('features.2', MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))\n",
      "16 -> ('features.3', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "17 -> ('features.3.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))\n",
      "18 -> ('features.3.ConvLayer.0', BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True))\n",
      "19 -> ('features.3.ConvLayer.1', BinActive(\n",
      "))\n",
      "20 -> ('features.3.ConvLayer.2', Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "21 -> ('features.4', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "22 -> ('features.4.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))\n",
      "23 -> ('features.4.ConvLayer.0', BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True))\n",
      "24 -> ('features.4.ConvLayer.1', BinActive(\n",
      "))\n",
      "25 -> ('features.4.ConvLayer.2', Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "26 -> ('features.5', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "))\n",
      "27 -> ('features.5.ConvLayer', Sequential(\n",
      "  (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "))\n",
      "28 -> ('features.5.ConvLayer.0', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True))\n",
      "29 -> ('features.5.ConvLayer.1', BinActive(\n",
      "))\n",
      "30 -> ('features.5.ConvLayer.2', Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "31 -> ('features.6', MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))\n",
      "32 -> ('features.7', BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True))\n",
      "33 -> ('features.8', BinActive(\n",
      "))\n",
      "34 -> ('classifier', Sequential(\n",
      "  (0): Linear(in_features=9216, out_features=4096)\n",
      "  (1): BinConv(\n",
      "    (ConvLayer): Sequential(\n",
      "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): BinActive(\n",
      "      )\n",
      "      (2): Linear(in_features=4096, out_features=4096)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "35 -> ('classifier.0', Linear(in_features=9216, out_features=4096))\n",
      "36 -> ('classifier.1', BinConv(\n",
      "  (ConvLayer): Sequential(\n",
      "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): BinActive(\n",
      "    )\n",
      "    (2): Linear(in_features=4096, out_features=4096)\n",
      "  )\n",
      "))\n",
      "37 -> ('classifier.1.ConvLayer', Sequential(\n",
      "  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): BinActive(\n",
      "  )\n",
      "  (2): Linear(in_features=4096, out_features=4096)\n",
      "))\n",
      "38 -> ('classifier.1.ConvLayer.0', BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True))\n",
      "39 -> ('classifier.1.ConvLayer.1', BinActive(\n",
      "))\n",
      "40 -> ('classifier.1.ConvLayer.2', Linear(in_features=4096, out_features=4096))\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(model.named_modules()):\n",
    "    print(i, '->', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (first_feature): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "  )\n",
       "  (last_layer): Sequential(\n",
       "    (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): BinActive(\n",
       "    )\n",
       "    (2): Linear(in_features=4096, out_features=1000)\n",
       "    (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (features): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (1): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): BinActive(\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=9216, out_features=4096)\n",
       "    (1): BinConv(\n",
       "      (ConvLayer): Sequential(\n",
       "        (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (1): BinActive(\n",
       "        )\n",
       "        (2): Linear(in_features=4096, out_features=4096)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv0 = model.first_feature[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "binconv0 = model.features[1]\n",
    "conv1 = binconv0.ConvLayer[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  3.2999e-02 -1.0853e-02  1.1804e-02  ...  -4.1577e-02 -4.5362e-02 -3.6687e-02\n",
       " -3.4907e-02  4.2962e-02  1.5821e-02  ...   1.2582e-02 -1.1026e-02  2.7164e-02\n",
       "  3.2744e-02  4.2307e-02 -4.9959e-02  ...   1.2236e-02  3.4432e-02 -2.2982e-02\n",
       "                 ...                                      ...                \n",
       " -2.3553e-02 -1.7643e-02  8.8369e-03  ...   1.0509e-02 -4.5603e-03 -1.2793e-02\n",
       "  3.6100e-02  1.9320e-02 -4.3570e-03  ...   3.2710e-02  4.9394e-02  3.6253e-02\n",
       " -1.5289e-02  1.6344e-02 -1.5270e-02  ...  -8.6423e-03 -4.2889e-02  4.5982e-02\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -2.7279e-02  5.1558e-02 -4.3274e-02  ...   4.1317e-02  1.3197e-02  3.0461e-02\n",
       " -3.9469e-02  1.6336e-02 -3.3295e-02  ...  -1.9971e-02  1.6847e-02  1.2991e-02\n",
       "  3.8012e-02  3.1919e-02  2.9151e-02  ...  -2.0594e-02 -1.5257e-02  1.7365e-02\n",
       "                 ...                                      ...                \n",
       "  2.5971e-02  4.3248e-02  4.6850e-02  ...   2.4054e-02  3.7525e-02 -3.7133e-02\n",
       "  4.7485e-02  1.9903e-02 -4.4855e-02  ...   4.3847e-03  1.7699e-03 -1.4564e-03\n",
       "  7.3245e-03  3.3046e-02 -4.0343e-02  ...   4.1058e-02  2.5732e-02 -3.3914e-02\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  1.8446e-02 -3.7720e-02  2.0100e-02  ...  -4.7902e-02 -3.6259e-02  4.9335e-02\n",
       "  3.6833e-02  2.3231e-02 -2.1703e-02  ...   3.1414e-02 -4.7204e-03 -3.3831e-02\n",
       " -3.9251e-02  1.3924e-02  4.2639e-02  ...  -5.0001e-03  3.1393e-02  1.0323e-05\n",
       "                 ...                                      ...                \n",
       " -4.7761e-02 -1.1185e-02 -2.6725e-03  ...  -2.0391e-02 -3.1019e-02 -9.8514e-03\n",
       " -2.5495e-02 -2.0870e-02  1.2991e-02  ...  -2.6045e-03  4.4398e-02  4.1875e-02\n",
       " -3.5004e-02 -3.8402e-02 -5.2194e-02  ...   1.0940e-02  4.7764e-02  3.2339e-02\n",
       "      \n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -4.8949e-02  1.0801e-02  1.1840e-02  ...  -4.3780e-02 -3.7543e-02 -3.3167e-03\n",
       "  4.9034e-02 -6.3099e-03  4.5036e-02  ...   2.5275e-02  1.6026e-02  1.9810e-02\n",
       " -3.3303e-02 -4.6995e-05 -2.7874e-02  ...   1.9950e-03  3.1941e-02  1.9932e-02\n",
       "                 ...                                      ...                \n",
       " -3.6337e-02  2.6111e-03  2.7535e-02  ...  -2.1565e-02 -1.1471e-02 -4.0516e-02\n",
       " -4.1085e-02 -2.5533e-02  1.4314e-02  ...  -2.3443e-02 -4.4454e-02  2.5374e-02\n",
       "  3.0477e-02  8.5221e-03  2.1774e-02  ...   3.4153e-02 -3.5590e-02  4.0577e-02\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -5.0247e-02 -1.7728e-02 -2.0536e-02  ...  -2.8544e-02 -3.2004e-02  2.7565e-02\n",
       " -2.5172e-02  3.5839e-02  1.0001e-02  ...  -7.5996e-03  3.2656e-02 -3.9260e-02\n",
       " -5.1810e-02  1.0543e-02  1.2813e-03  ...   4.0077e-02  3.5727e-02  3.4856e-03\n",
       "                 ...                                      ...                \n",
       " -3.2160e-03  1.7420e-02  2.6060e-02  ...   4.2485e-02  1.9580e-03 -3.2402e-03\n",
       "  4.5588e-02  2.1972e-02 -2.3518e-02  ...  -2.8124e-02 -1.0550e-02  2.6779e-02\n",
       " -3.3804e-02 -3.5657e-02  4.8387e-02  ...   1.5823e-02  4.7710e-02 -4.0243e-02\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       " -1.8897e-02 -5.0992e-02  1.5223e-02  ...   4.0199e-02  2.4667e-03  3.6036e-02\n",
       " -6.9892e-03  4.5105e-02  3.6083e-02  ...   3.0142e-02  3.7849e-02 -5.1707e-02\n",
       " -3.6184e-02  2.1879e-02 -1.9368e-02  ...   4.7268e-02  8.0504e-03 -4.4737e-02\n",
       "                 ...                                      ...                \n",
       "  1.3322e-03 -1.0451e-02 -3.2699e-02  ...   3.4329e-02  1.0897e-02 -4.8557e-02\n",
       " -4.0427e-02  3.4188e-02 -4.7399e-03  ...   2.6455e-02 -8.9261e-03 -2.9779e-02\n",
       " -4.9021e-02  8.5581e-03 -3.2265e-02  ...   1.6170e-02  1.3189e-02  5.2298e-03\n",
       "      \n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -4.9754e-02  4.5416e-02 -5.0693e-02  ...   3.2644e-02  2.3523e-02 -4.2697e-02\n",
       "  4.4257e-02  7.4371e-04  3.1994e-02  ...  -1.7712e-02 -2.6291e-02 -3.6240e-02\n",
       " -4.9615e-02  3.6499e-03  4.9852e-02  ...   1.9752e-02  7.0548e-03 -2.4841e-02\n",
       "                 ...                                      ...                \n",
       "  2.6958e-02  3.4276e-02  1.7205e-02  ...   1.6361e-02  3.8701e-02  2.1451e-02\n",
       " -2.0986e-02 -1.0001e-02 -1.6346e-02  ...  -4.1696e-02  9.9569e-03 -1.7953e-02\n",
       " -3.4293e-02  1.6747e-02 -4.7268e-02  ...  -2.8127e-02 -2.6314e-02  4.2379e-02\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       " -2.9391e-02  4.3907e-02  3.6541e-05  ...   1.4785e-02 -1.0000e-02 -2.1125e-02\n",
       "  4.8315e-02  3.7067e-02 -2.2366e-03  ...  -1.2091e-02 -2.5682e-02 -2.7795e-02\n",
       "  1.3105e-02  3.1762e-02 -4.2714e-03  ...  -1.9078e-02  1.4803e-02 -1.9222e-02\n",
       "                 ...                                      ...                \n",
       " -9.4321e-03 -3.1413e-02 -4.9547e-02  ...  -1.9316e-02  4.8413e-03  1.7179e-03\n",
       "  8.7469e-03 -2.5569e-03  2.9455e-02  ...   8.6682e-03  2.9681e-02 -5.6467e-03\n",
       " -4.0702e-02 -2.4344e-02 -4.4011e-02  ...   7.3988e-03 -8.9066e-03  2.8827e-02\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       " -4.9800e-02 -5.1054e-02 -3.8221e-02  ...   4.8082e-02  4.1030e-02  5.7901e-03\n",
       " -3.1044e-02  4.8783e-02  3.6585e-02  ...  -4.8259e-02  2.5614e-03  3.1870e-02\n",
       " -1.2801e-02 -4.6420e-02 -3.7321e-02  ...   1.2702e-02  4.3262e-02 -1.0455e-02\n",
       "                 ...                                      ...                \n",
       " -4.1411e-02 -1.4644e-03 -2.8396e-02  ...   3.7390e-04 -2.0314e-02 -9.1728e-03\n",
       "  2.9835e-02 -1.2354e-03  3.5785e-02  ...   2.8200e-02 -1.3448e-02  5.6612e-03\n",
       " -6.0041e-03 -4.3043e-02 -1.7418e-02  ...  -1.3919e-02  4.0268e-02 -5.0351e-02\n",
       "...   \n",
       "      \n",
       "\n",
       "(61,0 ,.,.) = \n",
       "  7.3115e-03  3.8591e-02  2.7361e-02  ...  -4.4841e-02  8.7547e-03 -3.3365e-02\n",
       "  3.4948e-03  4.5212e-02  1.1058e-02  ...   3.9052e-02 -3.0010e-02  3.8651e-02\n",
       "  6.2280e-03 -1.6482e-02 -2.9194e-02  ...   2.8355e-02 -3.7675e-02 -9.6229e-03\n",
       "                 ...                                      ...                \n",
       " -1.4234e-02 -2.6179e-02 -2.8449e-02  ...   9.5990e-03 -4.1067e-02  4.7750e-02\n",
       " -5.1657e-03 -2.1737e-02  4.0944e-02  ...  -4.0484e-02 -1.6657e-02  3.8688e-02\n",
       " -4.2760e-02  4.3737e-02  1.3252e-04  ...  -1.9911e-02  4.9514e-02  6.7544e-04\n",
       "\n",
       "(61,1 ,.,.) = \n",
       " -3.4590e-02 -6.6681e-03  7.8101e-03  ...   2.3470e-02 -3.0213e-02 -2.4038e-02\n",
       " -4.8962e-02 -3.0325e-02  1.2641e-02  ...   1.9539e-02 -2.5474e-02 -3.9432e-02\n",
       "  1.5776e-02  2.0596e-02 -2.6774e-02  ...   5.1481e-02 -4.3664e-02  2.1911e-02\n",
       "                 ...                                      ...                \n",
       " -3.5131e-02  3.0520e-02 -4.3845e-02  ...  -4.1844e-02 -3.4401e-02  3.0631e-02\n",
       " -5.2180e-02  1.5623e-02  3.6122e-02  ...  -6.4967e-03 -3.4376e-02  5.0647e-02\n",
       " -4.5267e-02  4.3073e-02 -3.8866e-02  ...  -3.6736e-02 -5.0881e-02  3.6510e-02\n",
       "\n",
       "(61,2 ,.,.) = \n",
       "  1.4893e-02  3.7595e-02  2.7698e-02  ...  -5.1658e-02  3.3902e-02  2.1046e-02\n",
       "  3.3472e-02  2.8681e-02  4.7969e-02  ...   4.6877e-02  3.5795e-02 -4.3196e-02\n",
       " -4.9246e-02  2.9552e-02  8.5751e-03  ...   1.4409e-02 -2.1694e-02 -4.7043e-02\n",
       "                 ...                                      ...                \n",
       " -2.2710e-02  4.7148e-02 -2.1859e-02  ...  -1.1290e-02 -2.6986e-02 -1.1519e-02\n",
       "  6.0625e-04 -9.0952e-03 -3.1353e-02  ...   8.9367e-03  4.1905e-02  1.3784e-02\n",
       " -3.5075e-02 -1.1479e-02 -1.0306e-02  ...  -1.5345e-02 -3.7110e-02  2.2129e-02\n",
       "      \n",
       "\n",
       "(62,0 ,.,.) = \n",
       "  1.3750e-02 -3.1203e-02  8.9080e-03  ...  -1.5969e-02 -1.6387e-02  5.1317e-02\n",
       "  4.1289e-02 -2.7868e-02 -4.3330e-02  ...   2.7857e-02 -2.5026e-04 -7.7631e-03\n",
       " -2.0977e-02 -2.1228e-02  4.4112e-02  ...   1.8558e-02 -2.5405e-02 -4.6899e-02\n",
       "                 ...                                      ...                \n",
       "  4.2343e-02  1.1087e-02 -4.7878e-02  ...  -2.8540e-02  6.0669e-04  2.7575e-02\n",
       " -4.2476e-03  1.3819e-02 -4.5166e-02  ...  -3.9234e-02 -1.6218e-02  5.0701e-02\n",
       " -4.0623e-02 -4.3302e-02 -3.7265e-03  ...  -3.0813e-02  4.2840e-03  4.4062e-02\n",
       "\n",
       "(62,1 ,.,.) = \n",
       " -3.2055e-02  3.3598e-02 -4.4914e-02  ...  -2.3554e-02  6.3798e-03  7.8456e-03\n",
       " -4.2648e-02 -1.7394e-03 -3.7993e-02  ...  -4.7689e-02 -5.0943e-02 -7.9190e-03\n",
       "  1.2606e-03  3.9080e-02 -3.1434e-03  ...  -4.6597e-02  4.5749e-02  2.4194e-02\n",
       "                 ...                                      ...                \n",
       "  3.8746e-02  5.0368e-02 -1.9522e-02  ...  -2.2404e-02  1.4490e-02  1.4451e-02\n",
       " -4.0561e-02  4.0578e-02  4.1698e-02  ...  -2.1993e-02  4.8109e-02 -1.7657e-02\n",
       "  2.9685e-02 -1.7017e-04 -6.0510e-03  ...   4.5362e-02 -1.7913e-02 -1.0549e-02\n",
       "\n",
       "(62,2 ,.,.) = \n",
       "  3.1349e-02  4.6469e-02 -1.9582e-02  ...  -2.7292e-02  6.9795e-03 -3.3763e-02\n",
       " -3.1817e-02 -2.1572e-03 -1.0463e-02  ...   2.5315e-02  8.2748e-03  2.4131e-02\n",
       "  5.0181e-04 -4.2734e-02 -4.3099e-02  ...  -5.0548e-02  2.1963e-02 -1.1622e-02\n",
       "                 ...                                      ...                \n",
       "  5.4666e-03 -2.1715e-02 -4.8719e-02  ...   4.2015e-02 -1.2923e-02 -3.0908e-02\n",
       "  2.7619e-02 -4.0617e-02  2.4933e-02  ...   2.7097e-02  2.6576e-02  1.0143e-02\n",
       " -4.4403e-02 -1.6402e-02  1.5109e-02  ...   1.5449e-02 -1.1133e-02  2.1465e-02\n",
       "      \n",
       "\n",
       "(63,0 ,.,.) = \n",
       " -2.2425e-02 -3.7557e-02  6.3248e-03  ...  -4.8733e-02  1.9949e-02  6.3706e-03\n",
       "  1.5029e-02 -1.7495e-02  2.2686e-02  ...   4.5170e-02  3.5441e-02  4.6835e-02\n",
       "  3.0772e-02 -4.9918e-03  4.8952e-02  ...   4.7277e-02  3.5557e-02  4.1353e-02\n",
       "                 ...                                      ...                \n",
       "  2.3704e-02  1.0380e-02  2.0053e-02  ...   1.6392e-02  3.0748e-02 -4.9073e-02\n",
       " -2.1951e-02  5.1511e-02 -4.5638e-02  ...   2.4018e-03  4.2857e-02 -1.8558e-02\n",
       " -1.8469e-02 -3.8206e-02  4.0208e-02  ...   4.0313e-02 -4.3609e-02 -8.7752e-03\n",
       "\n",
       "(63,1 ,.,.) = \n",
       "  2.9873e-02  4.2784e-02 -3.0551e-02  ...   2.3626e-02 -3.0017e-02 -7.6230e-03\n",
       "  3.1964e-02 -7.7508e-03  5.0717e-02  ...   4.9635e-03 -2.8670e-02  4.3045e-02\n",
       " -1.6577e-02 -3.9755e-02 -4.9705e-02  ...   1.5441e-02 -2.5371e-02  3.4165e-02\n",
       "                 ...                                      ...                \n",
       "  1.0620e-02  4.7821e-02  4.2127e-02  ...   3.3615e-02 -6.4136e-03  3.6790e-02\n",
       "  3.2490e-02  3.1920e-02 -2.9890e-02  ...   1.5705e-02  2.0314e-02  1.2580e-03\n",
       "  1.0209e-02  3.8687e-02 -3.2131e-02  ...  -2.9449e-02  2.2135e-02  4.2448e-02\n",
       "\n",
       "(63,2 ,.,.) = \n",
       "  5.0314e-02 -7.7857e-03 -4.3619e-02  ...  -6.9891e-03  1.8081e-03 -3.7500e-02\n",
       " -3.5853e-02  4.7128e-02 -2.1555e-02  ...  -2.8939e-02 -4.4286e-02 -2.9030e-02\n",
       " -1.2133e-02 -3.8383e-02 -4.7598e-02  ...   4.9313e-02  3.2528e-02 -3.6192e-02\n",
       "                 ...                                      ...                \n",
       " -2.0070e-02  3.9971e-02  4.3316e-02  ...  -5.7051e-03  1.9613e-03  8.4997e-03\n",
       "  1.1742e-02  2.9378e-02 -2.6888e-03  ...  -5.3498e-03  2.9373e-02  2.5790e-02\n",
       "  2.2103e-03 -2.3938e-02 -2.6683e-02  ...   5.0494e-02  6.9834e-03  2.6730e-02\n",
       "[torch.FloatTensor of size 64x3x11x11]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1 -1 -1\n",
       " -1  1 -1 -1 -1\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1 -1  1  1\n",
       " -1 -1 -1  1  1\n",
       "  1  1 -1  1 -1\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -1 -1  1 -1  1\n",
       " -1 -1  1  1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -1  1  1  1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "  1 -1 -1  1  1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  1 -1 -1  1 -1\n",
       " -1 -1  1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1  1 -1 -1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1 -1 -1  1  1\n",
       "  1  1  1 -1  1\n",
       " -1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1 -1 -1  1\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1  1  1  1  1\n",
       " -1  1  1 -1 -1\n",
       " -1 -1  1 -1 -1\n",
       "  1  1 -1  1 -1\n",
       "  1  1 -1  1 -1\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  1  1 -1  1  1\n",
       " -1  1  1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  1  1  1  1  1\n",
       "  1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1  1 -1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       " -1 -1  1 -1  1\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -1 -1 -1 -1  1\n",
       " -1  1 -1 -1  1\n",
       "  1  1 -1  1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "  1  1  1  1  1\n",
       "  1  1 -1  1 -1\n",
       "  1 -1  1  1  1\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1  1\n",
       "  1  1  1  1 -1\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       "  1  1  1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1  1  1\n",
       " -1  1 -1 -1  1\n",
       "  1 -1 -1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1 -1 -1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "  1 -1  1  1  1\n",
       " -1 -1  1 -1  1\n",
       "  1  1 -1 -1  1\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1  1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       "  1 -1 -1 -1  1\n",
       " -1  1 -1  1  1\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1 -1 -1  1 -1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1 -1  1  1 -1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -1 -1  1 -1 -1\n",
       "  1  1  1 -1 -1\n",
       "  1 -1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1  1 -1  1  1\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1  1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1 -1\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1 -1\n",
       "  1 -1 -1  1  1\n",
       "  1 -1  1  1 -1\n",
       "  1  1 -1 -1 -1\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1  1 -1 -1\n",
       " -1  1  1 -1  1\n",
       " -1 -1 -1 -1 -1\n",
       "  1 -1 -1  1 -1\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  1 -1  1  1  1\n",
       "  1  1 -1 -1  1\n",
       " -1  1  1  1  1\n",
       " -1  1  1  1 -1\n",
       " -1 -1 -1 -1  1\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  1 -1 -1  1  1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1 -1  1\n",
       " -1 -1 -1  1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "  1 -1  1 -1 -1\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1 -1  1 -1\n",
       " -1  1  1  1 -1\n",
       "  1 -1  1  1  1\n",
       " -1  1 -1  1 -1\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1 -1  1  1 -1\n",
       "  1  1  1 -1  1\n",
       "  1 -1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       "  1 -1  1  1 -1\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  1  1  1 -1  1\n",
       " -1 -1  1 -1  1\n",
       " -1 -1  1  1  1\n",
       "  1  1  1 -1  1\n",
       " -1 -1 -1  1 -1\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1  1  1  1 -1\n",
       " -1 -1  1  1  1\n",
       "  1  1 -1  1  1\n",
       " -1  1 -1 -1  1\n",
       " -1  1  1 -1  1\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1  1\n",
       " -1 -1 -1  1  1\n",
       " -1  1  1 -1  1\n",
       " -1  1  1  1  1\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1 -1  1  1 -1\n",
       " -1  1  1 -1  1\n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1  1  1\n",
       "  1  1  1 -1  1\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1 -1  1 -1  1\n",
       "  1  1 -1 -1 -1\n",
       " -1 -1  1  1 -1\n",
       " -1  1  1 -1 -1\n",
       "  1  1 -1 -1  1\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.binarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-3105480b3974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/binop.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateBinaryGradWeight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BNN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateBinaryGradWeight2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/quantization/binop.py\u001b[0m in \u001b[0;36mupdateBinaryGradWeight2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_conv2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;31m#            n = weight[0].nelement()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m#            s = weight.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "quantizer.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0 = model.first_feature(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpoo = model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = maxpoo(out0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = binconv0.ConvLayer[0]\n",
    "binact1 = binconv0.ConvLayer[1]\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = batch1(out1)\n",
    "out3 = binact1(out2)\n",
    "conv1.bias = None\n",
    "out4 = conv1(out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "   -8  -26   -4  ...   -20  -22  -18\n",
       "   -2  -18  -10  ...   -28  -22  -22\n",
       "   -6  -24   -2  ...   -18  -16  -12\n",
       "      ...                 ...      \n",
       "    2  -10    8  ...    -2  -12   -8\n",
       "   -2  -14    6  ...    -6  -14   -2\n",
       "   10   10   18  ...    22    8   10\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "   44   28   24  ...    36   22   16\n",
       "   64   50   40  ...    58   40   36\n",
       "   98   76   62  ...    62   56   42\n",
       "      ...                 ...      \n",
       "   90   66   64  ...    70   48   38\n",
       "   62   36   58  ...    66   60   44\n",
       "   54   46   48  ...    48   46   38\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "    2    4   12  ...     4    4    6\n",
       "   20   12   34  ...    20    8   20\n",
       "   10    2   24  ...    12    2   16\n",
       "      ...                 ...      \n",
       "   22    4   18  ...     8    6   12\n",
       "    4  -22  -16  ...   -24  -12    4\n",
       "  -20  -34  -28  ...   -20  -30    8\n",
       "    ... \n",
       "\n",
       "( 0 ,189,.,.) = \n",
       "   -6  -10   -2  ...    -6    2   -8\n",
       "  -26  -20  -44  ...   -46  -34  -22\n",
       "  -36  -26  -56  ...   -60  -50  -26\n",
       "      ...                 ...      \n",
       "  -40  -28  -42  ...   -60  -38  -10\n",
       "  -70  -62  -88  ...  -104  -70  -32\n",
       "  -28  -14  -26  ...   -42  -36  -14\n",
       "\n",
       "( 0 ,190,.,.) = \n",
       "   32   30    0  ...     0  -20  -10\n",
       "   38   50   30  ...    12  -20  -18\n",
       "   80   78   68  ...    56   -4   -8\n",
       "      ...                 ...      \n",
       "   68   72   62  ...    64   -4   -8\n",
       "   60   78   64  ...    68   22    8\n",
       "   52   72   60  ...    52   10    2\n",
       "\n",
       "( 0 ,191,.,.) = \n",
       "   22   50   40  ...    32   30   16\n",
       "   32   54   22  ...    32   32   22\n",
       "   48   74   40  ...    48   44   32\n",
       "      ...                 ...      \n",
       "   48   72   30  ...    40   32   12\n",
       "   34   62   40  ...    36   52   20\n",
       "   28   52   22  ...    26   24   26\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  -24   12   48  ...   -66  -46  -46\n",
       "   28   36   64  ...   -68  -34  -42\n",
       "   -8   42   22  ...   -90  -22  -10\n",
       "      ...                 ...      \n",
       "   38   22  -18  ...    -8    4  -10\n",
       "   34    2  -34  ...   -24    0   16\n",
       "  -10  -16  -20  ...   -32   10   10\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  -36  -46  -60  ...    -2   14   16\n",
       "   -2  -20  -66  ...     6   16    0\n",
       "  -12  -22  -58  ...    42   30   16\n",
       "      ...                 ...      \n",
       "  -70  -14   34  ...    32   72   64\n",
       "  -34  -24    6  ...   -24  -10   -6\n",
       "  -34  -24  -10  ...    -6    0   26\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "   14    2    8  ...    -6  -28  -50\n",
       "   -2  -18    0  ...    36  -20  -24\n",
       "   -8  -28  -44  ...   -16  -24  -22\n",
       "      ...                 ...      \n",
       "  -10   12   40  ...   -26  -22   -6\n",
       "  -28   14   20  ...    10   58  -38\n",
       "   -8   36   22  ...   -50  -40   -4\n",
       "    ... \n",
       "\n",
       "( 1 ,189,.,.) = \n",
       "   22   16   -6  ...   -24   18    8\n",
       "   20   -2   30  ...   -66    6   -2\n",
       "   66   36   68  ...   -60    4    0\n",
       "      ...                 ...      \n",
       "   28    4    0  ...   -38   18   12\n",
       "   -2  -42   12  ...   -14  -12   10\n",
       "    0  -36    8  ...   -12    6  -22\n",
       "\n",
       "( 1 ,190,.,.) = \n",
       "   12   40   44  ...   -26  -32  -14\n",
       "  -24   -8   -8  ...   -36  -20  -14\n",
       "  -30  -56  -80  ...   -12  -34   -6\n",
       "      ...                 ...      \n",
       "  -36  -16  -52  ...   -10  -56   -2\n",
       "   -8  -42  -32  ...   -22    8   -2\n",
       "  -20  -26  -66  ...    62   24   -2\n",
       "\n",
       "( 1 ,191,.,.) = \n",
       "    2   12   12  ...    42   50    8\n",
       "    2  -20  -32  ...    40    4    6\n",
       "    2   16   -8  ...    60   26  -10\n",
       "      ...                 ...      \n",
       "   12  -24   -8  ...     2   12   26\n",
       "   -6  -90  -28  ...    -2   -6   10\n",
       "    4  -14    4  ...   -24   -2    2\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       "  -28  -24   -8  ...   -24   -8   -8\n",
       "  -24   -4   22  ...   -12  -14   -4\n",
       "   -6  -12   30  ...    40    4   32\n",
       "      ...                 ...      \n",
       "   50   76   54  ...     8   14    0\n",
       "   30   52   44  ...    14   16   10\n",
       "    4   18   16  ...    24   18   10\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  -16  -10   16  ...    52   44   26\n",
       "  -14   28   68  ...    98   80   66\n",
       "  -34   20   70  ...   124   88   74\n",
       "      ...                 ...      \n",
       "  -66  -76  -98  ...   -76  -70  -50\n",
       "  -18  -10  -36  ...   -62  -54  -52\n",
       "  -24  -38  -30  ...   -22    0  -22\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       "   10   26   20  ...   -20   -6   -4\n",
       "    2   10   18  ...   -12   16   42\n",
       "   22    2   44  ...   -26    6    4\n",
       "      ...                 ...      \n",
       "  -10   34   20  ...    34   48   24\n",
       "  -32   16    2  ...    36    6   -4\n",
       "  -30   14   26  ...    -6    0    8\n",
       "    ... \n",
       "\n",
       "( 2 ,189,.,.) = \n",
       "  -10  -12   26  ...    22   20    6\n",
       "  -12  -22  -12  ...   -42  -42  -16\n",
       "   -8  -22   12  ...   -38  -26  -10\n",
       "      ...                 ...      \n",
       "   40   -2   16  ...    26    4   14\n",
       "   78   56   58  ...    48   28   12\n",
       "   46   42   56  ...     8    6    6\n",
       "\n",
       "( 2 ,190,.,.) = \n",
       "   12   16   12  ...   -12  -34  -44\n",
       "   40   44   30  ...   -28  -76  -68\n",
       "   48   58   24  ...    34  -36  -44\n",
       "      ...                 ...      \n",
       "   -4  -10    4  ...    18  -26  -16\n",
       "  -44  -64  -58  ...   -20  -12   -8\n",
       "  -38  -56  -70  ...   -54  -32  -30\n",
       "\n",
       "( 2 ,191,.,.) = \n",
       "   -2   56   16  ...    40   36    2\n",
       "   18   64   58  ...    16    0    8\n",
       "   12   50   80  ...    18   40  -24\n",
       "      ...                 ...      \n",
       "  -12  -46  -28  ...   -26  -34    4\n",
       "   10  -24   22  ...   -12  -30    4\n",
       "   14  -20   28  ...     0  -22   10\n",
       "        \n",
       "\n",
       "( 3 , 0 ,.,.) = \n",
       "   24   52   26  ...    40   34   14\n",
       "   32   64   30  ...    54   40   20\n",
       "    8   32   38  ...    60   44   20\n",
       "      ...                 ...      \n",
       "    4   38   18  ...    46   32   22\n",
       "   18   24   -8  ...    26   16   12\n",
       "    6   20   -2  ...     0   -6  -14\n",
       "\n",
       "( 3 , 1 ,.,.) = \n",
       "  -32  -46  -30  ...   -64  -34  -12\n",
       "  -30  -40  -40  ...   -64  -22  -26\n",
       "  -32  -68  -78  ...   -68  -32  -18\n",
       "      ...                 ...      \n",
       "  -96  -98  -70  ...   -86  -52  -28\n",
       "  -30  -42  -36  ...   -38  -34  -18\n",
       "  -34  -28  -20  ...   -34  -24  -26\n",
       "\n",
       "( 3 , 2 ,.,.) = \n",
       "   -6   22   26  ...    24   36   26\n",
       "   -6   10   42  ...    -2   22    6\n",
       "    4   90   32  ...     6   22    4\n",
       "      ...                 ...      \n",
       "    4   32   16  ...    20   34   18\n",
       "  -16   24   38  ...    40   30   26\n",
       "  -16   32   24  ...    22   36   16\n",
       "    ... \n",
       "\n",
       "( 3 ,189,.,.) = \n",
       "    2  -28    4  ...    -6  -18  -24\n",
       "   20   14   28  ...     8    8    0\n",
       "    6   18  -12  ...    18   14    6\n",
       "      ...                 ...      \n",
       "   58   20   28  ...    24   14    0\n",
       "   70   56   66  ...    72   48   34\n",
       "   32   28   38  ...    20   18   14\n",
       "\n",
       "( 3 ,190,.,.) = \n",
       "    0  -12    6  ...    -4   12   10\n",
       "    8   -4  -22  ...   -10   14   12\n",
       "  -18  -42  -68  ...   -30    0   -8\n",
       "      ...                 ...      \n",
       "  -38  -28  -16  ...   -32    0    2\n",
       "  -32  -60  -34  ...   -60  -20  -10\n",
       "  -40  -46  -52  ...   -70  -36  -26\n",
       "\n",
       "( 3 ,191,.,.) = \n",
       "  -14  -44  -10  ...   -56  -42  -28\n",
       "  -18  -28   18  ...   -58  -46  -28\n",
       "  -22  -34   16  ...   -58  -48  -28\n",
       "      ...                 ...      \n",
       "  -74 -116  -80  ...   -52  -52  -34\n",
       "  -26  -64  -42  ...   -20  -30  -30\n",
       "   -4  -30   -2  ...    -4  -18  -26\n",
       "[torch.FloatTensor of size 4x192x27x27]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 , 0 ,.,.) = \n",
       "  1.9854e-02  1.7430e-03  1.5917e-03 -1.2705e-02  1.4768e-02\n",
       "  8.8550e-03  1.9816e-02 -1.2634e-02 -4.5918e-03 -1.1488e-02\n",
       "  2.3038e-02 -2.1017e-02 -1.0588e-02  3.8625e-03  2.4222e-02\n",
       "  2.4811e-02 -1.9536e-02  2.0398e-02 -2.3971e-04 -1.2513e-02\n",
       " -1.0656e-02  1.3622e-03 -6.4876e-03 -2.2427e-02 -3.1160e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -2.7056e-03 -4.3975e-03 -1.3461e-02 -1.5300e-02 -1.4789e-03\n",
       " -1.5433e-02 -2.3503e-02  9.6584e-03  2.3697e-02 -2.5940e-03\n",
       "  4.8158e-04 -1.5426e-02 -5.9504e-03  2.0913e-02  3.2624e-03\n",
       " -1.6488e-04 -5.5602e-03 -6.5532e-03  1.3503e-02  1.3679e-02\n",
       "  1.2023e-02  1.0051e-02 -1.0832e-03  9.2694e-03 -1.1804e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " -8.8570e-03 -1.2342e-03  6.4216e-03 -2.3477e-02  1.5734e-04\n",
       " -1.3358e-02 -3.8932e-03  1.6753e-02  2.1799e-03 -1.8357e-02\n",
       " -2.2244e-02  3.7937e-03 -2.4062e-02 -8.4986e-03  1.2075e-02\n",
       "  2.3265e-02 -1.9646e-02  2.0102e-02  1.2907e-02 -1.7970e-02\n",
       " -1.4858e-02 -3.2150e-03 -1.8726e-02 -1.7776e-03  2.7310e-03\n",
       "    ... \n",
       "\n",
       "( 0 ,61 ,.,.) = \n",
       " -7.3302e-03  3.0292e-03  6.9196e-03  2.0550e-02 -1.6001e-04\n",
       " -1.1818e-02 -1.6222e-02  3.3308e-03 -2.4760e-02 -1.8461e-02\n",
       "  2.3474e-02 -2.4786e-02 -1.2760e-02  9.6861e-03 -2.2014e-02\n",
       "  8.9485e-03 -5.1274e-03 -1.1390e-02  1.9786e-02  2.2038e-02\n",
       " -2.2137e-02 -1.3512e-02  1.2226e-02 -1.2807e-02 -1.1306e-02\n",
       "\n",
       "( 0 ,62 ,.,.) = \n",
       "  4.8804e-03 -2.0451e-02 -2.0770e-02  6.3498e-03 -9.1271e-03\n",
       " -1.9409e-02 -1.7721e-02  1.4353e-02  8.7453e-03  1.4858e-02\n",
       "  6.0360e-04 -9.0843e-03 -2.6196e-03  1.9454e-02  2.4763e-02\n",
       "  5.0270e-03 -1.5967e-02 -1.9239e-02 -1.1342e-02 -1.8448e-02\n",
       " -4.6556e-03 -1.6809e-02  2.0078e-02 -4.1498e-03 -7.2842e-03\n",
       "\n",
       "( 0 ,63 ,.,.) = \n",
       " -8.6723e-03 -2.0662e-02  2.3798e-02 -8.1032e-03 -1.2880e-02\n",
       " -1.6554e-02  1.4447e-02 -1.3346e-02 -6.4114e-05 -1.8483e-02\n",
       " -2.1542e-04 -2.4597e-02 -6.8266e-03  3.0878e-04  2.1622e-02\n",
       " -7.7929e-03  1.4521e-02 -1.3189e-02  1.9729e-02 -5.2617e-03\n",
       " -2.9792e-03 -1.7042e-02 -2.1484e-02 -2.2779e-02  9.0785e-03\n",
       "        \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  2.0838e-02 -1.7166e-02  2.0419e-02 -1.7027e-02  1.4212e-02\n",
       "  3.7366e-03 -1.4275e-02 -3.4016e-03  1.3498e-02  1.7567e-02\n",
       "  1.7418e-02  1.9651e-02  3.4423e-03 -3.4519e-04  5.8621e-03\n",
       " -1.3422e-02  3.2643e-03  2.2332e-02  2.0855e-02 -4.8584e-03\n",
       " -7.6416e-03 -1.7579e-03  8.1759e-03  2.2670e-02  9.1680e-03\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  2.3112e-02  1.4516e-02 -1.8848e-02 -2.2834e-02 -8.2304e-03\n",
       "  2.4237e-02 -1.9323e-02  3.6882e-03  2.7886e-03 -1.3281e-02\n",
       "  3.2728e-03 -1.6618e-02  6.4820e-03  4.6459e-03  1.5193e-02\n",
       "  1.1980e-02 -1.6462e-02 -1.1259e-02  1.5864e-02  2.0908e-02\n",
       "  5.0138e-03 -7.8098e-03 -2.1828e-02 -1.3298e-02  1.1850e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " -1.7439e-02  1.9328e-02  2.3524e-03  7.2331e-03  2.4050e-02\n",
       " -1.9137e-02  1.4781e-02  1.8644e-02 -3.0069e-03 -2.4654e-02\n",
       " -3.8269e-03 -2.6762e-03  4.2193e-03 -9.5031e-03 -1.9745e-02\n",
       "  1.0290e-03  1.4972e-02 -6.1306e-03  4.6130e-03 -1.9238e-02\n",
       "  4.2447e-03  7.1926e-03 -5.4337e-03  1.8431e-02 -1.2376e-02\n",
       "    ... \n",
       "\n",
       "( 1 ,61 ,.,.) = \n",
       "  1.9709e-02  9.2529e-03 -1.5114e-02  2.1490e-02  1.7834e-02\n",
       " -2.4458e-03  2.4002e-02  1.2939e-04  2.4232e-02  1.9488e-02\n",
       " -1.5682e-02  9.9362e-03  2.1416e-02 -1.3486e-02  2.2604e-02\n",
       " -5.3627e-03 -1.2552e-02 -2.5265e-03 -1.5452e-02  4.7519e-03\n",
       " -4.0226e-03  3.8359e-03  1.7007e-02  2.0425e-02  5.1884e-03\n",
       "\n",
       "( 1 ,62 ,.,.) = \n",
       "  2.2875e-02  1.2164e-02  1.5693e-02  2.3740e-03  1.1231e-02\n",
       "  9.4551e-03 -1.8749e-02 -1.9110e-02 -1.1099e-02  3.1733e-03\n",
       " -1.7446e-02 -5.7096e-03 -8.1767e-03 -5.5684e-03 -1.7105e-03\n",
       " -1.9883e-02  3.8021e-03 -1.9653e-02 -1.7639e-02  6.4315e-03\n",
       "  1.2999e-02 -2.1152e-02  4.0205e-03  2.2254e-02  1.1492e-02\n",
       "\n",
       "( 1 ,63 ,.,.) = \n",
       " -1.8502e-03  2.0180e-03 -1.8306e-02  2.1523e-02 -1.4540e-02\n",
       " -1.0754e-02 -1.2098e-02 -1.9344e-02 -1.5891e-02  2.7898e-03\n",
       "  1.0310e-02  1.1895e-02  2.1419e-02 -3.2787e-03  1.9926e-02\n",
       " -2.3149e-02 -1.9988e-02 -2.1385e-02 -1.3293e-02 -7.3689e-03\n",
       " -1.1967e-02 -2.4420e-02  2.3919e-02 -6.6533e-04  1.0620e-02\n",
       "        \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " -9.9342e-03 -7.5570e-03 -7.4831e-03 -1.5130e-02  1.5311e-02\n",
       " -7.3587e-03  1.0918e-02 -1.3033e-02 -2.1886e-02  7.3727e-03\n",
       "  1.8948e-02  1.9881e-02 -8.3328e-03  1.1044e-02 -2.1743e-02\n",
       " -5.1825e-03 -1.8960e-02 -1.2902e-02 -2.2781e-02 -6.1994e-03\n",
       "  2.2198e-02  1.8943e-02  1.6051e-02 -4.7882e-03  5.0082e-03\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       "  8.1466e-03 -1.3025e-03  9.8606e-03  4.3614e-03 -1.4155e-02\n",
       " -1.5571e-02 -2.4084e-02 -1.5086e-02 -1.5991e-02  6.3563e-03\n",
       "  5.5683e-03  1.0311e-02  2.4140e-02  1.6521e-02  8.2294e-03\n",
       "  2.4304e-02  1.3524e-02 -1.1113e-02  1.1686e-02 -1.6772e-02\n",
       "  1.3817e-02 -8.4687e-03  5.6774e-03  8.1050e-03  1.0674e-02\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " -1.6359e-02 -5.9574e-03  4.6379e-03 -8.6397e-03 -1.5324e-02\n",
       " -2.8514e-03 -1.5732e-02 -1.7158e-02 -9.2285e-03 -2.0912e-02\n",
       "  2.4143e-02 -2.4835e-02  1.2965e-02  5.7063e-03 -1.2693e-02\n",
       " -1.7027e-02  1.2335e-03  8.5668e-03  1.8335e-02  1.9499e-02\n",
       "  1.7550e-02  1.0694e-02  1.4704e-02  1.0481e-02 -3.9471e-04\n",
       "    ... \n",
       "\n",
       "( 2 ,61 ,.,.) = \n",
       " -1.8472e-02 -4.7188e-04 -9.3636e-03  3.8030e-03  2.0256e-02\n",
       "  1.0402e-02  9.4909e-05  1.2701e-02  1.2810e-02 -1.1584e-02\n",
       " -3.1607e-03 -1.0397e-02 -1.3792e-02  2.4134e-02  1.9665e-02\n",
       " -1.4472e-02 -1.5569e-02 -2.3018e-02 -5.9907e-03  7.6282e-03\n",
       " -9.5890e-03 -1.4694e-02 -2.0230e-02 -1.9771e-02 -7.4646e-03\n",
       "\n",
       "( 2 ,62 ,.,.) = \n",
       "  1.7123e-02  9.3277e-03  1.9131e-02  1.3765e-02 -1.1188e-02\n",
       "  1.1731e-02 -2.0179e-02  2.0241e-02  2.4899e-02 -1.0635e-02\n",
       "  1.9443e-02  5.8916e-03  1.6959e-02 -9.9126e-03  6.1570e-03\n",
       " -3.0022e-03  9.4650e-03  5.2554e-03  1.5059e-02  1.7561e-02\n",
       "  5.5280e-03  1.8784e-02  1.4374e-05 -1.6806e-02  1.1462e-02\n",
       "\n",
       "( 2 ,63 ,.,.) = \n",
       "  1.2957e-02 -2.4798e-02  6.4062e-03  1.3216e-02 -1.8366e-02\n",
       " -2.3874e-02  1.6108e-02  9.3074e-03  1.7774e-02  6.6088e-04\n",
       " -4.3282e-03  2.1828e-02 -3.4549e-03 -1.9577e-02  1.8828e-02\n",
       "  2.2225e-02 -2.1485e-02 -1.6169e-02  2.3927e-02 -1.0626e-02\n",
       "  2.3764e-02  2.3794e-02 -1.1182e-02 -6.5404e-03 -7.2618e-03\n",
       "...     \n",
       "        \n",
       "\n",
       "(189, 0 ,.,.) = \n",
       "  1.5635e-02 -2.1905e-02 -1.4917e-02 -5.3635e-03 -1.5996e-02\n",
       "  1.7399e-02  1.0834e-02 -6.9166e-03 -1.9229e-02  1.8639e-02\n",
       "  2.3202e-02 -7.1915e-03  1.7287e-02  2.2046e-02  8.5515e-03\n",
       " -3.9023e-04 -1.0858e-02  2.5152e-03 -8.0841e-03  2.0089e-02\n",
       "  6.3684e-03  1.6113e-02 -9.3315e-03 -1.6732e-02  9.7985e-03\n",
       "\n",
       "(189, 1 ,.,.) = \n",
       " -1.5764e-02  1.2319e-02  6.0850e-03  1.4213e-02  3.2527e-03\n",
       "  1.8271e-02  5.1857e-03 -1.3262e-02 -1.4277e-02  1.3021e-02\n",
       "  8.6343e-03  1.5280e-02 -1.3722e-02 -3.0856e-03 -1.2075e-03\n",
       "  6.9296e-03 -1.0479e-02 -1.5263e-02 -1.4376e-02  1.8475e-02\n",
       " -1.1565e-03  1.5493e-02 -2.0651e-02  6.4736e-03  1.8749e-03\n",
       "\n",
       "(189, 2 ,.,.) = \n",
       " -1.4078e-02 -6.9254e-03 -1.5528e-02  1.0844e-02 -1.1971e-02\n",
       " -2.0339e-02 -1.9003e-02 -5.3223e-03  9.8675e-04  2.3771e-02\n",
       " -1.6623e-03 -2.0006e-02  8.5511e-03  1.3913e-02  2.4279e-02\n",
       "  9.5306e-03  2.1134e-02  2.4300e-02 -2.0977e-02 -1.0278e-02\n",
       " -2.6330e-04 -1.1883e-02  6.0306e-03  1.8253e-02 -1.4015e-02\n",
       "    ... \n",
       "\n",
       "(189,61 ,.,.) = \n",
       "  8.8635e-03 -8.4161e-03  2.2538e-02  9.7375e-03 -1.9545e-03\n",
       "  7.9347e-03 -1.0975e-02  8.3711e-03  1.9597e-02 -6.9698e-03\n",
       " -2.0963e-02  1.8808e-02 -7.1602e-03  2.1720e-03 -1.1723e-02\n",
       " -2.1846e-02 -7.8462e-03  1.9474e-02  2.0094e-02 -2.1806e-02\n",
       "  2.0398e-02 -3.3057e-03  2.4476e-02 -2.3777e-02 -2.4508e-02\n",
       "\n",
       "(189,62 ,.,.) = \n",
       " -4.0223e-03 -5.5134e-03  1.2366e-02 -1.1902e-02 -7.2144e-03\n",
       "  1.2204e-03  1.4203e-02  2.3681e-02 -3.5730e-03 -8.7688e-03\n",
       "  1.2305e-02 -1.9874e-02  6.9832e-05 -5.8470e-03  2.3449e-02\n",
       " -7.4324e-03 -8.7251e-03 -1.0410e-02 -1.9703e-02 -2.0944e-02\n",
       "  9.7120e-03  1.3593e-02 -2.2260e-02  2.3648e-02  1.3540e-02\n",
       "\n",
       "(189,63 ,.,.) = \n",
       "  1.2142e-02 -1.2699e-02  1.0479e-02  2.3704e-03  2.1253e-02\n",
       "  2.4791e-02 -1.5393e-02 -2.0718e-02 -3.8529e-03 -1.1238e-02\n",
       " -1.9666e-02  6.9082e-03 -1.0071e-02  1.9719e-02 -1.0337e-02\n",
       " -7.3649e-03  1.7355e-02  1.0153e-02  9.8394e-03 -2.0604e-02\n",
       "  5.0624e-03 -1.7510e-02  1.4682e-02  1.3721e-02 -1.4146e-02\n",
       "        \n",
       "\n",
       "(190, 0 ,.,.) = \n",
       " -9.0408e-03 -1.1374e-02 -1.6532e-02  4.1837e-03  1.2326e-02\n",
       " -2.0340e-02  2.3781e-02  9.2538e-03 -3.8662e-03 -1.9292e-02\n",
       "  7.1958e-03 -1.4099e-02 -1.7540e-02  1.6272e-03  3.8305e-03\n",
       "  1.9320e-02 -4.6725e-03  1.5192e-03  2.4391e-03 -2.0538e-02\n",
       "  9.6530e-03  2.1617e-02 -1.7407e-02 -1.5714e-03 -1.8874e-04\n",
       "\n",
       "(190, 1 ,.,.) = \n",
       "  1.9720e-02 -3.2369e-03  8.0435e-03 -1.1808e-02  1.4272e-02\n",
       "  2.4401e-02  7.3105e-03  2.1292e-02 -2.1069e-02 -7.6669e-03\n",
       " -1.8252e-02  1.7331e-02  3.6883e-03 -1.9803e-02  1.7602e-02\n",
       " -1.7566e-02 -5.8743e-03 -1.7282e-02 -5.4293e-03 -1.0521e-02\n",
       "  2.3586e-02 -1.2848e-02 -1.2264e-02  8.1812e-03 -2.1584e-02\n",
       "\n",
       "(190, 2 ,.,.) = \n",
       "  1.6321e-03 -4.6769e-03  3.8751e-03  8.7882e-03  1.7828e-02\n",
       "  2.2123e-02  9.7174e-03 -1.0895e-02 -1.7916e-02  1.7501e-02\n",
       " -1.3542e-02  2.4631e-02  5.2292e-03  1.2313e-02  2.2903e-02\n",
       " -1.1280e-03  2.0739e-02  6.9324e-04  1.6232e-03 -1.0036e-02\n",
       " -6.5036e-03 -4.3390e-03 -1.8387e-02 -1.5042e-02  1.6587e-02\n",
       "    ... \n",
       "\n",
       "(190,61 ,.,.) = \n",
       "  4.0268e-03 -1.7984e-02 -1.4797e-02  1.1469e-02  4.4895e-04\n",
       " -2.1843e-02 -1.5622e-02  2.0893e-02  1.9581e-02  2.4423e-02\n",
       " -9.0238e-03 -1.8496e-02 -2.1421e-02 -1.9540e-02  1.8311e-02\n",
       " -9.9901e-03 -1.2800e-04 -1.2173e-02  1.2952e-02  2.0067e-02\n",
       " -3.9160e-03 -2.2863e-02 -9.9492e-03  1.8969e-02 -4.1425e-03\n",
       "\n",
       "(190,62 ,.,.) = \n",
       "  7.2305e-03  5.9649e-03 -7.9054e-03 -1.9049e-02 -1.0308e-02\n",
       " -2.0787e-02 -6.4898e-03  2.0110e-03  5.8777e-04  4.7147e-03\n",
       " -5.0571e-03  1.5965e-02  1.9840e-02 -8.7040e-03  1.0467e-02\n",
       " -1.4221e-02  1.6044e-02  7.3287e-03  8.0455e-03  3.6455e-03\n",
       "  4.7871e-03 -1.8121e-02  1.7589e-02 -2.2254e-02 -1.3177e-02\n",
       "\n",
       "(190,63 ,.,.) = \n",
       "  5.4768e-04  8.9232e-03  1.2413e-02  1.3991e-02 -2.4472e-03\n",
       " -1.7849e-02 -8.2152e-03 -6.1811e-03  4.2707e-03 -1.8766e-02\n",
       " -1.0777e-02  1.9783e-02  2.1870e-02  7.2394e-03 -2.4944e-02\n",
       "  1.0110e-02 -1.5324e-02  2.4233e-02  2.2607e-02  1.8181e-02\n",
       " -1.9821e-02  6.2794e-04 -3.9675e-03  6.5120e-03 -2.4649e-02\n",
       "        \n",
       "\n",
       "(191, 0 ,.,.) = \n",
       " -1.4060e-02 -1.1325e-02  2.2004e-02  2.3182e-02 -1.8038e-02\n",
       "  1.4292e-02  2.3396e-02  1.0923e-02 -1.0830e-02  2.0888e-02\n",
       "  8.7100e-03 -2.3080e-02 -1.1648e-02 -8.6593e-03 -2.3800e-02\n",
       " -6.1057e-03 -2.3775e-02  4.9375e-03  1.3761e-02  1.4105e-02\n",
       "  6.0622e-03 -2.4035e-02  1.4276e-02  3.4533e-03 -2.2199e-02\n",
       "\n",
       "(191, 1 ,.,.) = \n",
       "  3.4678e-03  1.5969e-02  2.4729e-02 -1.2554e-02  7.4369e-03\n",
       " -1.5219e-02 -3.3141e-03  4.1415e-03 -2.2851e-02  8.2671e-03\n",
       " -1.5156e-02 -1.9750e-02  2.0525e-02  1.3250e-02  4.9593e-03\n",
       "  1.4523e-02  5.2613e-03  1.7515e-02 -3.1524e-03  2.8825e-04\n",
       " -1.7185e-02 -1.3600e-02 -4.6367e-03  2.3545e-02 -2.4017e-03\n",
       "\n",
       "(191, 2 ,.,.) = \n",
       "  1.0800e-02  9.4258e-03  9.9980e-03  2.1472e-02 -5.2750e-03\n",
       " -2.1795e-02 -9.5383e-03  1.1552e-02  2.0296e-02  7.6475e-03\n",
       "  9.4512e-03  5.7468e-03 -1.8897e-02  7.3692e-03  2.4674e-02\n",
       " -1.5409e-02  1.2050e-02 -9.8997e-03 -3.9811e-03  1.6003e-02\n",
       " -7.5817e-03  3.9097e-03  1.6256e-02 -2.1891e-02  8.5148e-04\n",
       "    ... \n",
       "\n",
       "(191,61 ,.,.) = \n",
       "  4.9299e-04  9.0351e-03 -6.1744e-03 -2.3208e-02 -1.9184e-02\n",
       " -1.9955e-02 -1.8184e-02  1.7821e-02  1.1954e-03  2.2655e-02\n",
       " -3.3274e-03 -1.0390e-02 -1.1048e-02  1.8866e-02  2.0701e-02\n",
       " -4.6972e-03  1.4192e-02  1.6157e-02 -1.6608e-02  4.4919e-03\n",
       " -2.0864e-02  1.5643e-02  2.5571e-03  1.5464e-02  6.2548e-03\n",
       "\n",
       "(191,62 ,.,.) = \n",
       "  1.6565e-02 -2.2293e-03  7.4521e-04  2.3971e-03 -1.0121e-02\n",
       " -1.9910e-02  6.0482e-03  7.9966e-03 -2.2359e-03  6.9555e-03\n",
       "  2.2061e-02 -2.4897e-02  6.5946e-03 -1.5809e-02  2.2906e-02\n",
       "  7.0468e-05  4.4416e-03 -2.0347e-02  5.7629e-03  2.2310e-03\n",
       "  4.0507e-03  2.1743e-02  1.4660e-02 -1.4139e-02  1.3042e-02\n",
       "\n",
       "(191,63 ,.,.) = \n",
       "  1.7396e-02 -1.3910e-02  1.5241e-02 -2.0242e-02  4.3292e-03\n",
       "  1.9315e-02  2.4034e-02 -2.2437e-03 -2.4523e-02 -2.0698e-02\n",
       " -1.6340e-02 -4.6525e-03  3.3425e-03  1.8519e-02 -1.5147e-02\n",
       " -7.0399e-03  8.1849e-03  5.2765e-03 -4.6968e-03 -8.3125e-03\n",
       "  5.3671e-03  1.6573e-02 -1.4628e-02 -5.4505e-03  5.8893e-03\n",
       "[torch.FloatTensor of size 192x64x5x5]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizer.saved_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_feature.0.weight\n",
      "first_feature.0.bias\n",
      "last_layer.0.weight\n",
      "last_layer.0.bias\n",
      "last_layer.0.running_mean\n",
      "last_layer.0.running_var\n",
      "last_layer.2.weight\n",
      "last_layer.2.bias\n",
      "last_layer.3.weight\n",
      "last_layer.3.bias\n",
      "last_layer.3.running_mean\n",
      "last_layer.3.running_var\n",
      "features.1.ConvLayer.0.weight\n",
      "features.1.ConvLayer.0.bias\n",
      "features.1.ConvLayer.0.running_mean\n",
      "features.1.ConvLayer.0.running_var\n",
      "features.1.ConvLayer.2.weight\n",
      "features.3.ConvLayer.0.weight\n",
      "features.3.ConvLayer.0.bias\n",
      "features.3.ConvLayer.0.running_mean\n",
      "features.3.ConvLayer.0.running_var\n",
      "features.3.ConvLayer.2.weight\n",
      "features.3.ConvLayer.2.bias\n",
      "features.4.ConvLayer.0.weight\n",
      "features.4.ConvLayer.0.bias\n",
      "features.4.ConvLayer.0.running_mean\n",
      "features.4.ConvLayer.0.running_var\n",
      "features.4.ConvLayer.2.weight\n",
      "features.4.ConvLayer.2.bias\n",
      "features.5.ConvLayer.0.weight\n",
      "features.5.ConvLayer.0.bias\n",
      "features.5.ConvLayer.0.running_mean\n",
      "features.5.ConvLayer.0.running_var\n",
      "features.5.ConvLayer.2.weight\n",
      "features.5.ConvLayer.2.bias\n",
      "features.7.weight\n",
      "features.7.bias\n",
      "features.7.running_mean\n",
      "features.7.running_var\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.1.ConvLayer.0.weight\n",
      "classifier.1.ConvLayer.0.bias\n",
      "classifier.1.ConvLayer.0.running_mean\n",
      "classifier.1.ConvLayer.0.running_var\n",
      "classifier.1.ConvLayer.2.weight\n",
      "classifier.1.ConvLayer.2.bias\n"
     ]
    }
   ],
   "source": [
    "for layer in model.state_dict():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2841  0.4114  0.9601  0.6169\n",
       " 0.2486  0.3997  0.9347  0.5635\n",
       " 0.0921  0.6981  0.7937  0.1600\n",
       " 0.7425  0.5263  0.4888  0.2621\n",
       " 0.8920  0.1198  0.2727  0.6142\n",
       "[torch.FloatTensor of size 5x4]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(5,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =t.size()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.5681 -0.5681 -0.5681 -0.5681\n",
       "-0.5366 -0.5366 -0.5366 -0.5366\n",
       "-0.4360 -0.4360 -0.4360 -0.4360\n",
       "-0.5049 -0.5049 -0.5049 -0.5049\n",
       "-0.4747 -0.4747 -0.4747 -0.4747\n",
       "[torch.FloatTensor of size 5x4]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negMean = t.mean(1, keepdim=True).mul(-1).expand_as(t)\n",
    "negMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = t.nelement()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.2725\n",
       " 2.1464\n",
       " 1.7439\n",
       " 2.0197\n",
       " 1.8987\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.norm(1,1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing Lut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tobin = lambda x, count=8: \"\".join(map(lambda y:str((x>>y)&1), range(count-1, -1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1111111111111001'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def from_uint4_to_int16(x):\n",
    "    b = ''\n",
    "    a = tobin(x,4)\n",
    "    for x in range(12):\n",
    "        b = b+ a[0]\n",
    "    b += a\n",
    "    return b\n",
    "\n",
    "from_uint4_to_int16(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twos_comp(binary_string, bits):\n",
    "    val = int(binary_string,2)\n",
    "    \"\"\"compute the 2's complement of int value val\"\"\"\n",
    "    if (val & (1 << (bits - 1))) != 0: # if sign bit is set e.g., 8bit: 128-255\n",
    "        val = val - (1 << bits)        # compute negative value\n",
    "    return val      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,65528,65529,65530,65531,65532,65533,65534,65535,65536,65537,65538,65539,65540,65541,65542,65543,131064,131065,131066,131067,131068,131069,131070,131071,131072,131073,131074,131075,131076,131077,131078,131079,196600,196601,196602,196603,196604,196605,196606,196607,196608,196609,196610,196611,196612,196613,196614,196615,262136,262137,262138,262139,262140,262141,262142,262143,262144,262145,262146,262147,262148,262149,262150,262151,327672,327673,327674,327675,327676,327677,327678,327679,327680,327681,327682,327683,327684,327685,327686,327687,393208,393209,393210,393211,393212,393213,393214,393215,393216,393217,393218,393219,393220,393221,393222,393223,458744,458745,458746,458747,458748,458749,458750,458751,458752,458753,458754,458755,458756,458757,458758,458759,524280,524281,524282,524283,524284,524285,524286,524287,-524288,-524287,-524286,-524285,-524284,-524283,-524282,-524281,-458760,-458759,-458758,-458757,-458756,-458755,-458754,-458753,-458752,-458751,-458750,-458749,-458748,-458747,-458746,-458745,-393224,-393223,-393222,-393221,-393220,-393219,-393218,-393217,-393216,-393215,-393214,-393213,-393212,-393211,-393210,-393209,-327688,-327687,-327686,-327685,-327684,-327683,-327682,-327681,-327680,-327679,-327678,-327677,-327676,-327675,-327674,-327673,-262152,-262151,-262150,-262149,-262148,-262147,-262146,-262145,-262144,-262143,-262142,-262141,-262140,-262139,-262138,-262137,-196616,-196615,-196614,-196613,-196612,-196611,-196610,-196609,-196608,-196607,-196606,-196605,-196604,-196603,-196602,-196601,-131080,-131079,-131078,-131077,-131076,-131075,-131074,-131073,-131072,-131071,-131070,-131069,-131068,-131067,-131066,-131065,-65544,-65543,-65542,-65541,-65540,-65539,-65538,-65537,-65536,-65535,-65534,-65533,-65532,-65531,-65530,-65529,-8,-7,-6,-5,-4,-3,-2,-1,\n"
     ]
    }
   ],
   "source": [
    "c = ''\n",
    "for i in range(256):\n",
    "    n_low = i % 16\n",
    "    n_low = from_uint4_to_int16(n_low)\n",
    "    n_high = int(i / 16)\n",
    "    n_high = from_uint4_to_int16(n_high)\n",
    "    int16_val = twos_comp(n_high+n_low, 32)\n",
    "    c += str(int16_val) + ','\n",
    "    #print(i,n_low, n_high,int16_val)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111111111111000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', '1', '1', '1', '1', '1', '1', '1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ''\n",
    "a = tobin(8,4)\n",
    "for x in range(12):\n",
    "    b = b+ a[0]\n",
    "b += a\n",
    "print(b)\n",
    "[a[0] for x in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1111111111111000'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_string = '1001' # or whatever... no '0b' prefix\n",
    "out = twos_comp(int(binary_string,2), len(binary_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_quant_devel(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d_SAME(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d_SAME(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d_SAME(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d_SAME(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d_SAME(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d_SAME(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d_SAME(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d_SAME(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d_SAME(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d_SAME(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d_SAME(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d_SAME(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d_SAME(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d_SAME(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d_SAME(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=0.001, momentum=0.003, affine=True, track_running_stats=True)\n",
       "      (2): LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "      tensor([6.], requires_grad=True), inplace)\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmdnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdnn.conversion.pytorch.pytorch_graph import PytorchGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = PytorchGraph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_jit_pass_onnx(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch::jit::Graph, arg1: torch._C._onnx.OperatorExportTypes) -> torch::jit::Graph\n\nInvoked with: graph(%0 : Float(1, 3, 224, 224),\n      %1 : Float(32, 3, 3, 3),\n      %2 : Float(32),\n      %3 : Float(32),\n      %4 : Float(32),\n      %5 : Float(32),\n      %6 : Long(),\n      %7 : Float(32, 1, 3, 3),\n      %8 : Float(32),\n      %9 : Float(32),\n      %10 : Float(32),\n      %11 : Float(32),\n      %12 : Long(),\n      %13 : Float(64, 32, 1, 1),\n      %14 : Float(64),\n      %15 : Float(64),\n      %16 : Float(64),\n      %17 : Float(64),\n      %18 : Long(),\n      %19 : Float(64, 1, 3, 3),\n      %20 : Float(64),\n      %21 : Float(64),\n      %22 : Float(64),\n      %23 : Float(64),\n      %24 : Long(),\n      %25 : Float(128, 64, 1, 1),\n      %26 : Float(128),\n      %27 : Float(128),\n      %28 : Float(128),\n      %29 : Float(128),\n      %30 : Long(),\n      %31 : Float(128, 1, 3, 3),\n      %32 : Float(128),\n      %33 : Float(128),\n      %34 : Float(128),\n      %35 : Float(128),\n      %36 : Long(),\n      %37 : Float(128, 128, 1, 1),\n      %38 : Float(128),\n      %39 : Float(128),\n      %40 : Float(128),\n      %41 : Float(128),\n      %42 : Long(),\n      %43 : Float(128, 1, 3, 3),\n      %44 : Float(128),\n      %45 : Float(128),\n      %46 : Float(128),\n      %47 : Float(128),\n      %48 : Long(),\n      %49 : Float(256, 128, 1, 1),\n      %50 : Float(256),\n      %51 : Float(256),\n      %52 : Float(256),\n      %53 : Float(256),\n      %54 : Long(),\n      %55 : Float(256, 1, 3, 3),\n      %56 : Float(256),\n      %57 : Float(256),\n      %58 : Float(256),\n      %59 : Float(256),\n      %60 : Long(),\n      %61 : Float(256, 256, 1, 1),\n      %62 : Float(256),\n      %63 : Float(256),\n      %64 : Float(256),\n      %65 : Float(256),\n      %66 : Long(),\n      %67 : Float(256, 1, 3, 3),\n      %68 : Float(256),\n      %69 : Float(256),\n      %70 : Float(256),\n      %71 : Float(256),\n      %72 : Long(),\n      %73 : Float(512, 256, 1, 1),\n      %74 : Float(512),\n      %75 : Float(512),\n      %76 : Float(512),\n      %77 : Float(512),\n      %78 : Long(),\n      %79 : Float(512, 1, 3, 3),\n      %80 : Float(512),\n      %81 : Float(512),\n      %82 : Float(512),\n      %83 : Float(512),\n      %84 : Long(),\n      %85 : Float(512, 512, 1, 1),\n      %86 : Float(512),\n      %87 : Float(512),\n      %88 : Float(512),\n      %89 : Float(512),\n      %90 : Long(),\n      %91 : Float(512, 1, 3, 3),\n      %92 : Float(512),\n      %93 : Float(512),\n      %94 : Float(512),\n      %95 : Float(512),\n      %96 : Long(),\n      %97 : Float(512, 512, 1, 1),\n      %98 : Float(512),\n      %99 : Float(512),\n      %100 : Float(512),\n      %101 : Float(512),\n      %102 : Long(),\n      %103 : Float(512, 1, 3, 3),\n      %104 : Float(512),\n      %105 : Float(512),\n      %106 : Float(512),\n      %107 : Float(512),\n      %108 : Long(),\n      %109 : Float(512, 512, 1, 1),\n      %110 : Float(512),\n      %111 : Float(512),\n      %112 : Float(512),\n      %113 : Float(512),\n      %114 : Long(),\n      %115 : Float(512, 1, 3, 3),\n      %116 : Float(512),\n      %117 : Float(512),\n      %118 : Float(512),\n      %119 : Float(512),\n      %120 : Long(),\n      %121 : Float(512, 512, 1, 1),\n      %122 : Float(512),\n      %123 : Float(512),\n      %124 : Float(512),\n      %125 : Float(512),\n      %126 : Long(),\n      %127 : Float(512, 1, 3, 3),\n      %128 : Float(512),\n      %129 : Float(512),\n      %130 : Float(512),\n      %131 : Float(512),\n      %132 : Long(),\n      %133 : Float(512, 512, 1, 1),\n      %134 : Float(512),\n      %135 : Float(512),\n      %136 : Float(512),\n      %137 : Float(512),\n      %138 : Long(),\n      %139 : Float(512, 1, 3, 3),\n      %140 : Float(512),\n      %141 : Float(512),\n      %142 : Float(512),\n      %143 : Float(512),\n      %144 : Long(),\n      %145 : Float(1024, 512, 1, 1),\n      %146 : Float(1024),\n      %147 : Float(1024),\n      %148 : Float(1024),\n      %149 : Float(1024),\n      %150 : Long(),\n      %151 : Float(1024, 1, 3, 3),\n      %152 : Float(1024),\n      %153 : Float(1024),\n      %154 : Float(1024),\n      %155 : Float(1024),\n      %156 : Long(),\n      %157 : Float(1024, 1024, 1, 1),\n      %158 : Float(1024),\n      %159 : Float(1024),\n      %160 : Float(1024),\n      %161 : Float(1024),\n      %162 : Long(),\n      %163 : Float(1000, 1024),\n      %164 : Float(1000)):\n  %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real\n  %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real\n  %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real\n  %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n  %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc]\n  %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n  return (%1078)\n, False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ac90457ff46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPytorchGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;31m# nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, aten, export_raw_ir)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexport_raw_ir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_peephole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _jit_pass_onnx(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch::jit::Graph, arg1: torch._C._onnx.OperatorExportTypes) -> torch::jit::Graph\n\nInvoked with: graph(%0 : Float(1, 3, 224, 224),\n      %1 : Float(32, 3, 3, 3),\n      %2 : Float(32),\n      %3 : Float(32),\n      %4 : Float(32),\n      %5 : Float(32),\n      %6 : Long(),\n      %7 : Float(32, 1, 3, 3),\n      %8 : Float(32),\n      %9 : Float(32),\n      %10 : Float(32),\n      %11 : Float(32),\n      %12 : Long(),\n      %13 : Float(64, 32, 1, 1),\n      %14 : Float(64),\n      %15 : Float(64),\n      %16 : Float(64),\n      %17 : Float(64),\n      %18 : Long(),\n      %19 : Float(64, 1, 3, 3),\n      %20 : Float(64),\n      %21 : Float(64),\n      %22 : Float(64),\n      %23 : Float(64),\n      %24 : Long(),\n      %25 : Float(128, 64, 1, 1),\n      %26 : Float(128),\n      %27 : Float(128),\n      %28 : Float(128),\n      %29 : Float(128),\n      %30 : Long(),\n      %31 : Float(128, 1, 3, 3),\n      %32 : Float(128),\n      %33 : Float(128),\n      %34 : Float(128),\n      %35 : Float(128),\n      %36 : Long(),\n      %37 : Float(128, 128, 1, 1),\n      %38 : Float(128),\n      %39 : Float(128),\n      %40 : Float(128),\n      %41 : Float(128),\n      %42 : Long(),\n      %43 : Float(128, 1, 3, 3),\n      %44 : Float(128),\n      %45 : Float(128),\n      %46 : Float(128),\n      %47 : Float(128),\n      %48 : Long(),\n      %49 : Float(256, 128, 1, 1),\n      %50 : Float(256),\n      %51 : Float(256),\n      %52 : Float(256),\n      %53 : Float(256),\n      %54 : Long(),\n      %55 : Float(256, 1, 3, 3),\n      %56 : Float(256),\n      %57 : Float(256),\n      %58 : Float(256),\n      %59 : Float(256),\n      %60 : Long(),\n      %61 : Float(256, 256, 1, 1),\n      %62 : Float(256),\n      %63 : Float(256),\n      %64 : Float(256),\n      %65 : Float(256),\n      %66 : Long(),\n      %67 : Float(256, 1, 3, 3),\n      %68 : Float(256),\n      %69 : Float(256),\n      %70 : Float(256),\n      %71 : Float(256),\n      %72 : Long(),\n      %73 : Float(512, 256, 1, 1),\n      %74 : Float(512),\n      %75 : Float(512),\n      %76 : Float(512),\n      %77 : Float(512),\n      %78 : Long(),\n      %79 : Float(512, 1, 3, 3),\n      %80 : Float(512),\n      %81 : Float(512),\n      %82 : Float(512),\n      %83 : Float(512),\n      %84 : Long(),\n      %85 : Float(512, 512, 1, 1),\n      %86 : Float(512),\n      %87 : Float(512),\n      %88 : Float(512),\n      %89 : Float(512),\n      %90 : Long(),\n      %91 : Float(512, 1, 3, 3),\n      %92 : Float(512),\n      %93 : Float(512),\n      %94 : Float(512),\n      %95 : Float(512),\n      %96 : Long(),\n      %97 : Float(512, 512, 1, 1),\n      %98 : Float(512),\n      %99 : Float(512),\n      %100 : Float(512),\n      %101 : Float(512),\n      %102 : Long(),\n      %103 : Float(512, 1, 3, 3),\n      %104 : Float(512),\n      %105 : Float(512),\n      %106 : Float(512),\n      %107 : Float(512),\n      %108 : Long(),\n      %109 : Float(512, 512, 1, 1),\n      %110 : Float(512),\n      %111 : Float(512),\n      %112 : Float(512),\n      %113 : Float(512),\n      %114 : Long(),\n      %115 : Float(512, 1, 3, 3),\n      %116 : Float(512),\n      %117 : Float(512),\n      %118 : Float(512),\n      %119 : Float(512),\n      %120 : Long(),\n      %121 : Float(512, 512, 1, 1),\n      %122 : Float(512),\n      %123 : Float(512),\n      %124 : Float(512),\n      %125 : Float(512),\n      %126 : Long(),\n      %127 : Float(512, 1, 3, 3),\n      %128 : Float(512),\n      %129 : Float(512),\n      %130 : Float(512),\n      %131 : Float(512),\n      %132 : Long(),\n      %133 : Float(512, 512, 1, 1),\n      %134 : Float(512),\n      %135 : Float(512),\n      %136 : Float(512),\n      %137 : Float(512),\n      %138 : Long(),\n      %139 : Float(512, 1, 3, 3),\n      %140 : Float(512),\n      %141 : Float(512),\n      %142 : Float(512),\n      %143 : Float(512),\n      %144 : Long(),\n      %145 : Float(1024, 512, 1, 1),\n      %146 : Float(1024),\n      %147 : Float(1024),\n      %148 : Float(1024),\n      %149 : Float(1024),\n      %150 : Long(),\n      %151 : Float(1024, 1, 3, 3),\n      %152 : Float(1024),\n      %153 : Float(1024),\n      %154 : Float(1024),\n      %155 : Float(1024),\n      %156 : Long(),\n      %157 : Float(1024, 1024, 1, 1),\n      %158 : Float(1024),\n      %159 : Float(1024),\n      %160 : Float(1024),\n      %161 : Float(1024),\n      %162 : Long(),\n      %163 : Float(1000, 1024),\n      %164 : Float(1000)):\n  %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n  %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n  %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n  %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n  %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n  %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n  %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n  %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n  %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n  %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n  %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n  %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n  %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n  %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n  %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n  %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n  %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n  %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n  %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n  %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n  %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n  %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n  %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n  %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n  %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n  %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n  %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n  %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n  %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n  %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n  %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n  %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n  %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n  %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n  %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n  %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n  %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n  %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n  %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n  %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n  %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n  %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n  %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n  %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n  %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n  %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n  %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n  %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n  %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n  %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n  %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n  %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n  %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n  %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n  %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n  %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n  %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n  %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n  %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n  %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n  %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n  %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n  %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n  %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n  %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n  %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n  %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n  %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n  %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n  %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n  %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n  %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n  %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n  %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n  %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n  %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n  %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n  %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n  %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n  %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n  %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n  %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n  %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real\n  %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real\n  %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real\n  %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n  %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc]\n  %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n  %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n  return (%1078)\n, False"
     ]
    }
   ],
   "source": [
    "g.build([1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.output_layers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "None None None\n",
      "1.0 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:209: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = models.__dict__[model_name]\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobilenet_real(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6()\n",
       "    )\n",
       "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in g.get_nodes():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = models.squeezenet1_0(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "/pytorch/torch/csrc/jit/tracer.h:143: getTracingState: Assertion `var_state == state` failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-15efc96e9f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytorchGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPytorchGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't stop after throw()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mset_training\u001b[0;34m(self, model, mode)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mold_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mmdnn/conversion/pytorch/pytorch_graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPytorchGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mget_trace_graph\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLegacyTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtrace_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_trace_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mout_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/quantized_neural_networks/models/imagenet/mobilenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth_mult\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /pytorch/torch/csrc/jit/tracer.h:143: getTracingState: Assertion `var_state == state` failed."
     ]
    }
   ],
   "source": [
    "g = PytorchGraph(model)\n",
    "g.build([1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(1, 3, 224, 224),\n",
      "      %model.0.0.weight : Float(32, 3, 3, 3),\n",
      "      %model.0.1.weight : Float(32),\n",
      "      %model.0.1.bias : Float(32),\n",
      "      %model.0.1.running_mean : Float(32),\n",
      "      %model.0.1.running_var : Float(32),\n",
      "      %model.0.1.num_batches_tracked : Long(),\n",
      "      %model.1.0.weight : Float(32, 1, 3, 3),\n",
      "      %model.1.1.weight : Float(32),\n",
      "      %model.1.1.bias : Float(32),\n",
      "      %model.1.1.running_mean : Float(32),\n",
      "      %model.1.1.running_var : Float(32),\n",
      "      %model.1.1.num_batches_tracked : Long(),\n",
      "      %model.2.0.weight : Float(64, 32, 1, 1),\n",
      "      %model.2.1.weight : Float(64),\n",
      "      %model.2.1.bias : Float(64),\n",
      "      %model.2.1.running_mean : Float(64),\n",
      "      %model.2.1.running_var : Float(64),\n",
      "      %model.2.1.num_batches_tracked : Long(),\n",
      "      %model.3.0.weight : Float(64, 1, 3, 3),\n",
      "      %model.3.1.weight : Float(64),\n",
      "      %model.3.1.bias : Float(64),\n",
      "      %model.3.1.running_mean : Float(64),\n",
      "      %model.3.1.running_var : Float(64),\n",
      "      %model.3.1.num_batches_tracked : Long(),\n",
      "      %model.4.0.weight : Float(128, 64, 1, 1),\n",
      "      %model.4.1.weight : Float(128),\n",
      "      %model.4.1.bias : Float(128),\n",
      "      %model.4.1.running_mean : Float(128),\n",
      "      %model.4.1.running_var : Float(128),\n",
      "      %model.4.1.num_batches_tracked : Long(),\n",
      "      %model.5.0.weight : Float(128, 1, 3, 3),\n",
      "      %model.5.1.weight : Float(128),\n",
      "      %model.5.1.bias : Float(128),\n",
      "      %model.5.1.running_mean : Float(128),\n",
      "      %model.5.1.running_var : Float(128),\n",
      "      %model.5.1.num_batches_tracked : Long(),\n",
      "      %model.6.0.weight : Float(128, 128, 1, 1),\n",
      "      %model.6.1.weight : Float(128),\n",
      "      %model.6.1.bias : Float(128),\n",
      "      %model.6.1.running_mean : Float(128),\n",
      "      %model.6.1.running_var : Float(128),\n",
      "      %model.6.1.num_batches_tracked : Long(),\n",
      "      %model.7.0.weight : Float(128, 1, 3, 3),\n",
      "      %model.7.1.weight : Float(128),\n",
      "      %model.7.1.bias : Float(128),\n",
      "      %model.7.1.running_mean : Float(128),\n",
      "      %model.7.1.running_var : Float(128),\n",
      "      %model.7.1.num_batches_tracked : Long(),\n",
      "      %model.8.0.weight : Float(256, 128, 1, 1),\n",
      "      %model.8.1.weight : Float(256),\n",
      "      %model.8.1.bias : Float(256),\n",
      "      %model.8.1.running_mean : Float(256),\n",
      "      %model.8.1.running_var : Float(256),\n",
      "      %model.8.1.num_batches_tracked : Long(),\n",
      "      %model.9.0.weight : Float(256, 1, 3, 3),\n",
      "      %model.9.1.weight : Float(256),\n",
      "      %model.9.1.bias : Float(256),\n",
      "      %model.9.1.running_mean : Float(256),\n",
      "      %model.9.1.running_var : Float(256),\n",
      "      %model.9.1.num_batches_tracked : Long(),\n",
      "      %model.10.0.weight : Float(256, 256, 1, 1),\n",
      "      %model.10.1.weight : Float(256),\n",
      "      %model.10.1.bias : Float(256),\n",
      "      %model.10.1.running_mean : Float(256),\n",
      "      %model.10.1.running_var : Float(256),\n",
      "      %model.10.1.num_batches_tracked : Long(),\n",
      "      %model.11.0.weight : Float(256, 1, 3, 3),\n",
      "      %model.11.1.weight : Float(256),\n",
      "      %model.11.1.bias : Float(256),\n",
      "      %model.11.1.running_mean : Float(256),\n",
      "      %model.11.1.running_var : Float(256),\n",
      "      %model.11.1.num_batches_tracked : Long(),\n",
      "      %model.12.0.weight : Float(512, 256, 1, 1),\n",
      "      %model.12.1.weight : Float(512),\n",
      "      %model.12.1.bias : Float(512),\n",
      "      %model.12.1.running_mean : Float(512),\n",
      "      %model.12.1.running_var : Float(512),\n",
      "      %model.12.1.num_batches_tracked : Long(),\n",
      "      %model.13.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.13.1.weight : Float(512),\n",
      "      %model.13.1.bias : Float(512),\n",
      "      %model.13.1.running_mean : Float(512),\n",
      "      %model.13.1.running_var : Float(512),\n",
      "      %model.13.1.num_batches_tracked : Long(),\n",
      "      %model.14.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.14.1.weight : Float(512),\n",
      "      %model.14.1.bias : Float(512),\n",
      "      %model.14.1.running_mean : Float(512),\n",
      "      %model.14.1.running_var : Float(512),\n",
      "      %model.14.1.num_batches_tracked : Long(),\n",
      "      %model.15.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.15.1.weight : Float(512),\n",
      "      %model.15.1.bias : Float(512),\n",
      "      %model.15.1.running_mean : Float(512),\n",
      "      %model.15.1.running_var : Float(512),\n",
      "      %model.15.1.num_batches_tracked : Long(),\n",
      "      %model.16.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.16.1.weight : Float(512),\n",
      "      %model.16.1.bias : Float(512),\n",
      "      %model.16.1.running_mean : Float(512),\n",
      "      %model.16.1.running_var : Float(512),\n",
      "      %model.16.1.num_batches_tracked : Long(),\n",
      "      %model.17.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.17.1.weight : Float(512),\n",
      "      %model.17.1.bias : Float(512),\n",
      "      %model.17.1.running_mean : Float(512),\n",
      "      %model.17.1.running_var : Float(512),\n",
      "      %model.17.1.num_batches_tracked : Long(),\n",
      "      %model.18.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.18.1.weight : Float(512),\n",
      "      %model.18.1.bias : Float(512),\n",
      "      %model.18.1.running_mean : Float(512),\n",
      "      %model.18.1.running_var : Float(512),\n",
      "      %model.18.1.num_batches_tracked : Long(),\n",
      "      %model.19.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.19.1.weight : Float(512),\n",
      "      %model.19.1.bias : Float(512),\n",
      "      %model.19.1.running_mean : Float(512),\n",
      "      %model.19.1.running_var : Float(512),\n",
      "      %model.19.1.num_batches_tracked : Long(),\n",
      "      %model.20.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.20.1.weight : Float(512),\n",
      "      %model.20.1.bias : Float(512),\n",
      "      %model.20.1.running_mean : Float(512),\n",
      "      %model.20.1.running_var : Float(512),\n",
      "      %model.20.1.num_batches_tracked : Long(),\n",
      "      %model.21.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.21.1.weight : Float(512),\n",
      "      %model.21.1.bias : Float(512),\n",
      "      %model.21.1.running_mean : Float(512),\n",
      "      %model.21.1.running_var : Float(512),\n",
      "      %model.21.1.num_batches_tracked : Long(),\n",
      "      %model.22.0.weight : Float(512, 512, 1, 1),\n",
      "      %model.22.1.weight : Float(512),\n",
      "      %model.22.1.bias : Float(512),\n",
      "      %model.22.1.running_mean : Float(512),\n",
      "      %model.22.1.running_var : Float(512),\n",
      "      %model.22.1.num_batches_tracked : Long(),\n",
      "      %model.23.0.weight : Float(512, 1, 3, 3),\n",
      "      %model.23.1.weight : Float(512),\n",
      "      %model.23.1.bias : Float(512),\n",
      "      %model.23.1.running_mean : Float(512),\n",
      "      %model.23.1.running_var : Float(512),\n",
      "      %model.23.1.num_batches_tracked : Long(),\n",
      "      %model.24.0.weight : Float(1024, 512, 1, 1),\n",
      "      %model.24.1.weight : Float(1024),\n",
      "      %model.24.1.bias : Float(1024),\n",
      "      %model.24.1.running_mean : Float(1024),\n",
      "      %model.24.1.running_var : Float(1024),\n",
      "      %model.24.1.num_batches_tracked : Long(),\n",
      "      %model.25.0.weight : Float(1024, 1, 3, 3),\n",
      "      %model.25.1.weight : Float(1024),\n",
      "      %model.25.1.bias : Float(1024),\n",
      "      %model.25.1.running_mean : Float(1024),\n",
      "      %model.25.1.running_var : Float(1024),\n",
      "      %model.25.1.num_batches_tracked : Long(),\n",
      "      %model.26.0.weight : Float(1024, 1024, 1, 1),\n",
      "      %model.26.1.weight : Float(1024),\n",
      "      %model.26.1.bias : Float(1024),\n",
      "      %model.26.1.running_mean : Float(1024),\n",
      "      %model.26.1.running_var : Float(1024),\n",
      "      %model.26.1.num_batches_tracked : Long(),\n",
      "      %fc.weight : Float(1000, 1024),\n",
      "      %fc.bias : Float(1000)):\n",
      "  %165 : Float(1, 32, 112, 112) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%0, %model.0.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "  %166 : Float(1, 32, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%165, %model.0.1.weight, %model.0.1.bias, %model.0.1.running_mean, %model.0.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "  %167 : Float(1, 32, 112, 112) = onnx::Clip[max=6, min=0](%166), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "  %168 : Float(1, 32, 112, 112) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%167, %model.1.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "  %169 : Float(1, 32, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%168, %model.1.1.weight, %model.1.1.bias, %model.1.1.running_mean, %model.1.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "  %170 : Float(1, 32, 112, 112) = onnx::Clip[max=6, min=0](%169), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "  %171 : Float(1, 64, 112, 112) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%170, %model.2.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "  %172 : Float(1, 64, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%171, %model.2.1.weight, %model.2.1.bias, %model.2.1.running_mean, %model.2.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "  %173 : Float(1, 64, 112, 112) = onnx::Clip[max=6, min=0](%172), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "  %174 : Float(1, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%173, %model.3.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "  %175 : Float(1, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%174, %model.3.1.weight, %model.3.1.bias, %model.3.1.running_mean, %model.3.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "  %176 : Float(1, 64, 56, 56) = onnx::Clip[max=6, min=0](%175), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "  %177 : Float(1, 128, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%176, %model.4.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "  %178 : Float(1, 128, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%177, %model.4.1.weight, %model.4.1.bias, %model.4.1.running_mean, %model.4.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "  %179 : Float(1, 128, 56, 56) = onnx::Clip[max=6, min=0](%178), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "  %180 : Float(1, 128, 56, 56) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%179, %model.5.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "  %181 : Float(1, 128, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%180, %model.5.1.weight, %model.5.1.bias, %model.5.1.running_mean, %model.5.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "  %182 : Float(1, 128, 56, 56) = onnx::Clip[max=6, min=0](%181), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "  %183 : Float(1, 128, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%182, %model.6.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "  %184 : Float(1, 128, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%183, %model.6.1.weight, %model.6.1.bias, %model.6.1.running_mean, %model.6.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "  %185 : Float(1, 128, 56, 56) = onnx::Clip[max=6, min=0](%184), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "  %186 : Float(1, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%185, %model.7.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "  %187 : Float(1, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%186, %model.7.1.weight, %model.7.1.bias, %model.7.1.running_mean, %model.7.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "  %188 : Float(1, 128, 28, 28) = onnx::Clip[max=6, min=0](%187), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "  %189 : Float(1, 256, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%188, %model.8.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "  %190 : Float(1, 256, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%189, %model.8.1.weight, %model.8.1.bias, %model.8.1.running_mean, %model.8.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "  %191 : Float(1, 256, 28, 28) = onnx::Clip[max=6, min=0](%190), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "  %192 : Float(1, 256, 28, 28) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%191, %model.9.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "  %193 : Float(1, 256, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%192, %model.9.1.weight, %model.9.1.bias, %model.9.1.running_mean, %model.9.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "  %194 : Float(1, 256, 28, 28) = onnx::Clip[max=6, min=0](%193), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "  %195 : Float(1, 256, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%194, %model.10.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "  %196 : Float(1, 256, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%195, %model.10.1.weight, %model.10.1.bias, %model.10.1.running_mean, %model.10.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "  %197 : Float(1, 256, 28, 28) = onnx::Clip[max=6, min=0](%196), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "  %198 : Float(1, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=256, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%197, %model.11.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "  %199 : Float(1, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%198, %model.11.1.weight, %model.11.1.bias, %model.11.1.running_mean, %model.11.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "  %200 : Float(1, 256, 14, 14) = onnx::Clip[max=6, min=0](%199), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "  %201 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%200, %model.12.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "  %202 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%201, %model.12.1.weight, %model.12.1.bias, %model.12.1.running_mean, %model.12.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "  %203 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%202), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "  %204 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%203, %model.13.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "  %205 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%204, %model.13.1.weight, %model.13.1.bias, %model.13.1.running_mean, %model.13.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "  %206 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%205), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "  %207 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%206, %model.14.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "  %208 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%207, %model.14.1.weight, %model.14.1.bias, %model.14.1.running_mean, %model.14.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "  %209 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%208), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "  %210 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%209, %model.15.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "  %211 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%210, %model.15.1.weight, %model.15.1.bias, %model.15.1.running_mean, %model.15.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "  %212 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%211), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "  %213 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%212, %model.16.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "  %214 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%213, %model.16.1.weight, %model.16.1.bias, %model.16.1.running_mean, %model.16.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "  %215 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%214), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "  %216 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%215, %model.17.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "  %217 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%216, %model.17.1.weight, %model.17.1.bias, %model.17.1.running_mean, %model.17.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "  %218 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%217), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "  %219 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%218, %model.18.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "  %220 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%219, %model.18.1.weight, %model.18.1.bias, %model.18.1.running_mean, %model.18.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "  %221 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%220), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "  %222 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%221, %model.19.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "  %223 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%222, %model.19.1.weight, %model.19.1.bias, %model.19.1.running_mean, %model.19.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "  %224 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%223), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "  %225 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%224, %model.20.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "  %226 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%225, %model.20.1.weight, %model.20.1.bias, %model.20.1.running_mean, %model.20.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "  %227 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%226), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "  %228 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%227, %model.21.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "  %229 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%228, %model.21.1.weight, %model.21.1.bias, %model.21.1.running_mean, %model.21.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "  %230 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%229), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "  %231 : Float(1, 512, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%230, %model.22.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "  %232 : Float(1, 512, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%231, %model.22.1.weight, %model.22.1.bias, %model.22.1.running_mean, %model.22.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "  %233 : Float(1, 512, 14, 14) = onnx::Clip[max=6, min=0](%232), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "  %234 : Float(1, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=512, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%233, %model.23.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "  %235 : Float(1, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%234, %model.23.1.weight, %model.23.1.bias, %model.23.1.running_mean, %model.23.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "  %236 : Float(1, 512, 7, 7) = onnx::Clip[max=6, min=0](%235), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "  %237 : Float(1, 1024, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%236, %model.24.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "  %238 : Float(1, 1024, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%237, %model.24.1.weight, %model.24.1.bias, %model.24.1.running_mean, %model.24.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "  %239 : Float(1, 1024, 7, 7) = onnx::Clip[max=6, min=0](%238), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "  %240 : Float(1, 1024, 7, 7) = onnx::Conv[dilations=[1, 1], group=1024, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%239, %model.25.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "  %241 : Float(1, 1024, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%240, %model.25.1.weight, %model.25.1.bias, %model.25.1.running_mean, %model.25.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "  %242 : Float(1, 1024, 7, 7) = onnx::Clip[max=6, min=0](%241), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "  %243 : Float(1, 1024, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%242, %model.26.0.weight), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "  %244 : Float(1, 1024, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%243, %model.26.1.weight, %model.26.1.bias, %model.26.1.running_mean, %model.26.1.running_var), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "  %245 : Float(1, 1024, 7, 7) = onnx::Clip[max=6, min=0](%244), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "  %246 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%245), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
      "  %247 : Float(1, 1024, 1, 1) = onnx::AveragePool[kernel_shape=[7, 7], pads=[0, 0, 0, 0], strides=[7, 7]](%246), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
      "  %248 : Tensor = onnx::Constant[value=   -1  1024 [ Variable[CPUType]{2} ]](), scope: mobilenet_real\n",
      "  %249 : Float(1, 1024) = onnx::Reshape(%247, %248), scope: mobilenet_real\n",
      "  %output1 : Float(1, 1000) = onnx::Gemm[alpha=1, beta=1, transB=1](%249, %fc.weight, %fc.bias), scope: mobilenet_real/Linear[fc]\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_input = Variable(torch.randn(1, 3, 224, 224))\n",
    "input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "output_names = [ \"output1\" ]\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "modelonn = onnx.load(\"alexnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(modelonn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %0[FLOAT, 1x3x224x224]\n",
      ") initializers (\n",
      "  %model.0.0.weight[FLOAT, 32x3x3x3]\n",
      "  %model.0.1.weight[FLOAT, 32]\n",
      "  %model.0.1.bias[FLOAT, 32]\n",
      "  %model.0.1.running_mean[FLOAT, 32]\n",
      "  %model.0.1.running_var[FLOAT, 32]\n",
      "  %model.0.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.1.0.weight[FLOAT, 32x1x3x3]\n",
      "  %model.1.1.weight[FLOAT, 32]\n",
      "  %model.1.1.bias[FLOAT, 32]\n",
      "  %model.1.1.running_mean[FLOAT, 32]\n",
      "  %model.1.1.running_var[FLOAT, 32]\n",
      "  %model.1.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.2.0.weight[FLOAT, 64x32x1x1]\n",
      "  %model.2.1.weight[FLOAT, 64]\n",
      "  %model.2.1.bias[FLOAT, 64]\n",
      "  %model.2.1.running_mean[FLOAT, 64]\n",
      "  %model.2.1.running_var[FLOAT, 64]\n",
      "  %model.2.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.3.0.weight[FLOAT, 64x1x3x3]\n",
      "  %model.3.1.weight[FLOAT, 64]\n",
      "  %model.3.1.bias[FLOAT, 64]\n",
      "  %model.3.1.running_mean[FLOAT, 64]\n",
      "  %model.3.1.running_var[FLOAT, 64]\n",
      "  %model.3.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.4.0.weight[FLOAT, 128x64x1x1]\n",
      "  %model.4.1.weight[FLOAT, 128]\n",
      "  %model.4.1.bias[FLOAT, 128]\n",
      "  %model.4.1.running_mean[FLOAT, 128]\n",
      "  %model.4.1.running_var[FLOAT, 128]\n",
      "  %model.4.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.5.0.weight[FLOAT, 128x1x3x3]\n",
      "  %model.5.1.weight[FLOAT, 128]\n",
      "  %model.5.1.bias[FLOAT, 128]\n",
      "  %model.5.1.running_mean[FLOAT, 128]\n",
      "  %model.5.1.running_var[FLOAT, 128]\n",
      "  %model.5.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.6.0.weight[FLOAT, 128x128x1x1]\n",
      "  %model.6.1.weight[FLOAT, 128]\n",
      "  %model.6.1.bias[FLOAT, 128]\n",
      "  %model.6.1.running_mean[FLOAT, 128]\n",
      "  %model.6.1.running_var[FLOAT, 128]\n",
      "  %model.6.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.7.0.weight[FLOAT, 128x1x3x3]\n",
      "  %model.7.1.weight[FLOAT, 128]\n",
      "  %model.7.1.bias[FLOAT, 128]\n",
      "  %model.7.1.running_mean[FLOAT, 128]\n",
      "  %model.7.1.running_var[FLOAT, 128]\n",
      "  %model.7.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.8.0.weight[FLOAT, 256x128x1x1]\n",
      "  %model.8.1.weight[FLOAT, 256]\n",
      "  %model.8.1.bias[FLOAT, 256]\n",
      "  %model.8.1.running_mean[FLOAT, 256]\n",
      "  %model.8.1.running_var[FLOAT, 256]\n",
      "  %model.8.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.9.0.weight[FLOAT, 256x1x3x3]\n",
      "  %model.9.1.weight[FLOAT, 256]\n",
      "  %model.9.1.bias[FLOAT, 256]\n",
      "  %model.9.1.running_mean[FLOAT, 256]\n",
      "  %model.9.1.running_var[FLOAT, 256]\n",
      "  %model.9.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.10.0.weight[FLOAT, 256x256x1x1]\n",
      "  %model.10.1.weight[FLOAT, 256]\n",
      "  %model.10.1.bias[FLOAT, 256]\n",
      "  %model.10.1.running_mean[FLOAT, 256]\n",
      "  %model.10.1.running_var[FLOAT, 256]\n",
      "  %model.10.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.11.0.weight[FLOAT, 256x1x3x3]\n",
      "  %model.11.1.weight[FLOAT, 256]\n",
      "  %model.11.1.bias[FLOAT, 256]\n",
      "  %model.11.1.running_mean[FLOAT, 256]\n",
      "  %model.11.1.running_var[FLOAT, 256]\n",
      "  %model.11.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.12.0.weight[FLOAT, 512x256x1x1]\n",
      "  %model.12.1.weight[FLOAT, 512]\n",
      "  %model.12.1.bias[FLOAT, 512]\n",
      "  %model.12.1.running_mean[FLOAT, 512]\n",
      "  %model.12.1.running_var[FLOAT, 512]\n",
      "  %model.12.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.13.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.13.1.weight[FLOAT, 512]\n",
      "  %model.13.1.bias[FLOAT, 512]\n",
      "  %model.13.1.running_mean[FLOAT, 512]\n",
      "  %model.13.1.running_var[FLOAT, 512]\n",
      "  %model.13.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.14.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.14.1.weight[FLOAT, 512]\n",
      "  %model.14.1.bias[FLOAT, 512]\n",
      "  %model.14.1.running_mean[FLOAT, 512]\n",
      "  %model.14.1.running_var[FLOAT, 512]\n",
      "  %model.14.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.15.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.15.1.weight[FLOAT, 512]\n",
      "  %model.15.1.bias[FLOAT, 512]\n",
      "  %model.15.1.running_mean[FLOAT, 512]\n",
      "  %model.15.1.running_var[FLOAT, 512]\n",
      "  %model.15.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.16.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.16.1.weight[FLOAT, 512]\n",
      "  %model.16.1.bias[FLOAT, 512]\n",
      "  %model.16.1.running_mean[FLOAT, 512]\n",
      "  %model.16.1.running_var[FLOAT, 512]\n",
      "  %model.16.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.17.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.17.1.weight[FLOAT, 512]\n",
      "  %model.17.1.bias[FLOAT, 512]\n",
      "  %model.17.1.running_mean[FLOAT, 512]\n",
      "  %model.17.1.running_var[FLOAT, 512]\n",
      "  %model.17.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.18.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.18.1.weight[FLOAT, 512]\n",
      "  %model.18.1.bias[FLOAT, 512]\n",
      "  %model.18.1.running_mean[FLOAT, 512]\n",
      "  %model.18.1.running_var[FLOAT, 512]\n",
      "  %model.18.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.19.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.19.1.weight[FLOAT, 512]\n",
      "  %model.19.1.bias[FLOAT, 512]\n",
      "  %model.19.1.running_mean[FLOAT, 512]\n",
      "  %model.19.1.running_var[FLOAT, 512]\n",
      "  %model.19.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.20.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.20.1.weight[FLOAT, 512]\n",
      "  %model.20.1.bias[FLOAT, 512]\n",
      "  %model.20.1.running_mean[FLOAT, 512]\n",
      "  %model.20.1.running_var[FLOAT, 512]\n",
      "  %model.20.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.21.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.21.1.weight[FLOAT, 512]\n",
      "  %model.21.1.bias[FLOAT, 512]\n",
      "  %model.21.1.running_mean[FLOAT, 512]\n",
      "  %model.21.1.running_var[FLOAT, 512]\n",
      "  %model.21.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.22.0.weight[FLOAT, 512x512x1x1]\n",
      "  %model.22.1.weight[FLOAT, 512]\n",
      "  %model.22.1.bias[FLOAT, 512]\n",
      "  %model.22.1.running_mean[FLOAT, 512]\n",
      "  %model.22.1.running_var[FLOAT, 512]\n",
      "  %model.22.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.23.0.weight[FLOAT, 512x1x3x3]\n",
      "  %model.23.1.weight[FLOAT, 512]\n",
      "  %model.23.1.bias[FLOAT, 512]\n",
      "  %model.23.1.running_mean[FLOAT, 512]\n",
      "  %model.23.1.running_var[FLOAT, 512]\n",
      "  %model.23.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.24.0.weight[FLOAT, 1024x512x1x1]\n",
      "  %model.24.1.weight[FLOAT, 1024]\n",
      "  %model.24.1.bias[FLOAT, 1024]\n",
      "  %model.24.1.running_mean[FLOAT, 1024]\n",
      "  %model.24.1.running_var[FLOAT, 1024]\n",
      "  %model.24.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.25.0.weight[FLOAT, 1024x1x3x3]\n",
      "  %model.25.1.weight[FLOAT, 1024]\n",
      "  %model.25.1.bias[FLOAT, 1024]\n",
      "  %model.25.1.running_mean[FLOAT, 1024]\n",
      "  %model.25.1.running_var[FLOAT, 1024]\n",
      "  %model.25.1.num_batches_tracked[INT64, scalar]\n",
      "  %model.26.0.weight[FLOAT, 1024x1024x1x1]\n",
      "  %model.26.1.weight[FLOAT, 1024]\n",
      "  %model.26.1.bias[FLOAT, 1024]\n",
      "  %model.26.1.running_mean[FLOAT, 1024]\n",
      "  %model.26.1.running_var[FLOAT, 1024]\n",
      "  %model.26.1.num_batches_tracked[INT64, scalar]\n",
      "  %fc.weight[FLOAT, 1000x1024]\n",
      "  %fc.bias[FLOAT, 1000]\n",
      ") {\n",
      "  %165 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%0, %model.0.0.weight)\n",
      "  %166 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%165, %model.0.1.weight, %model.0.1.bias, %model.0.1.running_mean, %model.0.1.running_var)\n",
      "  %167 = Clip[max = 6, min = 0](%166)\n",
      "  %168 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%167, %model.1.0.weight)\n",
      "  %169 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%168, %model.1.1.weight, %model.1.1.bias, %model.1.1.running_mean, %model.1.1.running_var)\n",
      "  %170 = Clip[max = 6, min = 0](%169)\n",
      "  %171 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%170, %model.2.0.weight)\n",
      "  %172 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%171, %model.2.1.weight, %model.2.1.bias, %model.2.1.running_mean, %model.2.1.running_var)\n",
      "  %173 = Clip[max = 6, min = 0](%172)\n",
      "  %174 = Conv[dilations = [1, 1], group = 64, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%173, %model.3.0.weight)\n",
      "  %175 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%174, %model.3.1.weight, %model.3.1.bias, %model.3.1.running_mean, %model.3.1.running_var)\n",
      "  %176 = Clip[max = 6, min = 0](%175)\n",
      "  %177 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%176, %model.4.0.weight)\n",
      "  %178 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%177, %model.4.1.weight, %model.4.1.bias, %model.4.1.running_mean, %model.4.1.running_var)\n",
      "  %179 = Clip[max = 6, min = 0](%178)\n",
      "  %180 = Conv[dilations = [1, 1], group = 128, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%179, %model.5.0.weight)\n",
      "  %181 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%180, %model.5.1.weight, %model.5.1.bias, %model.5.1.running_mean, %model.5.1.running_var)\n",
      "  %182 = Clip[max = 6, min = 0](%181)\n",
      "  %183 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%182, %model.6.0.weight)\n",
      "  %184 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%183, %model.6.1.weight, %model.6.1.bias, %model.6.1.running_mean, %model.6.1.running_var)\n",
      "  %185 = Clip[max = 6, min = 0](%184)\n",
      "  %186 = Conv[dilations = [1, 1], group = 128, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%185, %model.7.0.weight)\n",
      "  %187 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%186, %model.7.1.weight, %model.7.1.bias, %model.7.1.running_mean, %model.7.1.running_var)\n",
      "  %188 = Clip[max = 6, min = 0](%187)\n",
      "  %189 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%188, %model.8.0.weight)\n",
      "  %190 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%189, %model.8.1.weight, %model.8.1.bias, %model.8.1.running_mean, %model.8.1.running_var)\n",
      "  %191 = Clip[max = 6, min = 0](%190)\n",
      "  %192 = Conv[dilations = [1, 1], group = 256, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%191, %model.9.0.weight)\n",
      "  %193 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%192, %model.9.1.weight, %model.9.1.bias, %model.9.1.running_mean, %model.9.1.running_var)\n",
      "  %194 = Clip[max = 6, min = 0](%193)\n",
      "  %195 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%194, %model.10.0.weight)\n",
      "  %196 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%195, %model.10.1.weight, %model.10.1.bias, %model.10.1.running_mean, %model.10.1.running_var)\n",
      "  %197 = Clip[max = 6, min = 0](%196)\n",
      "  %198 = Conv[dilations = [1, 1], group = 256, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%197, %model.11.0.weight)\n",
      "  %199 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%198, %model.11.1.weight, %model.11.1.bias, %model.11.1.running_mean, %model.11.1.running_var)\n",
      "  %200 = Clip[max = 6, min = 0](%199)\n",
      "  %201 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%200, %model.12.0.weight)\n",
      "  %202 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%201, %model.12.1.weight, %model.12.1.bias, %model.12.1.running_mean, %model.12.1.running_var)\n",
      "  %203 = Clip[max = 6, min = 0](%202)\n",
      "  %204 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%203, %model.13.0.weight)\n",
      "  %205 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%204, %model.13.1.weight, %model.13.1.bias, %model.13.1.running_mean, %model.13.1.running_var)\n",
      "  %206 = Clip[max = 6, min = 0](%205)\n",
      "  %207 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%206, %model.14.0.weight)\n",
      "  %208 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%207, %model.14.1.weight, %model.14.1.bias, %model.14.1.running_mean, %model.14.1.running_var)\n",
      "  %209 = Clip[max = 6, min = 0](%208)\n",
      "  %210 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%209, %model.15.0.weight)\n",
      "  %211 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%210, %model.15.1.weight, %model.15.1.bias, %model.15.1.running_mean, %model.15.1.running_var)\n",
      "  %212 = Clip[max = 6, min = 0](%211)\n",
      "  %213 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%212, %model.16.0.weight)\n",
      "  %214 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%213, %model.16.1.weight, %model.16.1.bias, %model.16.1.running_mean, %model.16.1.running_var)\n",
      "  %215 = Clip[max = 6, min = 0](%214)\n",
      "  %216 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%215, %model.17.0.weight)\n",
      "  %217 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%216, %model.17.1.weight, %model.17.1.bias, %model.17.1.running_mean, %model.17.1.running_var)\n",
      "  %218 = Clip[max = 6, min = 0](%217)\n",
      "  %219 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%218, %model.18.0.weight)\n",
      "  %220 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%219, %model.18.1.weight, %model.18.1.bias, %model.18.1.running_mean, %model.18.1.running_var)\n",
      "  %221 = Clip[max = 6, min = 0](%220)\n",
      "  %222 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%221, %model.19.0.weight)\n",
      "  %223 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%222, %model.19.1.weight, %model.19.1.bias, %model.19.1.running_mean, %model.19.1.running_var)\n",
      "  %224 = Clip[max = 6, min = 0](%223)\n",
      "  %225 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%224, %model.20.0.weight)\n",
      "  %226 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%225, %model.20.1.weight, %model.20.1.bias, %model.20.1.running_mean, %model.20.1.running_var)\n",
      "  %227 = Clip[max = 6, min = 0](%226)\n",
      "  %228 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%227, %model.21.0.weight)\n",
      "  %229 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%228, %model.21.1.weight, %model.21.1.bias, %model.21.1.running_mean, %model.21.1.running_var)\n",
      "  %230 = Clip[max = 6, min = 0](%229)\n",
      "  %231 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%230, %model.22.0.weight)\n",
      "  %232 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%231, %model.22.1.weight, %model.22.1.bias, %model.22.1.running_mean, %model.22.1.running_var)\n",
      "  %233 = Clip[max = 6, min = 0](%232)\n",
      "  %234 = Conv[dilations = [1, 1], group = 512, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%233, %model.23.0.weight)\n",
      "  %235 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%234, %model.23.1.weight, %model.23.1.bias, %model.23.1.running_mean, %model.23.1.running_var)\n",
      "  %236 = Clip[max = 6, min = 0](%235)\n",
      "  %237 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%236, %model.24.0.weight)\n",
      "  %238 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%237, %model.24.1.weight, %model.24.1.bias, %model.24.1.running_mean, %model.24.1.running_var)\n",
      "  %239 = Clip[max = 6, min = 0](%238)\n",
      "  %240 = Conv[dilations = [1, 1], group = 1024, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%239, %model.25.0.weight)\n",
      "  %241 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%240, %model.25.1.weight, %model.25.1.bias, %model.25.1.running_mean, %model.25.1.running_var)\n",
      "  %242 = Clip[max = 6, min = 0](%241)\n",
      "  %243 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%242, %model.26.0.weight)\n",
      "  %244 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%243, %model.26.1.weight, %model.26.1.bias, %model.26.1.running_mean, %model.26.1.running_var)\n",
      "  %245 = Clip[max = 6, min = 0](%244)\n",
      "  %246 = Pad[mode = 'constant', pads = [0, 0, 0, 0, 0, 0, 0, 0], value = 0](%245)\n",
      "  %247 = AveragePool[kernel_shape = [7, 7], pads = [0, 0, 0, 0], strides = [7, 7]](%246)\n",
      "  %248 = Constant[value = <Tensor>]()\n",
      "  %249 = Reshape(%247, %248)\n",
      "  %output1 = Gemm[alpha = 1, beta = 1, transB = 1](%249, %fc.weight, %fc.bias)\n",
      "  return %output1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(onnx.helper.printable_graph(modelonn.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trace, output = torch.jit.get_trace_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%0 : Float(1, 3, 224, 224),\n",
       "      %1 : Float(32, 3, 3, 3),\n",
       "      %2 : Float(32),\n",
       "      %3 : Float(32),\n",
       "      %4 : Float(32),\n",
       "      %5 : Float(32),\n",
       "      %6 : Long(),\n",
       "      %7 : Float(32, 1, 3, 3),\n",
       "      %8 : Float(32),\n",
       "      %9 : Float(32),\n",
       "      %10 : Float(32),\n",
       "      %11 : Float(32),\n",
       "      %12 : Long(),\n",
       "      %13 : Float(64, 32, 1, 1),\n",
       "      %14 : Float(64),\n",
       "      %15 : Float(64),\n",
       "      %16 : Float(64),\n",
       "      %17 : Float(64),\n",
       "      %18 : Long(),\n",
       "      %19 : Float(64, 1, 3, 3),\n",
       "      %20 : Float(64),\n",
       "      %21 : Float(64),\n",
       "      %22 : Float(64),\n",
       "      %23 : Float(64),\n",
       "      %24 : Long(),\n",
       "      %25 : Float(128, 64, 1, 1),\n",
       "      %26 : Float(128),\n",
       "      %27 : Float(128),\n",
       "      %28 : Float(128),\n",
       "      %29 : Float(128),\n",
       "      %30 : Long(),\n",
       "      %31 : Float(128, 1, 3, 3),\n",
       "      %32 : Float(128),\n",
       "      %33 : Float(128),\n",
       "      %34 : Float(128),\n",
       "      %35 : Float(128),\n",
       "      %36 : Long(),\n",
       "      %37 : Float(128, 128, 1, 1),\n",
       "      %38 : Float(128),\n",
       "      %39 : Float(128),\n",
       "      %40 : Float(128),\n",
       "      %41 : Float(128),\n",
       "      %42 : Long(),\n",
       "      %43 : Float(128, 1, 3, 3),\n",
       "      %44 : Float(128),\n",
       "      %45 : Float(128),\n",
       "      %46 : Float(128),\n",
       "      %47 : Float(128),\n",
       "      %48 : Long(),\n",
       "      %49 : Float(256, 128, 1, 1),\n",
       "      %50 : Float(256),\n",
       "      %51 : Float(256),\n",
       "      %52 : Float(256),\n",
       "      %53 : Float(256),\n",
       "      %54 : Long(),\n",
       "      %55 : Float(256, 1, 3, 3),\n",
       "      %56 : Float(256),\n",
       "      %57 : Float(256),\n",
       "      %58 : Float(256),\n",
       "      %59 : Float(256),\n",
       "      %60 : Long(),\n",
       "      %61 : Float(256, 256, 1, 1),\n",
       "      %62 : Float(256),\n",
       "      %63 : Float(256),\n",
       "      %64 : Float(256),\n",
       "      %65 : Float(256),\n",
       "      %66 : Long(),\n",
       "      %67 : Float(256, 1, 3, 3),\n",
       "      %68 : Float(256),\n",
       "      %69 : Float(256),\n",
       "      %70 : Float(256),\n",
       "      %71 : Float(256),\n",
       "      %72 : Long(),\n",
       "      %73 : Float(512, 256, 1, 1),\n",
       "      %74 : Float(512),\n",
       "      %75 : Float(512),\n",
       "      %76 : Float(512),\n",
       "      %77 : Float(512),\n",
       "      %78 : Long(),\n",
       "      %79 : Float(512, 1, 3, 3),\n",
       "      %80 : Float(512),\n",
       "      %81 : Float(512),\n",
       "      %82 : Float(512),\n",
       "      %83 : Float(512),\n",
       "      %84 : Long(),\n",
       "      %85 : Float(512, 512, 1, 1),\n",
       "      %86 : Float(512),\n",
       "      %87 : Float(512),\n",
       "      %88 : Float(512),\n",
       "      %89 : Float(512),\n",
       "      %90 : Long(),\n",
       "      %91 : Float(512, 1, 3, 3),\n",
       "      %92 : Float(512),\n",
       "      %93 : Float(512),\n",
       "      %94 : Float(512),\n",
       "      %95 : Float(512),\n",
       "      %96 : Long(),\n",
       "      %97 : Float(512, 512, 1, 1),\n",
       "      %98 : Float(512),\n",
       "      %99 : Float(512),\n",
       "      %100 : Float(512),\n",
       "      %101 : Float(512),\n",
       "      %102 : Long(),\n",
       "      %103 : Float(512, 1, 3, 3),\n",
       "      %104 : Float(512),\n",
       "      %105 : Float(512),\n",
       "      %106 : Float(512),\n",
       "      %107 : Float(512),\n",
       "      %108 : Long(),\n",
       "      %109 : Float(512, 512, 1, 1),\n",
       "      %110 : Float(512),\n",
       "      %111 : Float(512),\n",
       "      %112 : Float(512),\n",
       "      %113 : Float(512),\n",
       "      %114 : Long(),\n",
       "      %115 : Float(512, 1, 3, 3),\n",
       "      %116 : Float(512),\n",
       "      %117 : Float(512),\n",
       "      %118 : Float(512),\n",
       "      %119 : Float(512),\n",
       "      %120 : Long(),\n",
       "      %121 : Float(512, 512, 1, 1),\n",
       "      %122 : Float(512),\n",
       "      %123 : Float(512),\n",
       "      %124 : Float(512),\n",
       "      %125 : Float(512),\n",
       "      %126 : Long(),\n",
       "      %127 : Float(512, 1, 3, 3),\n",
       "      %128 : Float(512),\n",
       "      %129 : Float(512),\n",
       "      %130 : Float(512),\n",
       "      %131 : Float(512),\n",
       "      %132 : Long(),\n",
       "      %133 : Float(512, 512, 1, 1),\n",
       "      %134 : Float(512),\n",
       "      %135 : Float(512),\n",
       "      %136 : Float(512),\n",
       "      %137 : Float(512),\n",
       "      %138 : Long(),\n",
       "      %139 : Float(512, 1, 3, 3),\n",
       "      %140 : Float(512),\n",
       "      %141 : Float(512),\n",
       "      %142 : Float(512),\n",
       "      %143 : Float(512),\n",
       "      %144 : Long(),\n",
       "      %145 : Float(1024, 512, 1, 1),\n",
       "      %146 : Float(1024),\n",
       "      %147 : Float(1024),\n",
       "      %148 : Float(1024),\n",
       "      %149 : Float(1024),\n",
       "      %150 : Long(),\n",
       "      %151 : Float(1024, 1, 3, 3),\n",
       "      %152 : Float(1024),\n",
       "      %153 : Float(1024),\n",
       "      %154 : Float(1024),\n",
       "      %155 : Float(1024),\n",
       "      %156 : Long(),\n",
       "      %157 : Float(1024, 1024, 1, 1),\n",
       "      %158 : Float(1024),\n",
       "      %159 : Float(1024),\n",
       "      %160 : Float(1024),\n",
       "      %161 : Float(1024),\n",
       "      %162 : Long(),\n",
       "      %163 : Float(1000, 1024),\n",
       "      %164 : Float(1000)):\n",
       "  %165 : Float(1, 3, 224, 224) = aten::clone(%0)\n",
       "  %166 : Float(32, 3, 3, 3) = aten::clone(%1)\n",
       "  %167 : Float(32) = aten::clone(%2)\n",
       "  %168 : Float(32) = aten::clone(%3)\n",
       "  %169 : Float(32) = aten::clone(%4)\n",
       "  %170 : Float(32) = aten::clone(%5)\n",
       "  %171 : Long() = aten::clone(%6)\n",
       "  %172 : Float(32, 1, 3, 3) = aten::clone(%7)\n",
       "  %173 : Float(32) = aten::clone(%8)\n",
       "  %174 : Float(32) = aten::clone(%9)\n",
       "  %175 : Float(32) = aten::clone(%10)\n",
       "  %176 : Float(32) = aten::clone(%11)\n",
       "  %177 : Long() = aten::clone(%12)\n",
       "  %178 : Float(64, 32, 1, 1) = aten::clone(%13)\n",
       "  %179 : Float(64) = aten::clone(%14)\n",
       "  %180 : Float(64) = aten::clone(%15)\n",
       "  %181 : Float(64) = aten::clone(%16)\n",
       "  %182 : Float(64) = aten::clone(%17)\n",
       "  %183 : Long() = aten::clone(%18)\n",
       "  %184 : Float(64, 1, 3, 3) = aten::clone(%19)\n",
       "  %185 : Float(64) = aten::clone(%20)\n",
       "  %186 : Float(64) = aten::clone(%21)\n",
       "  %187 : Float(64) = aten::clone(%22)\n",
       "  %188 : Float(64) = aten::clone(%23)\n",
       "  %189 : Long() = aten::clone(%24)\n",
       "  %190 : Float(128, 64, 1, 1) = aten::clone(%25)\n",
       "  %191 : Float(128) = aten::clone(%26)\n",
       "  %192 : Float(128) = aten::clone(%27)\n",
       "  %193 : Float(128) = aten::clone(%28)\n",
       "  %194 : Float(128) = aten::clone(%29)\n",
       "  %195 : Long() = aten::clone(%30)\n",
       "  %196 : Float(128, 1, 3, 3) = aten::clone(%31)\n",
       "  %197 : Float(128) = aten::clone(%32)\n",
       "  %198 : Float(128) = aten::clone(%33)\n",
       "  %199 : Float(128) = aten::clone(%34)\n",
       "  %200 : Float(128) = aten::clone(%35)\n",
       "  %201 : Long() = aten::clone(%36)\n",
       "  %202 : Float(128, 128, 1, 1) = aten::clone(%37)\n",
       "  %203 : Float(128) = aten::clone(%38)\n",
       "  %204 : Float(128) = aten::clone(%39)\n",
       "  %205 : Float(128) = aten::clone(%40)\n",
       "  %206 : Float(128) = aten::clone(%41)\n",
       "  %207 : Long() = aten::clone(%42)\n",
       "  %208 : Float(128, 1, 3, 3) = aten::clone(%43)\n",
       "  %209 : Float(128) = aten::clone(%44)\n",
       "  %210 : Float(128) = aten::clone(%45)\n",
       "  %211 : Float(128) = aten::clone(%46)\n",
       "  %212 : Float(128) = aten::clone(%47)\n",
       "  %213 : Long() = aten::clone(%48)\n",
       "  %214 : Float(256, 128, 1, 1) = aten::clone(%49)\n",
       "  %215 : Float(256) = aten::clone(%50)\n",
       "  %216 : Float(256) = aten::clone(%51)\n",
       "  %217 : Float(256) = aten::clone(%52)\n",
       "  %218 : Float(256) = aten::clone(%53)\n",
       "  %219 : Long() = aten::clone(%54)\n",
       "  %220 : Float(256, 1, 3, 3) = aten::clone(%55)\n",
       "  %221 : Float(256) = aten::clone(%56)\n",
       "  %222 : Float(256) = aten::clone(%57)\n",
       "  %223 : Float(256) = aten::clone(%58)\n",
       "  %224 : Float(256) = aten::clone(%59)\n",
       "  %225 : Long() = aten::clone(%60)\n",
       "  %226 : Float(256, 256, 1, 1) = aten::clone(%61)\n",
       "  %227 : Float(256) = aten::clone(%62)\n",
       "  %228 : Float(256) = aten::clone(%63)\n",
       "  %229 : Float(256) = aten::clone(%64)\n",
       "  %230 : Float(256) = aten::clone(%65)\n",
       "  %231 : Long() = aten::clone(%66)\n",
       "  %232 : Float(256, 1, 3, 3) = aten::clone(%67)\n",
       "  %233 : Float(256) = aten::clone(%68)\n",
       "  %234 : Float(256) = aten::clone(%69)\n",
       "  %235 : Float(256) = aten::clone(%70)\n",
       "  %236 : Float(256) = aten::clone(%71)\n",
       "  %237 : Long() = aten::clone(%72)\n",
       "  %238 : Float(512, 256, 1, 1) = aten::clone(%73)\n",
       "  %239 : Float(512) = aten::clone(%74)\n",
       "  %240 : Float(512) = aten::clone(%75)\n",
       "  %241 : Float(512) = aten::clone(%76)\n",
       "  %242 : Float(512) = aten::clone(%77)\n",
       "  %243 : Long() = aten::clone(%78)\n",
       "  %244 : Float(512, 1, 3, 3) = aten::clone(%79)\n",
       "  %245 : Float(512) = aten::clone(%80)\n",
       "  %246 : Float(512) = aten::clone(%81)\n",
       "  %247 : Float(512) = aten::clone(%82)\n",
       "  %248 : Float(512) = aten::clone(%83)\n",
       "  %249 : Long() = aten::clone(%84)\n",
       "  %250 : Float(512, 512, 1, 1) = aten::clone(%85)\n",
       "  %251 : Float(512) = aten::clone(%86)\n",
       "  %252 : Float(512) = aten::clone(%87)\n",
       "  %253 : Float(512) = aten::clone(%88)\n",
       "  %254 : Float(512) = aten::clone(%89)\n",
       "  %255 : Long() = aten::clone(%90)\n",
       "  %256 : Float(512, 1, 3, 3) = aten::clone(%91)\n",
       "  %257 : Float(512) = aten::clone(%92)\n",
       "  %258 : Float(512) = aten::clone(%93)\n",
       "  %259 : Float(512) = aten::clone(%94)\n",
       "  %260 : Float(512) = aten::clone(%95)\n",
       "  %261 : Long() = aten::clone(%96)\n",
       "  %262 : Float(512, 512, 1, 1) = aten::clone(%97)\n",
       "  %263 : Float(512) = aten::clone(%98)\n",
       "  %264 : Float(512) = aten::clone(%99)\n",
       "  %265 : Float(512) = aten::clone(%100)\n",
       "  %266 : Float(512) = aten::clone(%101)\n",
       "  %267 : Long() = aten::clone(%102)\n",
       "  %268 : Float(512, 1, 3, 3) = aten::clone(%103)\n",
       "  %269 : Float(512) = aten::clone(%104)\n",
       "  %270 : Float(512) = aten::clone(%105)\n",
       "  %271 : Float(512) = aten::clone(%106)\n",
       "  %272 : Float(512) = aten::clone(%107)\n",
       "  %273 : Long() = aten::clone(%108)\n",
       "  %274 : Float(512, 512, 1, 1) = aten::clone(%109)\n",
       "  %275 : Float(512) = aten::clone(%110)\n",
       "  %276 : Float(512) = aten::clone(%111)\n",
       "  %277 : Float(512) = aten::clone(%112)\n",
       "  %278 : Float(512) = aten::clone(%113)\n",
       "  %279 : Long() = aten::clone(%114)\n",
       "  %280 : Float(512, 1, 3, 3) = aten::clone(%115)\n",
       "  %281 : Float(512) = aten::clone(%116)\n",
       "  %282 : Float(512) = aten::clone(%117)\n",
       "  %283 : Float(512) = aten::clone(%118)\n",
       "  %284 : Float(512) = aten::clone(%119)\n",
       "  %285 : Long() = aten::clone(%120)\n",
       "  %286 : Float(512, 512, 1, 1) = aten::clone(%121)\n",
       "  %287 : Float(512) = aten::clone(%122)\n",
       "  %288 : Float(512) = aten::clone(%123)\n",
       "  %289 : Float(512) = aten::clone(%124)\n",
       "  %290 : Float(512) = aten::clone(%125)\n",
       "  %291 : Long() = aten::clone(%126)\n",
       "  %292 : Float(512, 1, 3, 3) = aten::clone(%127)\n",
       "  %293 : Float(512) = aten::clone(%128)\n",
       "  %294 : Float(512) = aten::clone(%129)\n",
       "  %295 : Float(512) = aten::clone(%130)\n",
       "  %296 : Float(512) = aten::clone(%131)\n",
       "  %297 : Long() = aten::clone(%132)\n",
       "  %298 : Float(512, 512, 1, 1) = aten::clone(%133)\n",
       "  %299 : Float(512) = aten::clone(%134)\n",
       "  %300 : Float(512) = aten::clone(%135)\n",
       "  %301 : Float(512) = aten::clone(%136)\n",
       "  %302 : Float(512) = aten::clone(%137)\n",
       "  %303 : Long() = aten::clone(%138)\n",
       "  %304 : Float(512, 1, 3, 3) = aten::clone(%139)\n",
       "  %305 : Float(512) = aten::clone(%140)\n",
       "  %306 : Float(512) = aten::clone(%141)\n",
       "  %307 : Float(512) = aten::clone(%142)\n",
       "  %308 : Float(512) = aten::clone(%143)\n",
       "  %309 : Long() = aten::clone(%144)\n",
       "  %310 : Float(1024, 512, 1, 1) = aten::clone(%145)\n",
       "  %311 : Float(1024) = aten::clone(%146)\n",
       "  %312 : Float(1024) = aten::clone(%147)\n",
       "  %313 : Float(1024) = aten::clone(%148)\n",
       "  %314 : Float(1024) = aten::clone(%149)\n",
       "  %315 : Long() = aten::clone(%150)\n",
       "  %316 : Float(1024, 1, 3, 3) = aten::clone(%151)\n",
       "  %317 : Float(1024) = aten::clone(%152)\n",
       "  %318 : Float(1024) = aten::clone(%153)\n",
       "  %319 : Float(1024) = aten::clone(%154)\n",
       "  %320 : Float(1024) = aten::clone(%155)\n",
       "  %321 : Long() = aten::clone(%156)\n",
       "  %322 : Float(1024, 1024, 1, 1) = aten::clone(%157)\n",
       "  %323 : Float(1024) = aten::clone(%158)\n",
       "  %324 : Float(1024) = aten::clone(%159)\n",
       "  %325 : Float(1024) = aten::clone(%160)\n",
       "  %326 : Float(1024) = aten::clone(%161)\n",
       "  %327 : Long() = aten::clone(%162)\n",
       "  %328 : Float(1000, 1024) = aten::clone(%163)\n",
       "  %329 : Float(1000) = aten::clone(%164)\n",
       "  %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
       "  %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
       "  %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
       "  %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
       "  %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
       "  %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
       "  %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
       "  %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
       "  %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
       "  %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
       "  %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
       "  %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
       "  %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
       "  %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
       "  %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
       "  %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
       "  %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
       "  %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
       "  %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
       "  %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
       "  %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
       "  %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
       "  %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
       "  %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
       "  %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
       "  %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
       "  %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
       "  %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
       "  %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
       "  %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
       "  %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
       "  %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
       "  %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
       "  %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
       "  %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
       "  %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
       "  %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
       "  %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
       "  %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
       "  %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
       "  %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
       "  %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
       "  %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
       "  %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
       "  %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
       "  %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
       "  %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
       "  %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
       "  %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
       "  %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
       "  %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
       "  %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
       "  %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
       "  %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
       "  %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
       "  %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
       "  %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
       "  %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
       "  %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
       "  %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
       "  %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
       "  %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
       "  %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
       "  %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
       "  %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
       "  %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
       "  %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
       "  %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
       "  %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
       "  %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
       "  %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
       "  %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
       "  %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
       "  %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
       "  %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
       "  %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
       "  %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
       "  %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
       "  %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
       "  %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
       "  %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
       "  %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
       "  %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
       "  %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
       "  %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
       "  %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
       "  %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
       "  %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
       "  %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
       "  %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
       "  %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
       "  %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
       "  %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
       "  %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
       "  %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
       "  %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
       "  %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
       "  %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
       "  %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
       "  %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
       "  %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
       "  %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
       "  %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
       "  %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
       "  %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
       "  %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
       "  %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
       "  %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
       "  %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
       "  %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
       "  %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
       "  %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
       "  %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
       "  %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
       "  %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
       "  %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
       "  %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
       "  %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
       "  %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
       "  %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
       "  %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
       "  %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
       "  %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
       "  %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
       "  %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
       "  %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
       "  %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
       "  %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
       "  %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
       "  %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
       "  %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
       "  %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
       "  %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
       "  %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
       "  %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
       "  %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
       "  %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real\n",
       "  %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real\n",
       "  %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real\n",
       "  %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n",
       "  %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc]\n",
       "  %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n",
       "  %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc]\n",
       "  %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n",
       "  return (%1078)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[%165 : Float(1, 3, 224, 224) = aten::clone(%0),\n",
       " %166 : Float(32, 3, 3, 3) = aten::clone(%1),\n",
       " %167 : Float(32) = aten::clone(%2),\n",
       " %168 : Float(32) = aten::clone(%3),\n",
       " %169 : Float(32) = aten::clone(%4),\n",
       " %170 : Float(32) = aten::clone(%5),\n",
       " %171 : Long() = aten::clone(%6),\n",
       " %172 : Float(32, 1, 3, 3) = aten::clone(%7),\n",
       " %173 : Float(32) = aten::clone(%8),\n",
       " %174 : Float(32) = aten::clone(%9),\n",
       " %175 : Float(32) = aten::clone(%10),\n",
       " %176 : Float(32) = aten::clone(%11),\n",
       " %177 : Long() = aten::clone(%12),\n",
       " %178 : Float(64, 32, 1, 1) = aten::clone(%13),\n",
       " %179 : Float(64) = aten::clone(%14),\n",
       " %180 : Float(64) = aten::clone(%15),\n",
       " %181 : Float(64) = aten::clone(%16),\n",
       " %182 : Float(64) = aten::clone(%17),\n",
       " %183 : Long() = aten::clone(%18),\n",
       " %184 : Float(64, 1, 3, 3) = aten::clone(%19),\n",
       " %185 : Float(64) = aten::clone(%20),\n",
       " %186 : Float(64) = aten::clone(%21),\n",
       " %187 : Float(64) = aten::clone(%22),\n",
       " %188 : Float(64) = aten::clone(%23),\n",
       " %189 : Long() = aten::clone(%24),\n",
       " %190 : Float(128, 64, 1, 1) = aten::clone(%25),\n",
       " %191 : Float(128) = aten::clone(%26),\n",
       " %192 : Float(128) = aten::clone(%27),\n",
       " %193 : Float(128) = aten::clone(%28),\n",
       " %194 : Float(128) = aten::clone(%29),\n",
       " %195 : Long() = aten::clone(%30),\n",
       " %196 : Float(128, 1, 3, 3) = aten::clone(%31),\n",
       " %197 : Float(128) = aten::clone(%32),\n",
       " %198 : Float(128) = aten::clone(%33),\n",
       " %199 : Float(128) = aten::clone(%34),\n",
       " %200 : Float(128) = aten::clone(%35),\n",
       " %201 : Long() = aten::clone(%36),\n",
       " %202 : Float(128, 128, 1, 1) = aten::clone(%37),\n",
       " %203 : Float(128) = aten::clone(%38),\n",
       " %204 : Float(128) = aten::clone(%39),\n",
       " %205 : Float(128) = aten::clone(%40),\n",
       " %206 : Float(128) = aten::clone(%41),\n",
       " %207 : Long() = aten::clone(%42),\n",
       " %208 : Float(128, 1, 3, 3) = aten::clone(%43),\n",
       " %209 : Float(128) = aten::clone(%44),\n",
       " %210 : Float(128) = aten::clone(%45),\n",
       " %211 : Float(128) = aten::clone(%46),\n",
       " %212 : Float(128) = aten::clone(%47),\n",
       " %213 : Long() = aten::clone(%48),\n",
       " %214 : Float(256, 128, 1, 1) = aten::clone(%49),\n",
       " %215 : Float(256) = aten::clone(%50),\n",
       " %216 : Float(256) = aten::clone(%51),\n",
       " %217 : Float(256) = aten::clone(%52),\n",
       " %218 : Float(256) = aten::clone(%53),\n",
       " %219 : Long() = aten::clone(%54),\n",
       " %220 : Float(256, 1, 3, 3) = aten::clone(%55),\n",
       " %221 : Float(256) = aten::clone(%56),\n",
       " %222 : Float(256) = aten::clone(%57),\n",
       " %223 : Float(256) = aten::clone(%58),\n",
       " %224 : Float(256) = aten::clone(%59),\n",
       " %225 : Long() = aten::clone(%60),\n",
       " %226 : Float(256, 256, 1, 1) = aten::clone(%61),\n",
       " %227 : Float(256) = aten::clone(%62),\n",
       " %228 : Float(256) = aten::clone(%63),\n",
       " %229 : Float(256) = aten::clone(%64),\n",
       " %230 : Float(256) = aten::clone(%65),\n",
       " %231 : Long() = aten::clone(%66),\n",
       " %232 : Float(256, 1, 3, 3) = aten::clone(%67),\n",
       " %233 : Float(256) = aten::clone(%68),\n",
       " %234 : Float(256) = aten::clone(%69),\n",
       " %235 : Float(256) = aten::clone(%70),\n",
       " %236 : Float(256) = aten::clone(%71),\n",
       " %237 : Long() = aten::clone(%72),\n",
       " %238 : Float(512, 256, 1, 1) = aten::clone(%73),\n",
       " %239 : Float(512) = aten::clone(%74),\n",
       " %240 : Float(512) = aten::clone(%75),\n",
       " %241 : Float(512) = aten::clone(%76),\n",
       " %242 : Float(512) = aten::clone(%77),\n",
       " %243 : Long() = aten::clone(%78),\n",
       " %244 : Float(512, 1, 3, 3) = aten::clone(%79),\n",
       " %245 : Float(512) = aten::clone(%80),\n",
       " %246 : Float(512) = aten::clone(%81),\n",
       " %247 : Float(512) = aten::clone(%82),\n",
       " %248 : Float(512) = aten::clone(%83),\n",
       " %249 : Long() = aten::clone(%84),\n",
       " %250 : Float(512, 512, 1, 1) = aten::clone(%85),\n",
       " %251 : Float(512) = aten::clone(%86),\n",
       " %252 : Float(512) = aten::clone(%87),\n",
       " %253 : Float(512) = aten::clone(%88),\n",
       " %254 : Float(512) = aten::clone(%89),\n",
       " %255 : Long() = aten::clone(%90),\n",
       " %256 : Float(512, 1, 3, 3) = aten::clone(%91),\n",
       " %257 : Float(512) = aten::clone(%92),\n",
       " %258 : Float(512) = aten::clone(%93),\n",
       " %259 : Float(512) = aten::clone(%94),\n",
       " %260 : Float(512) = aten::clone(%95),\n",
       " %261 : Long() = aten::clone(%96),\n",
       " %262 : Float(512, 512, 1, 1) = aten::clone(%97),\n",
       " %263 : Float(512) = aten::clone(%98),\n",
       " %264 : Float(512) = aten::clone(%99),\n",
       " %265 : Float(512) = aten::clone(%100),\n",
       " %266 : Float(512) = aten::clone(%101),\n",
       " %267 : Long() = aten::clone(%102),\n",
       " %268 : Float(512, 1, 3, 3) = aten::clone(%103),\n",
       " %269 : Float(512) = aten::clone(%104),\n",
       " %270 : Float(512) = aten::clone(%105),\n",
       " %271 : Float(512) = aten::clone(%106),\n",
       " %272 : Float(512) = aten::clone(%107),\n",
       " %273 : Long() = aten::clone(%108),\n",
       " %274 : Float(512, 512, 1, 1) = aten::clone(%109),\n",
       " %275 : Float(512) = aten::clone(%110),\n",
       " %276 : Float(512) = aten::clone(%111),\n",
       " %277 : Float(512) = aten::clone(%112),\n",
       " %278 : Float(512) = aten::clone(%113),\n",
       " %279 : Long() = aten::clone(%114),\n",
       " %280 : Float(512, 1, 3, 3) = aten::clone(%115),\n",
       " %281 : Float(512) = aten::clone(%116),\n",
       " %282 : Float(512) = aten::clone(%117),\n",
       " %283 : Float(512) = aten::clone(%118),\n",
       " %284 : Float(512) = aten::clone(%119),\n",
       " %285 : Long() = aten::clone(%120),\n",
       " %286 : Float(512, 512, 1, 1) = aten::clone(%121),\n",
       " %287 : Float(512) = aten::clone(%122),\n",
       " %288 : Float(512) = aten::clone(%123),\n",
       " %289 : Float(512) = aten::clone(%124),\n",
       " %290 : Float(512) = aten::clone(%125),\n",
       " %291 : Long() = aten::clone(%126),\n",
       " %292 : Float(512, 1, 3, 3) = aten::clone(%127),\n",
       " %293 : Float(512) = aten::clone(%128),\n",
       " %294 : Float(512) = aten::clone(%129),\n",
       " %295 : Float(512) = aten::clone(%130),\n",
       " %296 : Float(512) = aten::clone(%131),\n",
       " %297 : Long() = aten::clone(%132),\n",
       " %298 : Float(512, 512, 1, 1) = aten::clone(%133),\n",
       " %299 : Float(512) = aten::clone(%134),\n",
       " %300 : Float(512) = aten::clone(%135),\n",
       " %301 : Float(512) = aten::clone(%136),\n",
       " %302 : Float(512) = aten::clone(%137),\n",
       " %303 : Long() = aten::clone(%138),\n",
       " %304 : Float(512, 1, 3, 3) = aten::clone(%139),\n",
       " %305 : Float(512) = aten::clone(%140),\n",
       " %306 : Float(512) = aten::clone(%141),\n",
       " %307 : Float(512) = aten::clone(%142),\n",
       " %308 : Float(512) = aten::clone(%143),\n",
       " %309 : Long() = aten::clone(%144),\n",
       " %310 : Float(1024, 512, 1, 1) = aten::clone(%145),\n",
       " %311 : Float(1024) = aten::clone(%146),\n",
       " %312 : Float(1024) = aten::clone(%147),\n",
       " %313 : Float(1024) = aten::clone(%148),\n",
       " %314 : Float(1024) = aten::clone(%149),\n",
       " %315 : Long() = aten::clone(%150),\n",
       " %316 : Float(1024, 1, 3, 3) = aten::clone(%151),\n",
       " %317 : Float(1024) = aten::clone(%152),\n",
       " %318 : Float(1024) = aten::clone(%153),\n",
       " %319 : Float(1024) = aten::clone(%154),\n",
       " %320 : Float(1024) = aten::clone(%155),\n",
       " %321 : Long() = aten::clone(%156),\n",
       " %322 : Float(1024, 1024, 1, 1) = aten::clone(%157),\n",
       " %323 : Float(1024) = aten::clone(%158),\n",
       " %324 : Float(1024) = aten::clone(%159),\n",
       " %325 : Float(1024) = aten::clone(%160),\n",
       " %326 : Float(1024) = aten::clone(%161),\n",
       " %327 : Long() = aten::clone(%162),\n",
       " %328 : Float(1000, 1024) = aten::clone(%163),\n",
       " %329 : Float(1000) = aten::clone(%164),\n",
       " %330 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %331 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %332 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %333 : int[] = prim::ListConstruct(%331, %332), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %334 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %335 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %336 : int[] = prim::ListConstruct(%334, %335), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %337 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %338 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %339 : int[] = prim::ListConstruct(%337, %338), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %340 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %341 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %342 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %343 : int[] = prim::ListConstruct(%341, %342), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %344 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %345 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %346 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %347 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0],\n",
       " %349 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %350 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %351 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %352 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1],\n",
       " %354 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2],\n",
       " %355 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2],\n",
       " %356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2],\n",
       " %357 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %358 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %359 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %360 : int[] = prim::ListConstruct(%358, %359), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %361 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %362 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %363 : int[] = prim::ListConstruct(%361, %362), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %364 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %365 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %366 : int[] = prim::ListConstruct(%364, %365), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %367 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %368 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %369 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %370 : int[] = prim::ListConstruct(%368, %369), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %371 : int = prim::Constant[value=32](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %372 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %373 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %374 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0],\n",
       " %376 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %377 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %378 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %379 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1],\n",
       " %381 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2],\n",
       " %382 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2],\n",
       " %383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2],\n",
       " %384 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %385 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %386 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %387 : int[] = prim::ListConstruct(%385, %386), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %388 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %389 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %390 : int[] = prim::ListConstruct(%388, %389), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %391 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %392 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %393 : int[] = prim::ListConstruct(%391, %392), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %394 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %395 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %396 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %397 : int[] = prim::ListConstruct(%395, %396), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %398 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %399 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %400 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %401 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0],\n",
       " %403 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %404 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %405 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %406 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1],\n",
       " %408 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2],\n",
       " %409 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2],\n",
       " %410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2],\n",
       " %411 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %412 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %413 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %414 : int[] = prim::ListConstruct(%412, %413), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %415 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %416 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %417 : int[] = prim::ListConstruct(%415, %416), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %418 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %419 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %420 : int[] = prim::ListConstruct(%418, %419), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %421 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %422 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %423 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %424 : int[] = prim::ListConstruct(%422, %423), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %425 : int = prim::Constant[value=64](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %426 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %427 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %428 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0],\n",
       " %430 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %431 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %432 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %433 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1],\n",
       " %435 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2],\n",
       " %436 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2],\n",
       " %437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2],\n",
       " %438 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %439 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %440 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %441 : int[] = prim::ListConstruct(%439, %440), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %442 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %443 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %444 : int[] = prim::ListConstruct(%442, %443), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %445 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %446 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %447 : int[] = prim::ListConstruct(%445, %446), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %448 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %449 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %450 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %451 : int[] = prim::ListConstruct(%449, %450), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %452 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %453 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %454 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %455 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0],\n",
       " %457 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %458 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %459 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %460 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1],\n",
       " %462 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2],\n",
       " %463 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2],\n",
       " %464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2],\n",
       " %465 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %466 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %467 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %468 : int[] = prim::ListConstruct(%466, %467), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %469 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %470 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %471 : int[] = prim::ListConstruct(%469, %470), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %472 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %473 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %474 : int[] = prim::ListConstruct(%472, %473), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %475 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %476 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %477 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %478 : int[] = prim::ListConstruct(%476, %477), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %479 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %480 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %481 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %482 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0],\n",
       " %484 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %485 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %486 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %487 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1],\n",
       " %489 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2],\n",
       " %490 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2],\n",
       " %491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2],\n",
       " %492 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %493 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %494 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %495 : int[] = prim::ListConstruct(%493, %494), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %496 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %497 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %498 : int[] = prim::ListConstruct(%496, %497), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %499 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %500 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %501 : int[] = prim::ListConstruct(%499, %500), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %502 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %503 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %504 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %505 : int[] = prim::ListConstruct(%503, %504), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %506 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %507 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %508 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %509 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0],\n",
       " %511 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %512 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %513 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %514 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1],\n",
       " %516 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2],\n",
       " %517 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2],\n",
       " %518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2],\n",
       " %519 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %520 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %521 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %522 : int[] = prim::ListConstruct(%520, %521), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %523 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %524 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %525 : int[] = prim::ListConstruct(%523, %524), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %526 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %527 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %528 : int[] = prim::ListConstruct(%526, %527), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %529 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %530 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %531 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %532 : int[] = prim::ListConstruct(%530, %531), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %533 : int = prim::Constant[value=128](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %534 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %535 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %536 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0],\n",
       " %538 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %539 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %540 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %541 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1],\n",
       " %543 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2],\n",
       " %544 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2],\n",
       " %545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2],\n",
       " %546 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %547 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %548 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %549 : int[] = prim::ListConstruct(%547, %548), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %550 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %551 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %552 : int[] = prim::ListConstruct(%550, %551), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %553 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %554 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %555 : int[] = prim::ListConstruct(%553, %554), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %556 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %557 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %558 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %559 : int[] = prim::ListConstruct(%557, %558), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %560 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %561 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %562 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %563 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0],\n",
       " %565 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %566 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %567 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %568 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1],\n",
       " %570 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2],\n",
       " %571 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2],\n",
       " %572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2],\n",
       " %573 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %574 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %575 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %576 : int[] = prim::ListConstruct(%574, %575), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %577 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %578 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %579 : int[] = prim::ListConstruct(%577, %578), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %580 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %581 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %582 : int[] = prim::ListConstruct(%580, %581), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %583 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %584 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %585 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %586 : int[] = prim::ListConstruct(%584, %585), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %587 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %588 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %589 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %590 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0],\n",
       " %592 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %593 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %594 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %595 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1],\n",
       " %597 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2],\n",
       " %598 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2],\n",
       " %599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2],\n",
       " %600 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %601 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %602 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %603 : int[] = prim::ListConstruct(%601, %602), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %604 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %605 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %606 : int[] = prim::ListConstruct(%604, %605), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %607 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %608 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %609 : int[] = prim::ListConstruct(%607, %608), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %610 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %611 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %612 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %613 : int[] = prim::ListConstruct(%611, %612), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %614 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %615 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %616 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %617 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0],\n",
       " %619 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %620 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %621 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %622 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1],\n",
       " %624 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2],\n",
       " %625 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2],\n",
       " %626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2],\n",
       " %627 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %628 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %629 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %630 : int[] = prim::ListConstruct(%628, %629), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %631 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %632 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %633 : int[] = prim::ListConstruct(%631, %632), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %634 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %635 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %636 : int[] = prim::ListConstruct(%634, %635), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %637 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %638 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %639 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %640 : int[] = prim::ListConstruct(%638, %639), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %641 : int = prim::Constant[value=256](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %642 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %643 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %644 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0],\n",
       " %646 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %647 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %648 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %649 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1],\n",
       " %651 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2],\n",
       " %652 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2],\n",
       " %653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2],\n",
       " %654 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %655 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %656 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %657 : int[] = prim::ListConstruct(%655, %656), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %658 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %659 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %660 : int[] = prim::ListConstruct(%658, %659), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %661 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %662 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %663 : int[] = prim::ListConstruct(%661, %662), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %664 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %665 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %666 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %667 : int[] = prim::ListConstruct(%665, %666), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %668 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %669 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %670 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %671 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0],\n",
       " %673 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %674 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %675 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %676 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1],\n",
       " %678 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2],\n",
       " %679 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2],\n",
       " %680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2],\n",
       " %681 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %682 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %683 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %684 : int[] = prim::ListConstruct(%682, %683), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %685 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %686 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %687 : int[] = prim::ListConstruct(%685, %686), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %688 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %689 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %690 : int[] = prim::ListConstruct(%688, %689), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %691 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %692 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %693 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %694 : int[] = prim::ListConstruct(%692, %693), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %695 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %696 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %697 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %698 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0],\n",
       " %700 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %701 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %702 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %703 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1],\n",
       " %705 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2],\n",
       " %706 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2],\n",
       " %707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2],\n",
       " %708 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %709 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %710 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %711 : int[] = prim::ListConstruct(%709, %710), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %712 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %713 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %714 : int[] = prim::ListConstruct(%712, %713), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %715 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %716 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %717 : int[] = prim::ListConstruct(%715, %716), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %718 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %719 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %720 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %721 : int[] = prim::ListConstruct(%719, %720), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %722 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %723 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %724 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %725 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0],\n",
       " %727 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %728 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %729 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %730 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1],\n",
       " %732 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2],\n",
       " %733 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2],\n",
       " %734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2],\n",
       " %735 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %736 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %737 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %738 : int[] = prim::ListConstruct(%736, %737), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %739 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %740 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %741 : int[] = prim::ListConstruct(%739, %740), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %742 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %743 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %744 : int[] = prim::ListConstruct(%742, %743), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %745 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %746 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %747 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %748 : int[] = prim::ListConstruct(%746, %747), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %749 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %750 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %751 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %752 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0],\n",
       " %754 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %755 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %756 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %757 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1],\n",
       " %759 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2],\n",
       " %760 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2],\n",
       " %761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2],\n",
       " %762 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %763 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %764 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %765 : int[] = prim::ListConstruct(%763, %764), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %766 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %767 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %768 : int[] = prim::ListConstruct(%766, %767), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %769 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %770 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %771 : int[] = prim::ListConstruct(%769, %770), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %772 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %773 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %774 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %775 : int[] = prim::ListConstruct(%773, %774), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %776 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %777 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %778 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %779 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0],\n",
       " %781 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %782 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %783 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %784 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1],\n",
       " %786 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2],\n",
       " %787 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2],\n",
       " %788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2],\n",
       " %789 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %790 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %791 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %792 : int[] = prim::ListConstruct(%790, %791), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %793 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %794 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %795 : int[] = prim::ListConstruct(%793, %794), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %796 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %797 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %798 : int[] = prim::ListConstruct(%796, %797), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %799 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %800 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %801 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %802 : int[] = prim::ListConstruct(%800, %801), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %803 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %804 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %805 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %806 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0],\n",
       " %808 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %809 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %810 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %811 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1],\n",
       " %813 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2],\n",
       " %814 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2],\n",
       " %815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2],\n",
       " %816 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %817 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %818 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %819 : int[] = prim::ListConstruct(%817, %818), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %820 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %821 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %822 : int[] = prim::ListConstruct(%820, %821), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %823 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %824 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %825 : int[] = prim::ListConstruct(%823, %824), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %826 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %827 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %828 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %829 : int[] = prim::ListConstruct(%827, %828), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %830 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %831 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %832 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %833 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0],\n",
       " %835 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %836 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %837 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %838 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1],\n",
       " %840 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2],\n",
       " %841 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2],\n",
       " %842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2],\n",
       " %843 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %844 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %845 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %846 : int[] = prim::ListConstruct(%844, %845), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %847 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %848 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %849 : int[] = prim::ListConstruct(%847, %848), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %850 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %851 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %852 : int[] = prim::ListConstruct(%850, %851), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %853 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %854 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %855 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %856 : int[] = prim::ListConstruct(%854, %855), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %857 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %858 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %859 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %860 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0],\n",
       " %862 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %863 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %864 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %865 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1],\n",
       " %867 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2],\n",
       " %868 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2],\n",
       " %869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2],\n",
       " %870 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %871 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %872 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %873 : int[] = prim::ListConstruct(%871, %872), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %874 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %875 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %876 : int[] = prim::ListConstruct(%874, %875), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %877 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %878 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %879 : int[] = prim::ListConstruct(%877, %878), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %880 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %881 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %882 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %883 : int[] = prim::ListConstruct(%881, %882), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %884 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %885 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %886 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %887 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0],\n",
       " %889 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %890 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %891 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %892 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1],\n",
       " %894 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2],\n",
       " %895 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2],\n",
       " %896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2],\n",
       " %897 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %898 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %899 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %900 : int[] = prim::ListConstruct(%898, %899), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %901 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %902 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %903 : int[] = prim::ListConstruct(%901, %902), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %904 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %905 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %906 : int[] = prim::ListConstruct(%904, %905), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %907 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %908 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %909 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %910 : int[] = prim::ListConstruct(%908, %909), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %911 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %912 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %913 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %914 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0],\n",
       " %916 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %917 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %918 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %919 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1],\n",
       " %921 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2],\n",
       " %922 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2],\n",
       " %923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2],\n",
       " %924 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %925 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %926 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %927 : int[] = prim::ListConstruct(%925, %926), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %928 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %929 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %930 : int[] = prim::ListConstruct(%928, %929), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %931 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %932 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %933 : int[] = prim::ListConstruct(%931, %932), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %934 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %935 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %936 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %937 : int[] = prim::ListConstruct(%935, %936), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %938 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %939 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %940 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %941 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0],\n",
       " %943 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %944 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %945 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %946 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1],\n",
       " %948 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2],\n",
       " %949 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2],\n",
       " %950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2],\n",
       " %951 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %952 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %953 : int = prim::Constant[value=2](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %954 : int[] = prim::ListConstruct(%952, %953), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %955 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %956 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %957 : int[] = prim::ListConstruct(%955, %956), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %958 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %959 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %960 : int[] = prim::ListConstruct(%958, %959), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %961 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %962 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %963 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %964 : int[] = prim::ListConstruct(%962, %963), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %965 : int = prim::Constant[value=512](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %966 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %967 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %968 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0],\n",
       " %970 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %971 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %972 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %973 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1],\n",
       " %975 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2],\n",
       " %976 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2],\n",
       " %977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2],\n",
       " %978 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %979 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %980 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %981 : int[] = prim::ListConstruct(%979, %980), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %982 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %983 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %984 : int[] = prim::ListConstruct(%982, %983), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %985 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %986 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %987 : int[] = prim::ListConstruct(%985, %986), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %988 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %989 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %990 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %991 : int[] = prim::ListConstruct(%989, %990), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %992 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %993 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %994 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %995 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0],\n",
       " %997 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %998 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %999 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %1000 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1],\n",
       " %1002 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2],\n",
       " %1003 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2],\n",
       " %1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2],\n",
       " %1005 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1006 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1007 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1008 : int[] = prim::ListConstruct(%1006, %1007), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1009 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1010 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1011 : int[] = prim::ListConstruct(%1009, %1010), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1012 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1013 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1014 : int[] = prim::ListConstruct(%1012, %1013), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1015 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1016 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1017 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1018 : int[] = prim::ListConstruct(%1016, %1017), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1019 : int = prim::Constant[value=1024](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1020 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1021 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1022 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0],\n",
       " %1024 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1025 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1026 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1027 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1],\n",
       " %1029 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2],\n",
       " %1030 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2],\n",
       " %1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2],\n",
       " %1032 : Tensor? = prim::Constant(), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1033 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1034 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1035 : int[] = prim::ListConstruct(%1033, %1034), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1036 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1037 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1038 : int[] = prim::ListConstruct(%1036, %1037), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1039 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1040 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1041 : int[] = prim::ListConstruct(%1039, %1040), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1042 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1043 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1044 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1045 : int[] = prim::ListConstruct(%1043, %1044), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1046 : int = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1047 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1048 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1049 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0],\n",
       " %1051 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1052 : float = prim::Constant[value=0.1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1053 : float = prim::Constant[value=1e-05](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1054 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1],\n",
       " %1056 : float = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2],\n",
       " %1057 : float = prim::Constant[value=6](), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2],\n",
       " %1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2],\n",
       " %1059 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1060 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1061 : int[] = prim::ListConstruct(%1059, %1060), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1062 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1063 : int = prim::Constant[value=7](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1064 : int[] = prim::ListConstruct(%1062, %1063), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1065 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1066 : int = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1067 : int[] = prim::ListConstruct(%1065, %1066), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1068 : bool = prim::Constant[value=0](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1069 : bool = prim::Constant[value=1](), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1070 : Float(1, 1024, 1, 1) = aten::avg_pool2d(%1058, %1061, %1064, %1067, %1068, %1069), scope: mobilenet_real/Sequential[model]/AvgPool2d[27],\n",
       " %1071 : int = prim::Constant[value=-1](), scope: mobilenet_real,\n",
       " %1072 : int = prim::Constant[value=1024](), scope: mobilenet_real,\n",
       " %1073 : int[] = prim::ListConstruct(%1071, %1072), scope: mobilenet_real,\n",
       " %input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real,\n",
       " %1075 : Float(1024!, 1000!) = aten::t(%163), scope: mobilenet_real/Linear[fc],\n",
       " %1076 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc],\n",
       " %1077 : int = prim::Constant[value=1](), scope: mobilenet_real/Linear[fc],\n",
       " %1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trace.graph().nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdnn.conversion.common.DataStructure.graph import GraphNode, Graph\n",
    "import torch\n",
    "import torch.jit\n",
    "import torch.autograd\n",
    "import torch.serialization\n",
    "import contextlib\n",
    "from torch.jit import _unique_state_dict\n",
    "\n",
    "\n",
    "\n",
    "class PytorchGraphNode(GraphNode):\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self._name = layer.scopeName()\n",
    "        self._kind = layer.kind()\n",
    "        import re\n",
    "        node_id = re.search(r\"[\\d]+\", layer.__str__())\n",
    "        self.id = node_id.group(0)\n",
    "\n",
    "        super(PytorchGraphNode, self).__init__(layer)\n",
    "        self.attrs = {k : layer[k] for k in layer.attributeNames()}\n",
    "\n",
    "        self.weights_name = '.'.join(\n",
    "            re.findall(r'\\[([\\w\\d.]+)\\]', self._name)\n",
    "        )\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = self._name + self.id\n",
    "        # Scopes created in a nested scope may have initial characters\n",
    "        # that are illegal as the initial character of an op name\n",
    "        # (viz. '-', '\\', '/', and '_').\n",
    "        name = name.replace('-','n').replace('\\\\','n').replace('/','n').replace('_','n').replace('[','n').replace(']','n')\n",
    "        return name\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self._kind\n",
    "\n",
    "    @property\n",
    "    def pytorch_layer(self):\n",
    "        return self.layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PytorchGraph(Graph):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        # sanity check.\n",
    "        super(PytorchGraph, self).__init__(model)\n",
    "        self.model = model\n",
    "        self.state_dict = _unique_state_dict(self.model)\n",
    "        self.shape_dict = dict()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _optimize_graph(graph, aten, export_raw_ir=False):\n",
    "        # run dce first to eliminate dead parts of the graph that might have been\n",
    "        # left behind by things like symbolic_override\n",
    "\n",
    "        torch._C._jit_pass_dce(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "\n",
    "        torch._C._jit_pass_peephole(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "        if not export_raw_ir:\n",
    "            graph = torch._C._jit_pass_onnx(graph, aten)\n",
    " \n",
    "            torch._C._jit_pass_lint(graph)\n",
    "            torch._C._jit_pass_onnx_peephole(graph)\n",
    "            torch._C._jit_pass_lint(graph)\n",
    "        torch._C._jit_pass_dce(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "        graph = torch._C._jit_pass_canonicalize(graph)\n",
    "        torch._C._jit_pass_lint(graph)\n",
    "        return graph\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_node_id(node):\n",
    "        import re\n",
    "        node_id = re.search(r\"[\\d]+\", node.__str__())\n",
    "        return node_id.group(0)\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def set_training(self, model, mode):\n",
    "        r\"\"\"\n",
    "        A context manager to temporarily set the training mode of 'model'\n",
    "        to 'mode', resetting it when we exit the with-block.  A no-op if\n",
    "        mode is None.\n",
    "        \"\"\"\n",
    "        if mode is None:\n",
    "            yield\n",
    "            return\n",
    "        old_mode = model.training\n",
    "        if old_mode != mode:\n",
    "            model.train(mode)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            if old_mode != mode:\n",
    "                model.train(old_mode)\n",
    "\n",
    "\n",
    "    def build(self, shape):\n",
    "        \"\"\"\n",
    "        build graph for pytorch 0.4.0\n",
    "        \"\"\"\n",
    "\n",
    "        import re\n",
    "        # construct graph\n",
    "        dummy_input = torch.autograd.Variable(torch.randn(shape), requires_grad=False)\n",
    "\n",
    "\n",
    "        with self.set_training(self.model, False):\n",
    "            trace, output = torch.jit.get_trace_graph(self.model, (dummy_input, ))\n",
    "\n",
    "        #trace.set_graph(PytorchGraph._optimize_graph(trace.graph(), False))\n",
    "        # nodes\n",
    "        nodes = list(trace.graph().nodes())\n",
    "        for node in nodes:\n",
    "            if 'input' in str(node):\n",
    "\n",
    "                print('*************************')\n",
    "                print(node)\n",
    "                print(node.scopeName())\n",
    "                for i,node_input in enumerate(list(node.inputs())):\n",
    "                    print(i, '\\t',PytorchGraph.get_node_id(node_input.node()), node_input.node().scopeName())\n",
    "                \n",
    "                \n",
    "\n",
    "        # input layer\n",
    "        # TODO\n",
    "        return True\n",
    "        print('going to read stuff')\n",
    "\n",
    "        # build each layer\n",
    "        for node in nodes:\n",
    "            node_id = PytorchGraph.get_node_id(node)\n",
    "            node_scope = node.scopeName()\n",
    "            node_name = node_scope + node_id\n",
    "            print(node)\n",
    "            print(node_name)\n",
    "            node_name = node_name.replace('-','n').replace('\\\\','n').replace('/','n').replace('_','n').replace('[','n').replace(']','n')\n",
    "            output_shape_str = re.findall(r'[^()!]+', node.__str__())[1]\n",
    "            #output_shape = [int(x.replace('!', '')) for x in output_shape_str.split(',')]\n",
    "            output_shape = [x.replace('!', '') for x in output_shape_str.split(',')]\n",
    "            print(output_shape)\n",
    "\n",
    "\n",
    "            self.shape_dict[node_name] = output_shape\n",
    "            self.layer_map[node_name] = PytorchGraphNode(node)\n",
    "            self.layer_name_map[node_name] = node_name\n",
    "\n",
    "            # input\n",
    "            for node_input in list(node.inputs()):\n",
    "\n",
    "                if PytorchGraph.get_node_id(node_input.node()) and node_input.node().scopeName():\n",
    "                    node_input_name = node_input.node().scopeName() + PytorchGraph.get_node_id(node_input.node())\n",
    "                    node_input_name = node_input_name.replace('-','n').replace('\\\\','n').replace('/','n').replace('_','n').replace('[','n').replace(']','n')\n",
    "                    print('Connection',node_input_name, node_name)\n",
    "                    self._make_connection(node_input_name, node_name)\n",
    "                    # print(node_input_name ,'->', node_name)\n",
    "        print('Complete')\n",
    "\n",
    "        super(PytorchGraph, self).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "%input.1 : Float(1, 32, 112, 112) = aten::_convolution(%0, %1, %330, %333, %336, %339, %340, %343, %344, %345, %346, %347), scope: mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "0 \t 0 \n",
      "1 \t 0 \n",
      "2 \t 330 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "3 \t 333 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "4 \t 336 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "5 \t 339 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "6 \t 340 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "7 \t 343 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "8 \t 344 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "9 \t 345 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "10 \t 346 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "11 \t 347 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "*************************\n",
      "%input.2 : Float(1, 32, 112, 112) = aten::batch_norm(%input.1, %2, %3, %4, %5, %349, %350, %351, %352), scope: mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "0 \t 1 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 349 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "6 \t 350 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "7 \t 351 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "8 \t 352 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "*************************\n",
      "%356 : Float(1, 32, 112, 112) = aten::hardtanh(%input.2, %354, %355), scope: mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "0 \t 2 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]\n",
      "1 \t 354 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "2 \t 355 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "*************************\n",
      "%input.3 : Float(1, 32, 112, 112) = aten::_convolution(%356, %7, %357, %360, %363, %366, %367, %370, %371, %372, %373, %374), scope: mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "0 \t 356 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 357 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "3 \t 360 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "4 \t 363 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "5 \t 366 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "6 \t 367 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "7 \t 370 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "8 \t 371 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "9 \t 372 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "10 \t 373 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "11 \t 374 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "*************************\n",
      "%input.4 : Float(1, 32, 112, 112) = aten::batch_norm(%input.3, %8, %9, %10, %11, %376, %377, %378, %379), scope: mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "0 \t 3 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 376 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "6 \t 377 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "7 \t 378 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "8 \t 379 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "*************************\n",
      "%383 : Float(1, 32, 112, 112) = aten::hardtanh(%input.4, %381, %382), scope: mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "0 \t 4 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]\n",
      "1 \t 381 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "2 \t 382 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "*************************\n",
      "%input.5 : Float(1, 64, 112, 112) = aten::_convolution(%383, %13, %384, %387, %390, %393, %394, %397, %398, %399, %400, %401), scope: mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "0 \t 383 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 384 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "3 \t 387 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "4 \t 390 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "5 \t 393 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "6 \t 394 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "7 \t 397 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "8 \t 398 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "9 \t 399 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "10 \t 400 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "11 \t 401 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "*************************\n",
      "%input.6 : Float(1, 64, 112, 112) = aten::batch_norm(%input.5, %14, %15, %16, %17, %403, %404, %405, %406), scope: mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "0 \t 5 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 403 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "6 \t 404 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "7 \t 405 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "8 \t 406 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "*************************\n",
      "%410 : Float(1, 64, 112, 112) = aten::hardtanh(%input.6, %408, %409), scope: mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "0 \t 6 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]\n",
      "1 \t 408 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "2 \t 409 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "*************************\n",
      "%input.7 : Float(1, 64, 56, 56) = aten::_convolution(%410, %19, %411, %414, %417, %420, %421, %424, %425, %426, %427, %428), scope: mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "0 \t 410 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 411 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "3 \t 414 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "4 \t 417 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "5 \t 420 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "6 \t 421 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "7 \t 424 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "8 \t 425 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "9 \t 426 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "10 \t 427 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "11 \t 428 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "*************************\n",
      "%input.8 : Float(1, 64, 56, 56) = aten::batch_norm(%input.7, %20, %21, %22, %23, %430, %431, %432, %433), scope: mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "0 \t 7 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 430 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "6 \t 431 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "7 \t 432 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "8 \t 433 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "*************************\n",
      "%437 : Float(1, 64, 56, 56) = aten::hardtanh(%input.8, %435, %436), scope: mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "0 \t 8 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]\n",
      "1 \t 435 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "2 \t 436 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "*************************\n",
      "%input.9 : Float(1, 128, 56, 56) = aten::_convolution(%437, %25, %438, %441, %444, %447, %448, %451, %452, %453, %454, %455), scope: mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "0 \t 437 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 438 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "3 \t 441 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "4 \t 444 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "5 \t 447 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "6 \t 448 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "7 \t 451 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "8 \t 452 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "9 \t 453 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "10 \t 454 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "11 \t 455 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "*************************\n",
      "%input.10 : Float(1, 128, 56, 56) = aten::batch_norm(%input.9, %26, %27, %28, %29, %457, %458, %459, %460), scope: mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "0 \t 9 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 457 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "6 \t 458 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "7 \t 459 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "8 \t 460 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "*************************\n",
      "%464 : Float(1, 128, 56, 56) = aten::hardtanh(%input.10, %462, %463), scope: mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "0 \t 10 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]\n",
      "1 \t 462 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "2 \t 463 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "*************************\n",
      "%input.11 : Float(1, 128, 56, 56) = aten::_convolution(%464, %31, %465, %468, %471, %474, %475, %478, %479, %480, %481, %482), scope: mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "0 \t 464 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 465 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "3 \t 468 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "4 \t 471 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "5 \t 474 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "6 \t 475 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "7 \t 478 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "8 \t 479 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "9 \t 480 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "10 \t 481 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "11 \t 482 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "*************************\n",
      "%input.12 : Float(1, 128, 56, 56) = aten::batch_norm(%input.11, %32, %33, %34, %35, %484, %485, %486, %487), scope: mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "0 \t 11 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 484 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "6 \t 485 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "7 \t 486 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "8 \t 487 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "*************************\n",
      "%491 : Float(1, 128, 56, 56) = aten::hardtanh(%input.12, %489, %490), scope: mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "0 \t 12 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]\n",
      "1 \t 489 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "2 \t 490 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "*************************\n",
      "%input.13 : Float(1, 128, 56, 56) = aten::_convolution(%491, %37, %492, %495, %498, %501, %502, %505, %506, %507, %508, %509), scope: mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "0 \t 491 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 492 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "3 \t 495 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "4 \t 498 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "5 \t 501 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "6 \t 502 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "7 \t 505 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "8 \t 506 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "9 \t 507 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "10 \t 508 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "11 \t 509 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "*************************\n",
      "%input.14 : Float(1, 128, 56, 56) = aten::batch_norm(%input.13, %38, %39, %40, %41, %511, %512, %513, %514), scope: mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "0 \t 13 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 511 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "6 \t 512 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "7 \t 513 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "8 \t 514 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "*************************\n",
      "%518 : Float(1, 128, 56, 56) = aten::hardtanh(%input.14, %516, %517), scope: mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "0 \t 14 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]\n",
      "1 \t 516 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "2 \t 517 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "*************************\n",
      "%input.15 : Float(1, 128, 28, 28) = aten::_convolution(%518, %43, %519, %522, %525, %528, %529, %532, %533, %534, %535, %536), scope: mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "0 \t 518 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 519 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "3 \t 522 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "4 \t 525 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "5 \t 528 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "6 \t 529 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "7 \t 532 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "8 \t 533 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "9 \t 534 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "10 \t 535 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "11 \t 536 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "*************************\n",
      "%input.16 : Float(1, 128, 28, 28) = aten::batch_norm(%input.15, %44, %45, %46, %47, %538, %539, %540, %541), scope: mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "0 \t 15 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 538 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "6 \t 539 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "7 \t 540 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "8 \t 541 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "*************************\n",
      "%545 : Float(1, 128, 28, 28) = aten::hardtanh(%input.16, %543, %544), scope: mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "0 \t 16 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]\n",
      "1 \t 543 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "2 \t 544 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "*************************\n",
      "%input.17 : Float(1, 256, 28, 28) = aten::_convolution(%545, %49, %546, %549, %552, %555, %556, %559, %560, %561, %562, %563), scope: mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "0 \t 545 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 546 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "3 \t 549 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "4 \t 552 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "5 \t 555 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "6 \t 556 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "7 \t 559 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "8 \t 560 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "9 \t 561 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "10 \t 562 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "11 \t 563 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "*************************\n",
      "%input.18 : Float(1, 256, 28, 28) = aten::batch_norm(%input.17, %50, %51, %52, %53, %565, %566, %567, %568), scope: mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "0 \t 17 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 565 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "6 \t 566 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "7 \t 567 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "8 \t 568 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "*************************\n",
      "%572 : Float(1, 256, 28, 28) = aten::hardtanh(%input.18, %570, %571), scope: mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "0 \t 18 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]\n",
      "1 \t 570 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "2 \t 571 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "*************************\n",
      "%input.19 : Float(1, 256, 28, 28) = aten::_convolution(%572, %55, %573, %576, %579, %582, %583, %586, %587, %588, %589, %590), scope: mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "0 \t 572 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 573 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "3 \t 576 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "4 \t 579 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "5 \t 582 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "6 \t 583 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "7 \t 586 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "8 \t 587 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "9 \t 588 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "10 \t 589 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "11 \t 590 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "*************************\n",
      "%input.20 : Float(1, 256, 28, 28) = aten::batch_norm(%input.19, %56, %57, %58, %59, %592, %593, %594, %595), scope: mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "0 \t 19 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 592 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "6 \t 593 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "7 \t 594 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "8 \t 595 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "*************************\n",
      "%599 : Float(1, 256, 28, 28) = aten::hardtanh(%input.20, %597, %598), scope: mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "0 \t 20 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]\n",
      "1 \t 597 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "2 \t 598 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "*************************\n",
      "%input.21 : Float(1, 256, 28, 28) = aten::_convolution(%599, %61, %600, %603, %606, %609, %610, %613, %614, %615, %616, %617), scope: mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "0 \t 599 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 600 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "3 \t 603 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "4 \t 606 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "5 \t 609 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "6 \t 610 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "7 \t 613 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "8 \t 614 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "9 \t 615 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "10 \t 616 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "11 \t 617 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "*************************\n",
      "%input.22 : Float(1, 256, 28, 28) = aten::batch_norm(%input.21, %62, %63, %64, %65, %619, %620, %621, %622), scope: mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "0 \t 21 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 619 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "6 \t 620 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "7 \t 621 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "8 \t 622 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "*************************\n",
      "%626 : Float(1, 256, 28, 28) = aten::hardtanh(%input.22, %624, %625), scope: mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "0 \t 22 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]\n",
      "1 \t 624 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "2 \t 625 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "*************************\n",
      "%input.23 : Float(1, 256, 14, 14) = aten::_convolution(%626, %67, %627, %630, %633, %636, %637, %640, %641, %642, %643, %644), scope: mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "0 \t 626 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 627 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "3 \t 630 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "4 \t 633 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "5 \t 636 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "6 \t 637 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "7 \t 640 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "8 \t 641 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "9 \t 642 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "10 \t 643 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "11 \t 644 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "*************************\n",
      "%input.24 : Float(1, 256, 14, 14) = aten::batch_norm(%input.23, %68, %69, %70, %71, %646, %647, %648, %649), scope: mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "0 \t 23 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 646 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "6 \t 647 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "7 \t 648 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "8 \t 649 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "*************************\n",
      "%653 : Float(1, 256, 14, 14) = aten::hardtanh(%input.24, %651, %652), scope: mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "0 \t 24 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]\n",
      "1 \t 651 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "2 \t 652 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "*************************\n",
      "%input.25 : Float(1, 512, 14, 14) = aten::_convolution(%653, %73, %654, %657, %660, %663, %664, %667, %668, %669, %670, %671), scope: mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "0 \t 653 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 654 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "3 \t 657 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "4 \t 660 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "5 \t 663 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "6 \t 664 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "7 \t 667 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "8 \t 668 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "9 \t 669 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "10 \t 670 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "11 \t 671 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "*************************\n",
      "%input.26 : Float(1, 512, 14, 14) = aten::batch_norm(%input.25, %74, %75, %76, %77, %673, %674, %675, %676), scope: mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "0 \t 25 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 673 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "6 \t 674 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "7 \t 675 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "8 \t 676 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "*************************\n",
      "%680 : Float(1, 512, 14, 14) = aten::hardtanh(%input.26, %678, %679), scope: mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "0 \t 26 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]\n",
      "1 \t 678 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "2 \t 679 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "*************************\n",
      "%input.27 : Float(1, 512, 14, 14) = aten::_convolution(%680, %79, %681, %684, %687, %690, %691, %694, %695, %696, %697, %698), scope: mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "0 \t 680 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 681 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "3 \t 684 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "4 \t 687 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "5 \t 690 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "6 \t 691 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "7 \t 694 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "8 \t 695 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "9 \t 696 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "10 \t 697 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "11 \t 698 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "*************************\n",
      "%input.28 : Float(1, 512, 14, 14) = aten::batch_norm(%input.27, %80, %81, %82, %83, %700, %701, %702, %703), scope: mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "0 \t 27 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 700 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "6 \t 701 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "7 \t 702 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "8 \t 703 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "*************************\n",
      "%707 : Float(1, 512, 14, 14) = aten::hardtanh(%input.28, %705, %706), scope: mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "0 \t 28 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]\n",
      "1 \t 705 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "2 \t 706 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "*************************\n",
      "%input.29 : Float(1, 512, 14, 14) = aten::_convolution(%707, %85, %708, %711, %714, %717, %718, %721, %722, %723, %724, %725), scope: mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "0 \t 707 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 708 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "3 \t 711 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "4 \t 714 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "5 \t 717 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "6 \t 718 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "7 \t 721 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "8 \t 722 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "9 \t 723 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "10 \t 724 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "11 \t 725 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "*************************\n",
      "%input.30 : Float(1, 512, 14, 14) = aten::batch_norm(%input.29, %86, %87, %88, %89, %727, %728, %729, %730), scope: mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "0 \t 29 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 727 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "6 \t 728 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "7 \t 729 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "8 \t 730 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "*************************\n",
      "%734 : Float(1, 512, 14, 14) = aten::hardtanh(%input.30, %732, %733), scope: mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "0 \t 30 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]\n",
      "1 \t 732 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "2 \t 733 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "*************************\n",
      "%input.31 : Float(1, 512, 14, 14) = aten::_convolution(%734, %91, %735, %738, %741, %744, %745, %748, %749, %750, %751, %752), scope: mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "0 \t 734 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 735 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "3 \t 738 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "4 \t 741 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "5 \t 744 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "6 \t 745 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "7 \t 748 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "8 \t 749 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "9 \t 750 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "10 \t 751 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "11 \t 752 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "*************************\n",
      "%input.32 : Float(1, 512, 14, 14) = aten::batch_norm(%input.31, %92, %93, %94, %95, %754, %755, %756, %757), scope: mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "0 \t 31 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 754 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "6 \t 755 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "7 \t 756 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "8 \t 757 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "*************************\n",
      "%761 : Float(1, 512, 14, 14) = aten::hardtanh(%input.32, %759, %760), scope: mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "0 \t 32 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]\n",
      "1 \t 759 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "2 \t 760 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "*************************\n",
      "%input.33 : Float(1, 512, 14, 14) = aten::_convolution(%761, %97, %762, %765, %768, %771, %772, %775, %776, %777, %778, %779), scope: mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "0 \t 761 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 762 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "3 \t 765 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "4 \t 768 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "5 \t 771 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "6 \t 772 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "7 \t 775 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "8 \t 776 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "9 \t 777 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "10 \t 778 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "11 \t 779 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "*************************\n",
      "%input.34 : Float(1, 512, 14, 14) = aten::batch_norm(%input.33, %98, %99, %100, %101, %781, %782, %783, %784), scope: mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "0 \t 33 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 781 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "6 \t 782 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "7 \t 783 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "8 \t 784 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "*************************\n",
      "%788 : Float(1, 512, 14, 14) = aten::hardtanh(%input.34, %786, %787), scope: mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "0 \t 34 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]\n",
      "1 \t 786 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "2 \t 787 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "*************************\n",
      "%input.35 : Float(1, 512, 14, 14) = aten::_convolution(%788, %103, %789, %792, %795, %798, %799, %802, %803, %804, %805, %806), scope: mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "0 \t 788 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 789 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "3 \t 792 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "4 \t 795 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "5 \t 798 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "6 \t 799 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "7 \t 802 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "8 \t 803 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "9 \t 804 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "10 \t 805 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "11 \t 806 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "*************************\n",
      "%input.36 : Float(1, 512, 14, 14) = aten::batch_norm(%input.35, %104, %105, %106, %107, %808, %809, %810, %811), scope: mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "0 \t 35 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 808 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "6 \t 809 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "7 \t 810 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "8 \t 811 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "*************************\n",
      "%815 : Float(1, 512, 14, 14) = aten::hardtanh(%input.36, %813, %814), scope: mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "0 \t 36 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]\n",
      "1 \t 813 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "2 \t 814 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "*************************\n",
      "%input.37 : Float(1, 512, 14, 14) = aten::_convolution(%815, %109, %816, %819, %822, %825, %826, %829, %830, %831, %832, %833), scope: mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "0 \t 815 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 816 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "3 \t 819 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "4 \t 822 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "5 \t 825 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "6 \t 826 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "7 \t 829 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "8 \t 830 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "9 \t 831 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "10 \t 832 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "11 \t 833 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "*************************\n",
      "%input.38 : Float(1, 512, 14, 14) = aten::batch_norm(%input.37, %110, %111, %112, %113, %835, %836, %837, %838), scope: mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "0 \t 37 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 835 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "6 \t 836 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "7 \t 837 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "8 \t 838 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "*************************\n",
      "%842 : Float(1, 512, 14, 14) = aten::hardtanh(%input.38, %840, %841), scope: mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "0 \t 38 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]\n",
      "1 \t 840 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "2 \t 841 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "*************************\n",
      "%input.39 : Float(1, 512, 14, 14) = aten::_convolution(%842, %115, %843, %846, %849, %852, %853, %856, %857, %858, %859, %860), scope: mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "0 \t 842 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 843 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "3 \t 846 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "4 \t 849 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "5 \t 852 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "6 \t 853 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "7 \t 856 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "8 \t 857 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "9 \t 858 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "10 \t 859 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "11 \t 860 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "*************************\n",
      "%input.40 : Float(1, 512, 14, 14) = aten::batch_norm(%input.39, %116, %117, %118, %119, %862, %863, %864, %865), scope: mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "0 \t 39 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 862 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "6 \t 863 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "7 \t 864 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "8 \t 865 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "*************************\n",
      "%869 : Float(1, 512, 14, 14) = aten::hardtanh(%input.40, %867, %868), scope: mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "0 \t 40 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]\n",
      "1 \t 867 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "2 \t 868 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "*************************\n",
      "%input.41 : Float(1, 512, 14, 14) = aten::_convolution(%869, %121, %870, %873, %876, %879, %880, %883, %884, %885, %886, %887), scope: mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "0 \t 869 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 870 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "3 \t 873 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "4 \t 876 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "5 \t 879 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "6 \t 880 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "7 \t 883 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "8 \t 884 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "9 \t 885 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "10 \t 886 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "11 \t 887 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "*************************\n",
      "%input.42 : Float(1, 512, 14, 14) = aten::batch_norm(%input.41, %122, %123, %124, %125, %889, %890, %891, %892), scope: mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "0 \t 41 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 889 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "6 \t 890 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "7 \t 891 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "8 \t 892 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "*************************\n",
      "%896 : Float(1, 512, 14, 14) = aten::hardtanh(%input.42, %894, %895), scope: mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "0 \t 42 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]\n",
      "1 \t 894 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "2 \t 895 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "*************************\n",
      "%input.43 : Float(1, 512, 14, 14) = aten::_convolution(%896, %127, %897, %900, %903, %906, %907, %910, %911, %912, %913, %914), scope: mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "0 \t 896 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 897 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "3 \t 900 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "4 \t 903 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "5 \t 906 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "6 \t 907 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "7 \t 910 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "8 \t 911 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "9 \t 912 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "10 \t 913 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "11 \t 914 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "*************************\n",
      "%input.44 : Float(1, 512, 14, 14) = aten::batch_norm(%input.43, %128, %129, %130, %131, %916, %917, %918, %919), scope: mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "0 \t 43 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 916 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "6 \t 917 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "7 \t 918 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "8 \t 919 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "*************************\n",
      "%923 : Float(1, 512, 14, 14) = aten::hardtanh(%input.44, %921, %922), scope: mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "0 \t 44 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]\n",
      "1 \t 921 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "2 \t 922 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "*************************\n",
      "%input.45 : Float(1, 512, 14, 14) = aten::_convolution(%923, %133, %924, %927, %930, %933, %934, %937, %938, %939, %940, %941), scope: mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "0 \t 923 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 924 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "3 \t 927 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "4 \t 930 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "5 \t 933 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "6 \t 934 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "7 \t 937 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "8 \t 938 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "9 \t 939 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "10 \t 940 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "11 \t 941 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "*************************\n",
      "%input.46 : Float(1, 512, 14, 14) = aten::batch_norm(%input.45, %134, %135, %136, %137, %943, %944, %945, %946), scope: mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "0 \t 45 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 943 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "6 \t 944 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "7 \t 945 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "8 \t 946 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "*************************\n",
      "%950 : Float(1, 512, 14, 14) = aten::hardtanh(%input.46, %948, %949), scope: mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "0 \t 46 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]\n",
      "1 \t 948 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "2 \t 949 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "*************************\n",
      "%input.47 : Float(1, 512, 7, 7) = aten::_convolution(%950, %139, %951, %954, %957, %960, %961, %964, %965, %966, %967, %968), scope: mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "0 \t 950 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 951 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "3 \t 954 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "4 \t 957 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "5 \t 960 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "6 \t 961 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "7 \t 964 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "8 \t 965 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "9 \t 966 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "10 \t 967 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "11 \t 968 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "*************************\n",
      "%input.48 : Float(1, 512, 7, 7) = aten::batch_norm(%input.47, %140, %141, %142, %143, %970, %971, %972, %973), scope: mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "0 \t 47 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 970 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "6 \t 971 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "7 \t 972 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "8 \t 973 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "*************************\n",
      "%977 : Float(1, 512, 7, 7) = aten::hardtanh(%input.48, %975, %976), scope: mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "0 \t 48 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]\n",
      "1 \t 975 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "2 \t 976 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "*************************\n",
      "%input.49 : Float(1, 1024, 7, 7) = aten::_convolution(%977, %145, %978, %981, %984, %987, %988, %991, %992, %993, %994, %995), scope: mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "0 \t 977 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 978 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "3 \t 981 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "4 \t 984 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "5 \t 987 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "6 \t 988 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "7 \t 991 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "8 \t 992 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "9 \t 993 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "10 \t 994 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "11 \t 995 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "*************************\n",
      "%input.50 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.49, %146, %147, %148, %149, %997, %998, %999, %1000), scope: mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "0 \t 49 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 997 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "6 \t 998 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "7 \t 999 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "8 \t 1000 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "*************************\n",
      "%1004 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.50, %1002, %1003), scope: mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "0 \t 50 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]\n",
      "1 \t 1002 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "2 \t 1003 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "*************************\n",
      "%input.51 : Float(1, 1024, 7, 7) = aten::_convolution(%1004, %151, %1005, %1008, %1011, %1014, %1015, %1018, %1019, %1020, %1021, %1022), scope: mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "0 \t 1004 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 1005 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "3 \t 1008 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "4 \t 1011 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "5 \t 1014 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "6 \t 1015 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "7 \t 1018 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "8 \t 1019 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "9 \t 1020 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "10 \t 1021 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "11 \t 1022 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "*************************\n",
      "%input.52 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.51, %152, %153, %154, %155, %1024, %1025, %1026, %1027), scope: mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "0 \t 51 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 1024 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "6 \t 1025 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "7 \t 1026 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "8 \t 1027 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "*************************\n",
      "%1031 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.52, %1029, %1030), scope: mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "0 \t 52 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]\n",
      "1 \t 1029 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "2 \t 1030 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "*************************\n",
      "%input.53 : Float(1, 1024, 7, 7) = aten::_convolution(%1031, %157, %1032, %1035, %1038, %1041, %1042, %1045, %1046, %1047, %1048, %1049), scope: mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "0 \t 1031 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]\n",
      "1 \t 0 \n",
      "2 \t 1032 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "3 \t 1035 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "4 \t 1038 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "5 \t 1041 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "6 \t 1042 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "7 \t 1045 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "8 \t 1046 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "9 \t 1047 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "10 \t 1048 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "11 \t 1049 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "*************************\n",
      "%input.54 : Float(1, 1024, 7, 7) = aten::batch_norm(%input.53, %158, %159, %160, %161, %1051, %1052, %1053, %1054), scope: mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "0 \t 53 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]\n",
      "1 \t 0 \n",
      "2 \t 0 \n",
      "3 \t 0 \n",
      "4 \t 0 \n",
      "5 \t 1051 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "6 \t 1052 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "7 \t 1053 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "8 \t 1054 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "*************************\n",
      "%1058 : Float(1, 1024, 7, 7) = aten::hardtanh(%input.54, %1056, %1057), scope: mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "0 \t 54 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]\n",
      "1 \t 1056 mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "2 \t 1057 mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]\n",
      "*************************\n",
      "%input : Float(1, 1024) = aten::view(%1070, %1073), scope: mobilenet_real\n",
      "\n",
      "mobilenet_real\n",
      "0 \t 1070 mobilenet_real/Sequential[model]/AvgPool2d[27]\n",
      "1 \t 1073 mobilenet_real\n",
      "*************************\n",
      "%1078 : Float(1, 1000) = aten::addmm(%164, %input, %1075, %1076, %1077), scope: mobilenet_real/Linear[fc]\n",
      "\n",
      "mobilenet_real/Linear[fc]\n",
      "0 \t 0 \n",
      "1 \t 1 mobilenet_real\n",
      "2 \t 1075 mobilenet_real/Linear[fc]\n",
      "3 \t 1076 mobilenet_real/Linear[fc]\n",
      "4 \t 1077 mobilenet_real/Linear[fc]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = PytorchGraph(model)\n",
    "g.build([1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'mobilenet_real' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d5dac23de4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'mobilenet_real' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "model.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = OrderedDict(model.named_modules())['model.26.0']\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.autograd.Variable(torch.randn([1,3,224,224]), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,v = torch.utils.tensorboard._pytorch_graph.graph(model,dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['', 'model', 'model.0', 'model.0.0', 'model.0.1', 'model.0.2', 'model.1', 'model.1.0', 'model.1.1', 'model.1.2', 'model.2', 'model.2.0', 'model.2.1', 'model.2.2', 'model.3', 'model.3.0', 'model.3.1', 'model.3.2', 'model.4', 'model.4.0', 'model.4.1', 'model.4.2', 'model.5', 'model.5.0', 'model.5.1', 'model.5.2', 'model.6', 'model.6.0', 'model.6.1', 'model.6.2', 'model.7', 'model.7.0', 'model.7.1', 'model.7.2', 'model.8', 'model.8.0', 'model.8.1', 'model.8.2', 'model.9', 'model.9.0', 'model.9.1', 'model.9.2', 'model.10', 'model.10.0', 'model.10.1', 'model.10.2', 'model.11', 'model.11.0', 'model.11.1', 'model.11.2', 'model.12', 'model.12.0', 'model.12.1', 'model.12.2', 'model.13', 'model.13.0', 'model.13.1', 'model.13.2', 'model.14', 'model.14.0', 'model.14.1', 'model.14.2', 'model.15', 'model.15.0', 'model.15.1', 'model.15.2', 'model.16', 'model.16.0', 'model.16.1', 'model.16.2', 'model.17', 'model.17.0', 'model.17.1', 'model.17.2', 'model.18', 'model.18.0', 'model.18.1', 'model.18.2', 'model.19', 'model.19.0', 'model.19.1', 'model.19.2', 'model.20', 'model.20.0', 'model.20.1', 'model.20.2', 'model.21', 'model.21.0', 'model.21.1', 'model.21.2', 'model.22', 'model.22.0', 'model.22.1', 'model.22.2', 'model.23', 'model.23.0', 'model.23.1', 'model.23.2', 'model.24', 'model.24.0', 'model.24.1', 'model.24.2', 'model.25', 'model.25.0', 'model.25.1', 'model.25.2', 'model.26', 'model.26.0', 'model.26.1', 'model.26.2', 'model.27', 'fc'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrderedDict(model.named_modules()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "def get_model_name(a):\n",
    "    s = ''\n",
    "    while a.find('[') > 0:\n",
    "        s += a[a.find('[')+1:a.find(']')]+'.'\n",
    "        a = a[a.find(']')+1:]\n",
    "    return s[:-1]\n",
    "\n",
    "def get_model_id(a, g):\n",
    "    for i, node in enumerate(g.node):\n",
    "        if node.name == a:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "ff = 'mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165'\n",
    "v = get_model_name(ff)\n",
    "v = get_model_id(ff, g)\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "input/0\n",
      "IO Node\n",
      "************\n",
      "138 138 Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165 onnx::Conv\n",
      "0\n",
      "Input 0 : 0 input/0 IO Node \n",
      "1\n",
      "Input 1 : 1 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/1 Parameter model.0.0\n",
      "************\n",
      "139 139 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/166 onnx::BatchNormalization\n",
      "138\n",
      "Input 0 : 138 mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165 onnx::Conv model.0.0\n",
      "2\n",
      "Input 1 : 2 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/2 Parameter model.0.1\n",
      "3\n",
      "Input 2 : 3 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/3 Parameter model.0.1\n",
      "4\n",
      "Input 3 : 4 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/4 Parameter model.0.1\n",
      "5\n",
      "Input 4 : 5 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/5 Parameter model.0.1\n",
      "************\n",
      "140 140 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]/167 onnx::Clip\n",
      "139\n",
      "Input 0 : 139 mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/166 onnx::BatchNormalization model.0.1\n",
      "************\n",
      "141 141 Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/168 onnx::Conv\n",
      "140\n",
      "Input 0 : 140 mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]/167 onnx::Clip model.0.2\n",
      "6\n",
      "Input 1 : 6 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/7 Parameter model.1.0\n",
      "************\n",
      "142 142 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/169 onnx::BatchNormalization\n",
      "141\n",
      "Input 0 : 141 mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/168 onnx::Conv model.1.0\n",
      "7\n",
      "Input 1 : 7 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/8 Parameter model.1.1\n",
      "8\n",
      "Input 2 : 8 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/9 Parameter model.1.1\n",
      "9\n",
      "Input 3 : 9 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/10 Parameter model.1.1\n",
      "10\n",
      "Input 4 : 10 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/11 Parameter model.1.1\n",
      "************\n",
      "143 143 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]/170 onnx::Clip\n",
      "142\n",
      "Input 0 : 142 mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/169 onnx::BatchNormalization model.1.1\n",
      "************\n",
      "144 144 Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/171 onnx::Conv\n",
      "143\n",
      "Input 0 : 143 mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]/170 onnx::Clip model.1.2\n",
      "11\n",
      "Input 1 : 11 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/13 Parameter model.2.0\n",
      "************\n",
      "145 145 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/172 onnx::BatchNormalization\n",
      "144\n",
      "Input 0 : 144 mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/171 onnx::Conv model.2.0\n",
      "12\n",
      "Input 1 : 12 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/14 Parameter model.2.1\n",
      "13\n",
      "Input 2 : 13 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/15 Parameter model.2.1\n",
      "14\n",
      "Input 3 : 14 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/16 Parameter model.2.1\n",
      "15\n",
      "Input 4 : 15 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/17 Parameter model.2.1\n",
      "************\n",
      "146 146 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]/173 onnx::Clip\n",
      "145\n",
      "Input 0 : 145 mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/172 onnx::BatchNormalization model.2.1\n",
      "************\n",
      "147 147 Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/174 onnx::Conv\n",
      "146\n",
      "Input 0 : 146 mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]/173 onnx::Clip model.2.2\n",
      "16\n",
      "Input 1 : 16 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/19 Parameter model.3.0\n",
      "************\n",
      "148 148 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/175 onnx::BatchNormalization\n",
      "147\n",
      "Input 0 : 147 mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/174 onnx::Conv model.3.0\n",
      "17\n",
      "Input 1 : 17 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/20 Parameter model.3.1\n",
      "18\n",
      "Input 2 : 18 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/21 Parameter model.3.1\n",
      "19\n",
      "Input 3 : 19 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/22 Parameter model.3.1\n",
      "20\n",
      "Input 4 : 20 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/23 Parameter model.3.1\n",
      "************\n",
      "149 149 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]/176 onnx::Clip\n",
      "148\n",
      "Input 0 : 148 mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/175 onnx::BatchNormalization model.3.1\n",
      "************\n",
      "150 150 Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/177 onnx::Conv\n",
      "149\n",
      "Input 0 : 149 mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]/176 onnx::Clip model.3.2\n",
      "21\n",
      "Input 1 : 21 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/25 Parameter model.4.0\n",
      "************\n",
      "151 151 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/178 onnx::BatchNormalization\n",
      "150\n",
      "Input 0 : 150 mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/177 onnx::Conv model.4.0\n",
      "22\n",
      "Input 1 : 22 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/26 Parameter model.4.1\n",
      "23\n",
      "Input 2 : 23 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/27 Parameter model.4.1\n",
      "24\n",
      "Input 3 : 24 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/28 Parameter model.4.1\n",
      "25\n",
      "Input 4 : 25 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/29 Parameter model.4.1\n",
      "************\n",
      "152 152 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]/179 onnx::Clip\n",
      "151\n",
      "Input 0 : 151 mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/178 onnx::BatchNormalization model.4.1\n",
      "************\n",
      "153 153 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/180 onnx::Conv\n",
      "152\n",
      "Input 0 : 152 mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]/179 onnx::Clip model.4.2\n",
      "26\n",
      "Input 1 : 26 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/31 Parameter model.5.0\n",
      "************\n",
      "154 154 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/181 onnx::BatchNormalization\n",
      "153\n",
      "Input 0 : 153 mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/180 onnx::Conv model.5.0\n",
      "27\n",
      "Input 1 : 27 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/32 Parameter model.5.1\n",
      "28\n",
      "Input 2 : 28 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/33 Parameter model.5.1\n",
      "29\n",
      "Input 3 : 29 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/34 Parameter model.5.1\n",
      "30\n",
      "Input 4 : 30 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/35 Parameter model.5.1\n",
      "************\n",
      "155 155 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]/182 onnx::Clip\n",
      "154\n",
      "Input 0 : 154 mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/181 onnx::BatchNormalization model.5.1\n",
      "************\n",
      "156 156 Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/183 onnx::Conv\n",
      "155\n",
      "Input 0 : 155 mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]/182 onnx::Clip model.5.2\n",
      "31\n",
      "Input 1 : 31 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/37 Parameter model.6.0\n",
      "************\n",
      "157 157 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/184 onnx::BatchNormalization\n",
      "156\n",
      "Input 0 : 156 mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/183 onnx::Conv model.6.0\n",
      "32\n",
      "Input 1 : 32 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/38 Parameter model.6.1\n",
      "33\n",
      "Input 2 : 33 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/39 Parameter model.6.1\n",
      "34\n",
      "Input 3 : 34 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/40 Parameter model.6.1\n",
      "35\n",
      "Input 4 : 35 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/41 Parameter model.6.1\n",
      "************\n",
      "158 158 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]/185 onnx::Clip\n",
      "157\n",
      "Input 0 : 157 mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/184 onnx::BatchNormalization model.6.1\n",
      "************\n",
      "159 159 Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/186 onnx::Conv\n",
      "158\n",
      "Input 0 : 158 mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]/185 onnx::Clip model.6.2\n",
      "36\n",
      "Input 1 : 36 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/43 Parameter model.7.0\n",
      "************\n",
      "160 160 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/187 onnx::BatchNormalization\n",
      "159\n",
      "Input 0 : 159 mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/186 onnx::Conv model.7.0\n",
      "37\n",
      "Input 1 : 37 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/44 Parameter model.7.1\n",
      "38\n",
      "Input 2 : 38 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/45 Parameter model.7.1\n",
      "39\n",
      "Input 3 : 39 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/46 Parameter model.7.1\n",
      "40\n",
      "Input 4 : 40 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/47 Parameter model.7.1\n",
      "************\n",
      "161 161 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]/188 onnx::Clip\n",
      "160\n",
      "Input 0 : 160 mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/187 onnx::BatchNormalization model.7.1\n",
      "************\n",
      "162 162 Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/189 onnx::Conv\n",
      "161\n",
      "Input 0 : 161 mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]/188 onnx::Clip model.7.2\n",
      "41\n",
      "Input 1 : 41 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/49 Parameter model.8.0\n",
      "************\n",
      "163 163 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/190 onnx::BatchNormalization\n",
      "162\n",
      "Input 0 : 162 mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/189 onnx::Conv model.8.0\n",
      "42\n",
      "Input 1 : 42 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/50 Parameter model.8.1\n",
      "43\n",
      "Input 2 : 43 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/51 Parameter model.8.1\n",
      "44\n",
      "Input 3 : 44 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/52 Parameter model.8.1\n",
      "45\n",
      "Input 4 : 45 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/53 Parameter model.8.1\n",
      "************\n",
      "164 164 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]/191 onnx::Clip\n",
      "163\n",
      "Input 0 : 163 mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/190 onnx::BatchNormalization model.8.1\n",
      "************\n",
      "165 165 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/192 onnx::Conv\n",
      "164\n",
      "Input 0 : 164 mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]/191 onnx::Clip model.8.2\n",
      "46\n",
      "Input 1 : 46 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/55 Parameter model.9.0\n",
      "************\n",
      "166 166 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/193 onnx::BatchNormalization\n",
      "165\n",
      "Input 0 : 165 mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/192 onnx::Conv model.9.0\n",
      "47\n",
      "Input 1 : 47 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/56 Parameter model.9.1\n",
      "48\n",
      "Input 2 : 48 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/57 Parameter model.9.1\n",
      "49\n",
      "Input 3 : 49 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/58 Parameter model.9.1\n",
      "50\n",
      "Input 4 : 50 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/59 Parameter model.9.1\n",
      "************\n",
      "167 167 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]/194 onnx::Clip\n",
      "166\n",
      "Input 0 : 166 mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/193 onnx::BatchNormalization model.9.1\n",
      "************\n",
      "168 168 Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/195 onnx::Conv\n",
      "167\n",
      "Input 0 : 167 mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]/194 onnx::Clip model.9.2\n",
      "51\n",
      "Input 1 : 51 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/61 Parameter model.10.0\n",
      "************\n",
      "169 169 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/196 onnx::BatchNormalization\n",
      "168\n",
      "Input 0 : 168 mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/195 onnx::Conv model.10.0\n",
      "52\n",
      "Input 1 : 52 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/62 Parameter model.10.1\n",
      "53\n",
      "Input 2 : 53 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/63 Parameter model.10.1\n",
      "54\n",
      "Input 3 : 54 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/64 Parameter model.10.1\n",
      "55\n",
      "Input 4 : 55 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/65 Parameter model.10.1\n",
      "************\n",
      "170 170 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]/197 onnx::Clip\n",
      "169\n",
      "Input 0 : 169 mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/196 onnx::BatchNormalization model.10.1\n",
      "************\n",
      "171 171 Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/198 onnx::Conv\n",
      "170\n",
      "Input 0 : 170 mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]/197 onnx::Clip model.10.2\n",
      "56\n",
      "Input 1 : 56 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/67 Parameter model.11.0\n",
      "************\n",
      "172 172 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/199 onnx::BatchNormalization\n",
      "171\n",
      "Input 0 : 171 mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/198 onnx::Conv model.11.0\n",
      "57\n",
      "Input 1 : 57 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/68 Parameter model.11.1\n",
      "58\n",
      "Input 2 : 58 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/69 Parameter model.11.1\n",
      "59\n",
      "Input 3 : 59 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/70 Parameter model.11.1\n",
      "60\n",
      "Input 4 : 60 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/71 Parameter model.11.1\n",
      "************\n",
      "173 173 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]/200 onnx::Clip\n",
      "172\n",
      "Input 0 : 172 mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/199 onnx::BatchNormalization model.11.1\n",
      "************\n",
      "174 174 Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/201 onnx::Conv\n",
      "173\n",
      "Input 0 : 173 mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]/200 onnx::Clip model.11.2\n",
      "61\n",
      "Input 1 : 61 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/73 Parameter model.12.0\n",
      "************\n",
      "175 175 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/202 onnx::BatchNormalization\n",
      "174\n",
      "Input 0 : 174 mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/201 onnx::Conv model.12.0\n",
      "62\n",
      "Input 1 : 62 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/74 Parameter model.12.1\n",
      "63\n",
      "Input 2 : 63 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/75 Parameter model.12.1\n",
      "64\n",
      "Input 3 : 64 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/76 Parameter model.12.1\n",
      "65\n",
      "Input 4 : 65 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/77 Parameter model.12.1\n",
      "************\n",
      "176 176 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]/203 onnx::Clip\n",
      "175\n",
      "Input 0 : 175 mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/202 onnx::BatchNormalization model.12.1\n",
      "************\n",
      "177 177 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/204 onnx::Conv\n",
      "176\n",
      "Input 0 : 176 mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]/203 onnx::Clip model.12.2\n",
      "66\n",
      "Input 1 : 66 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/79 Parameter model.13.0\n",
      "************\n",
      "178 178 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/205 onnx::BatchNormalization\n",
      "177\n",
      "Input 0 : 177 mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/204 onnx::Conv model.13.0\n",
      "67\n",
      "Input 1 : 67 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/80 Parameter model.13.1\n",
      "68\n",
      "Input 2 : 68 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/81 Parameter model.13.1\n",
      "69\n",
      "Input 3 : 69 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/82 Parameter model.13.1\n",
      "70\n",
      "Input 4 : 70 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/83 Parameter model.13.1\n",
      "************\n",
      "179 179 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]/206 onnx::Clip\n",
      "178\n",
      "Input 0 : 178 mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/205 onnx::BatchNormalization model.13.1\n",
      "************\n",
      "180 180 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/207 onnx::Conv\n",
      "179\n",
      "Input 0 : 179 mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]/206 onnx::Clip model.13.2\n",
      "71\n",
      "Input 1 : 71 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/85 Parameter model.14.0\n",
      "************\n",
      "181 181 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/208 onnx::BatchNormalization\n",
      "180\n",
      "Input 0 : 180 mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/207 onnx::Conv model.14.0\n",
      "72\n",
      "Input 1 : 72 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/86 Parameter model.14.1\n",
      "73\n",
      "Input 2 : 73 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/87 Parameter model.14.1\n",
      "74\n",
      "Input 3 : 74 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/88 Parameter model.14.1\n",
      "75\n",
      "Input 4 : 75 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/89 Parameter model.14.1\n",
      "************\n",
      "182 182 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]/209 onnx::Clip\n",
      "181\n",
      "Input 0 : 181 mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/208 onnx::BatchNormalization model.14.1\n",
      "************\n",
      "183 183 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/210 onnx::Conv\n",
      "182\n",
      "Input 0 : 182 mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]/209 onnx::Clip model.14.2\n",
      "76\n",
      "Input 1 : 76 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/91 Parameter model.15.0\n",
      "************\n",
      "184 184 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/211 onnx::BatchNormalization\n",
      "183\n",
      "Input 0 : 183 mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/210 onnx::Conv model.15.0\n",
      "77\n",
      "Input 1 : 77 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/92 Parameter model.15.1\n",
      "78\n",
      "Input 2 : 78 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/93 Parameter model.15.1\n",
      "79\n",
      "Input 3 : 79 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/94 Parameter model.15.1\n",
      "80\n",
      "Input 4 : 80 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/95 Parameter model.15.1\n",
      "************\n",
      "185 185 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]/212 onnx::Clip\n",
      "184\n",
      "Input 0 : 184 mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/211 onnx::BatchNormalization model.15.1\n",
      "************\n",
      "186 186 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/213 onnx::Conv\n",
      "185\n",
      "Input 0 : 185 mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]/212 onnx::Clip model.15.2\n",
      "81\n",
      "Input 1 : 81 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/97 Parameter model.16.0\n",
      "************\n",
      "187 187 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/214 onnx::BatchNormalization\n",
      "186\n",
      "Input 0 : 186 mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/213 onnx::Conv model.16.0\n",
      "82\n",
      "Input 1 : 82 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/98 Parameter model.16.1\n",
      "83\n",
      "Input 2 : 83 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/99 Parameter model.16.1\n",
      "84\n",
      "Input 3 : 84 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/100 Parameter model.16.1\n",
      "85\n",
      "Input 4 : 85 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/101 Parameter model.16.1\n",
      "************\n",
      "188 188 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]/215 onnx::Clip\n",
      "187\n",
      "Input 0 : 187 mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/214 onnx::BatchNormalization model.16.1\n",
      "************\n",
      "189 189 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/216 onnx::Conv\n",
      "188\n",
      "Input 0 : 188 mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]/215 onnx::Clip model.16.2\n",
      "86\n",
      "Input 1 : 86 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/103 Parameter model.17.0\n",
      "************\n",
      "190 190 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/217 onnx::BatchNormalization\n",
      "189\n",
      "Input 0 : 189 mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/216 onnx::Conv model.17.0\n",
      "87\n",
      "Input 1 : 87 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/104 Parameter model.17.1\n",
      "88\n",
      "Input 2 : 88 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/105 Parameter model.17.1\n",
      "89\n",
      "Input 3 : 89 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/106 Parameter model.17.1\n",
      "90\n",
      "Input 4 : 90 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/107 Parameter model.17.1\n",
      "************\n",
      "191 191 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]/218 onnx::Clip\n",
      "190\n",
      "Input 0 : 190 mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/217 onnx::BatchNormalization model.17.1\n",
      "************\n",
      "192 192 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/219 onnx::Conv\n",
      "191\n",
      "Input 0 : 191 mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]/218 onnx::Clip model.17.2\n",
      "91\n",
      "Input 1 : 91 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/109 Parameter model.18.0\n",
      "************\n",
      "193 193 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/220 onnx::BatchNormalization\n",
      "192\n",
      "Input 0 : 192 mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/219 onnx::Conv model.18.0\n",
      "92\n",
      "Input 1 : 92 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/110 Parameter model.18.1\n",
      "93\n",
      "Input 2 : 93 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/111 Parameter model.18.1\n",
      "94\n",
      "Input 3 : 94 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/112 Parameter model.18.1\n",
      "95\n",
      "Input 4 : 95 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/113 Parameter model.18.1\n",
      "************\n",
      "194 194 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]/221 onnx::Clip\n",
      "193\n",
      "Input 0 : 193 mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/220 onnx::BatchNormalization model.18.1\n",
      "************\n",
      "195 195 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/222 onnx::Conv\n",
      "194\n",
      "Input 0 : 194 mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]/221 onnx::Clip model.18.2\n",
      "96\n",
      "Input 1 : 96 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/115 Parameter model.19.0\n",
      "************\n",
      "196 196 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/223 onnx::BatchNormalization\n",
      "195\n",
      "Input 0 : 195 mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/222 onnx::Conv model.19.0\n",
      "97\n",
      "Input 1 : 97 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/116 Parameter model.19.1\n",
      "98\n",
      "Input 2 : 98 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/117 Parameter model.19.1\n",
      "99\n",
      "Input 3 : 99 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/118 Parameter model.19.1\n",
      "100\n",
      "Input 4 : 100 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/119 Parameter model.19.1\n",
      "************\n",
      "197 197 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]/224 onnx::Clip\n",
      "196\n",
      "Input 0 : 196 mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/223 onnx::BatchNormalization model.19.1\n",
      "************\n",
      "198 198 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/225 onnx::Conv\n",
      "197\n",
      "Input 0 : 197 mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]/224 onnx::Clip model.19.2\n",
      "101\n",
      "Input 1 : 101 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/121 Parameter model.20.0\n",
      "************\n",
      "199 199 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/226 onnx::BatchNormalization\n",
      "198\n",
      "Input 0 : 198 mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/225 onnx::Conv model.20.0\n",
      "102\n",
      "Input 1 : 102 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/122 Parameter model.20.1\n",
      "103\n",
      "Input 2 : 103 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/123 Parameter model.20.1\n",
      "104\n",
      "Input 3 : 104 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/124 Parameter model.20.1\n",
      "105\n",
      "Input 4 : 105 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/125 Parameter model.20.1\n",
      "************\n",
      "200 200 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]/227 onnx::Clip\n",
      "199\n",
      "Input 0 : 199 mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/226 onnx::BatchNormalization model.20.1\n",
      "************\n",
      "201 201 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/228 onnx::Conv\n",
      "200\n",
      "Input 0 : 200 mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]/227 onnx::Clip model.20.2\n",
      "106\n",
      "Input 1 : 106 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/127 Parameter model.21.0\n",
      "************\n",
      "202 202 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/229 onnx::BatchNormalization\n",
      "201\n",
      "Input 0 : 201 mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/228 onnx::Conv model.21.0\n",
      "107\n",
      "Input 1 : 107 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/128 Parameter model.21.1\n",
      "108\n",
      "Input 2 : 108 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/129 Parameter model.21.1\n",
      "109\n",
      "Input 3 : 109 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/130 Parameter model.21.1\n",
      "110\n",
      "Input 4 : 110 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/131 Parameter model.21.1\n",
      "************\n",
      "203 203 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]/230 onnx::Clip\n",
      "202\n",
      "Input 0 : 202 mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/229 onnx::BatchNormalization model.21.1\n",
      "************\n",
      "204 204 Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/231 onnx::Conv\n",
      "203\n",
      "Input 0 : 203 mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]/230 onnx::Clip model.21.2\n",
      "111\n",
      "Input 1 : 111 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/133 Parameter model.22.0\n",
      "************\n",
      "205 205 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/232 onnx::BatchNormalization\n",
      "204\n",
      "Input 0 : 204 mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/231 onnx::Conv model.22.0\n",
      "112\n",
      "Input 1 : 112 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/134 Parameter model.22.1\n",
      "113\n",
      "Input 2 : 113 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/135 Parameter model.22.1\n",
      "114\n",
      "Input 3 : 114 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/136 Parameter model.22.1\n",
      "115\n",
      "Input 4 : 115 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/137 Parameter model.22.1\n",
      "************\n",
      "206 206 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]/233 onnx::Clip\n",
      "205\n",
      "Input 0 : 205 mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/232 onnx::BatchNormalization model.22.1\n",
      "************\n",
      "207 207 Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/234 onnx::Conv\n",
      "206\n",
      "Input 0 : 206 mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]/233 onnx::Clip model.22.2\n",
      "116\n",
      "Input 1 : 116 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/139 Parameter model.23.0\n",
      "************\n",
      "208 208 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/235 onnx::BatchNormalization\n",
      "207\n",
      "Input 0 : 207 mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/234 onnx::Conv model.23.0\n",
      "117\n",
      "Input 1 : 117 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/140 Parameter model.23.1\n",
      "118\n",
      "Input 2 : 118 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/141 Parameter model.23.1\n",
      "119\n",
      "Input 3 : 119 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/142 Parameter model.23.1\n",
      "120\n",
      "Input 4 : 120 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/143 Parameter model.23.1\n",
      "************\n",
      "209 209 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]/236 onnx::Clip\n",
      "208\n",
      "Input 0 : 208 mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/235 onnx::BatchNormalization model.23.1\n",
      "************\n",
      "210 210 Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237 onnx::Conv\n",
      "209\n",
      "Input 0 : 209 mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]/236 onnx::Clip model.23.2\n",
      "121\n",
      "Input 1 : 121 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/145 Parameter model.24.0\n",
      "************\n",
      "211 211 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/238 onnx::BatchNormalization\n",
      "210\n",
      "Input 0 : 210 mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237 onnx::Conv model.24.0\n",
      "122\n",
      "Input 1 : 122 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/146 Parameter model.24.1\n",
      "123\n",
      "Input 2 : 123 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/147 Parameter model.24.1\n",
      "124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 3 : 124 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/148 Parameter model.24.1\n",
      "125\n",
      "Input 4 : 125 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/149 Parameter model.24.1\n",
      "************\n",
      "212 212 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]/239 onnx::Clip\n",
      "211\n",
      "Input 0 : 211 mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/238 onnx::BatchNormalization model.24.1\n",
      "************\n",
      "213 213 Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/240 onnx::Conv\n",
      "212\n",
      "Input 0 : 212 mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]/239 onnx::Clip model.24.2\n",
      "126\n",
      "Input 1 : 126 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/151 Parameter model.25.0\n",
      "************\n",
      "214 214 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/241 onnx::BatchNormalization\n",
      "213\n",
      "Input 0 : 213 mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/240 onnx::Conv model.25.0\n",
      "127\n",
      "Input 1 : 127 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/152 Parameter model.25.1\n",
      "128\n",
      "Input 2 : 128 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/153 Parameter model.25.1\n",
      "129\n",
      "Input 3 : 129 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/154 Parameter model.25.1\n",
      "130\n",
      "Input 4 : 130 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/155 Parameter model.25.1\n",
      "************\n",
      "215 215 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]/242 onnx::Clip\n",
      "214\n",
      "Input 0 : 214 mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/241 onnx::BatchNormalization model.25.1\n",
      "************\n",
      "216 216 Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/243 onnx::Conv\n",
      "215\n",
      "Input 0 : 215 mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]/242 onnx::Clip model.25.2\n",
      "131\n",
      "Input 1 : 131 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/157 Parameter model.26.0\n",
      "************\n",
      "217 217 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/244 onnx::BatchNormalization\n",
      "216\n",
      "Input 0 : 216 mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/243 onnx::Conv model.26.0\n",
      "132\n",
      "Input 1 : 132 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/158 Parameter model.26.1\n",
      "133\n",
      "Input 2 : 133 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/159 Parameter model.26.1\n",
      "134\n",
      "Input 3 : 134 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/160 Parameter model.26.1\n",
      "135\n",
      "Input 4 : 135 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/161 Parameter model.26.1\n",
      "************\n",
      "218 218 ReLU6()\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]/245 onnx::Clip\n",
      "217\n",
      "Input 0 : 217 mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/244 onnx::BatchNormalization model.26.1\n",
      "************\n",
      "220 220 AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "mobilenet_real/Sequential[model]/AvgPool2d[27]/247 onnx::AveragePool\n",
      "219\n",
      "Input 0 : 219 mobilenet_real/Sequential[model]/AvgPool2d[27]/246 onnx::Pad model.27\n",
      "************\n",
      "222 222 mobilenet_real(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "mobilenet_real/249 onnx::Reshape\n",
      "220\n",
      "Input 0 : 220 mobilenet_real/Sequential[model]/AvgPool2d[27]/247 onnx::AveragePool model.27\n",
      "221\n",
      "Input 1 : 221 mobilenet_real/248 onnx::Constant \n",
      "************\n",
      "224 224 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Linear[fc]/251 onnx::Gemm\n",
      "222\n",
      "Input 0 : 222 mobilenet_real/249 onnx::Reshape \n",
      "223\n",
      "Input 1 : 223 mobilenet_real/Linear[fc]/250 onnx::Transpose fc\n",
      "137\n",
      "Input 2 : 137 mobilenet_real/Linear[fc]/164 Parameter fc\n"
     ]
    }
   ],
   "source": [
    "m = OrderedDict(model.named_modules())\n",
    "\n",
    "for i,node in enumerate(g.node):\n",
    "    if node.op in _IO_LIST:\n",
    "        print('************')\n",
    "        print(node.name)\n",
    "        print(node.op)\n",
    "    if node.op in _OP_LIST:\n",
    "        print('************')\n",
    "        v = get_model_name(node.name)\n",
    "        print(i,get_model_id(node.name,g), m[v])\n",
    "        print(node.name,node.op)\n",
    "        for j, x in enumerate(node.input):\n",
    "            id = get_model_id(x,g)\n",
    "            print('Input',j,':',id, x, g.node[id].op, get_model_name(x))\n",
    "            if g.node[id].op in _IO_LIST:\n",
    "            elif g.node[id].op in _OP_LIST:\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new Quantop Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "input/0\n",
      "IO Node\n",
      "************\n",
      "138 138 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/165 onnx::Conv\n",
      "************\n",
      "139 139 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/BatchNorm2d[1]/166 onnx::BatchNormalization\n",
      "************\n",
      "140 140 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[0]/ReLU6[2]/167 onnx::Clip\n",
      "************\n",
      "141 141 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/Conv2d[0]/168 onnx::Conv\n",
      "************\n",
      "142 142 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/BatchNorm2d[1]/169 onnx::BatchNormalization\n",
      "************\n",
      "143 143 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[1]/ReLU6[2]/170 onnx::Clip\n",
      "************\n",
      "144 144 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/Conv2d[0]/171 onnx::Conv\n",
      "************\n",
      "145 145 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/BatchNorm2d[1]/172 onnx::BatchNormalization\n",
      "************\n",
      "146 146 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[2]/ReLU6[2]/173 onnx::Clip\n",
      "************\n",
      "147 147 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/Conv2d[0]/174 onnx::Conv\n",
      "************\n",
      "148 148 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/BatchNorm2d[1]/175 onnx::BatchNormalization\n",
      "************\n",
      "149 149 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[3]/ReLU6[2]/176 onnx::Clip\n",
      "************\n",
      "150 150 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/Conv2d[0]/177 onnx::Conv\n",
      "************\n",
      "151 151 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/BatchNorm2d[1]/178 onnx::BatchNormalization\n",
      "************\n",
      "152 152 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[4]/ReLU6[2]/179 onnx::Clip\n",
      "************\n",
      "153 153 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/Conv2d[0]/180 onnx::Conv\n",
      "************\n",
      "154 154 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/BatchNorm2d[1]/181 onnx::BatchNormalization\n",
      "************\n",
      "155 155 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[5]/ReLU6[2]/182 onnx::Clip\n",
      "************\n",
      "156 156 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/Conv2d[0]/183 onnx::Conv\n",
      "************\n",
      "157 157 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/BatchNorm2d[1]/184 onnx::BatchNormalization\n",
      "************\n",
      "158 158 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[6]/ReLU6[2]/185 onnx::Clip\n",
      "************\n",
      "159 159 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/Conv2d[0]/186 onnx::Conv\n",
      "************\n",
      "160 160 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/BatchNorm2d[1]/187 onnx::BatchNormalization\n",
      "************\n",
      "161 161 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[7]/ReLU6[2]/188 onnx::Clip\n",
      "************\n",
      "162 162 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/Conv2d[0]/189 onnx::Conv\n",
      "************\n",
      "163 163 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/BatchNorm2d[1]/190 onnx::BatchNormalization\n",
      "************\n",
      "164 164 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[8]/ReLU6[2]/191 onnx::Clip\n",
      "************\n",
      "165 165 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/Conv2d[0]/192 onnx::Conv\n",
      "************\n",
      "166 166 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/BatchNorm2d[1]/193 onnx::BatchNormalization\n",
      "************\n",
      "167 167 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[9]/ReLU6[2]/194 onnx::Clip\n",
      "************\n",
      "168 168 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/Conv2d[0]/195 onnx::Conv\n",
      "************\n",
      "169 169 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/BatchNorm2d[1]/196 onnx::BatchNormalization\n",
      "************\n",
      "170 170 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[10]/ReLU6[2]/197 onnx::Clip\n",
      "************\n",
      "171 171 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/Conv2d[0]/198 onnx::Conv\n",
      "************\n",
      "172 172 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/BatchNorm2d[1]/199 onnx::BatchNormalization\n",
      "************\n",
      "173 173 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[11]/ReLU6[2]/200 onnx::Clip\n",
      "************\n",
      "174 174 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/Conv2d[0]/201 onnx::Conv\n",
      "************\n",
      "175 175 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/BatchNorm2d[1]/202 onnx::BatchNormalization\n",
      "************\n",
      "176 176 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[12]/ReLU6[2]/203 onnx::Clip\n",
      "************\n",
      "177 177 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/Conv2d[0]/204 onnx::Conv\n",
      "************\n",
      "178 178 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/BatchNorm2d[1]/205 onnx::BatchNormalization\n",
      "************\n",
      "179 179 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[13]/ReLU6[2]/206 onnx::Clip\n",
      "************\n",
      "180 180 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/Conv2d[0]/207 onnx::Conv\n",
      "************\n",
      "181 181 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/BatchNorm2d[1]/208 onnx::BatchNormalization\n",
      "************\n",
      "182 182 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[14]/ReLU6[2]/209 onnx::Clip\n",
      "************\n",
      "183 183 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/Conv2d[0]/210 onnx::Conv\n",
      "************\n",
      "184 184 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/BatchNorm2d[1]/211 onnx::BatchNormalization\n",
      "************\n",
      "185 185 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[15]/ReLU6[2]/212 onnx::Clip\n",
      "************\n",
      "186 186 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/Conv2d[0]/213 onnx::Conv\n",
      "************\n",
      "187 187 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/BatchNorm2d[1]/214 onnx::BatchNormalization\n",
      "************\n",
      "188 188 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[16]/ReLU6[2]/215 onnx::Clip\n",
      "************\n",
      "189 189 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/Conv2d[0]/216 onnx::Conv\n",
      "************\n",
      "190 190 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/BatchNorm2d[1]/217 onnx::BatchNormalization\n",
      "************\n",
      "191 191 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[17]/ReLU6[2]/218 onnx::Clip\n",
      "************\n",
      "192 192 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/Conv2d[0]/219 onnx::Conv\n",
      "************\n",
      "193 193 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/BatchNorm2d[1]/220 onnx::BatchNormalization\n",
      "************\n",
      "194 194 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[18]/ReLU6[2]/221 onnx::Clip\n",
      "************\n",
      "195 195 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/Conv2d[0]/222 onnx::Conv\n",
      "************\n",
      "196 196 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/BatchNorm2d[1]/223 onnx::BatchNormalization\n",
      "************\n",
      "197 197 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[19]/ReLU6[2]/224 onnx::Clip\n",
      "************\n",
      "198 198 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/Conv2d[0]/225 onnx::Conv\n",
      "************\n",
      "199 199 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/BatchNorm2d[1]/226 onnx::BatchNormalization\n",
      "************\n",
      "200 200 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[20]/ReLU6[2]/227 onnx::Clip\n",
      "************\n",
      "201 201 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/Conv2d[0]/228 onnx::Conv\n",
      "************\n",
      "202 202 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/BatchNorm2d[1]/229 onnx::BatchNormalization\n",
      "************\n",
      "203 203 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[21]/ReLU6[2]/230 onnx::Clip\n",
      "************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 204 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/Conv2d[0]/231 onnx::Conv\n",
      "************\n",
      "205 205 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/BatchNorm2d[1]/232 onnx::BatchNormalization\n",
      "************\n",
      "206 206 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[22]/ReLU6[2]/233 onnx::Clip\n",
      "************\n",
      "207 207 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/Conv2d[0]/234 onnx::Conv\n",
      "************\n",
      "208 208 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/BatchNorm2d[1]/235 onnx::BatchNormalization\n",
      "************\n",
      "209 209 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[23]/ReLU6[2]/236 onnx::Clip\n",
      "************\n",
      "210 210 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237 onnx::Conv\n",
      "************\n",
      "211 211 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/BatchNorm2d[1]/238 onnx::BatchNormalization\n",
      "************\n",
      "212 212 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[24]/ReLU6[2]/239 onnx::Clip\n",
      "************\n",
      "213 213 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/Conv2d[0]/240 onnx::Conv\n",
      "************\n",
      "214 214 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/BatchNorm2d[1]/241 onnx::BatchNormalization\n",
      "************\n",
      "215 215 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[25]/ReLU6[2]/242 onnx::Clip\n",
      "************\n",
      "216 216 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/Conv2d[0]/243 onnx::Conv\n",
      "************\n",
      "217 217 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/BatchNorm2d[1]/244 onnx::BatchNormalization\n",
      "************\n",
      "218 218 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/Sequential[26]/ReLU6[2]/245 onnx::Clip\n",
      "************\n",
      "220 220 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Sequential[model]/AvgPool2d[27]/247 onnx::AveragePool\n",
      "************\n",
      "222 222 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/249 onnx::Reshape\n",
      "Else name =  \n",
      "************\n",
      "224 224 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "mobilenet_real/Linear[fc]/251 onnx::Gemm\n",
      "This is the quantized network: \n",
      "mobilenet_real(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "***** Layer 0 **********\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "138\n",
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 1 **********\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "141\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 2 **********\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "144\n",
      "Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 3 **********\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "147\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 4 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "150\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 5 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "153\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 6 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "156\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 7 **********\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "159\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 8 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "162\n",
      "Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 9 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "165\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 10 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "168\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 11 **********\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "171\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 12 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "174\n",
      "Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 13 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "177\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 14 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "180\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 15 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "183\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 16 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "186\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 17 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "189\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 18 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "192\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 19 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "195\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 20 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "198\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 21 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "201\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 22 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "204\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 23 **********\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "207\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 24 **********\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "210\n",
      "Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 25 **********\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "213\n",
      "Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 26 **********\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "216\n",
      "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)\n",
      "***** Layer 27 **********\n",
      "None\n",
      "224\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = OrderedDict(model.named_modules())\n",
    "_OP_LIST = ['onnx::Gemm','onnx::BatchNormalization','onnx::Conv','onnx::Clip','onnx::AveragePool','onnx::Reshape']\n",
    "_IO_LIST = ['IO Node']\n",
    "\n",
    "quant_type =  'PerLayerAsymPACT'\n",
    "weight_bits= 8\n",
    "bias_bits = 8\n",
    "batch_fold_type = 'folding_weights', \n",
    "batch_fold_delay = 0 \n",
    "act_bits = 8\n",
    "add_config = [] \n",
    "\n",
    "\n",
    "# generate quantized model\n",
    "deployment_model = copy.deepcopy(model)\n",
    "param_to_quantize = []\n",
    "batch_fold = False\n",
    "\n",
    "\n",
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "\n",
    "        \n",
    "def find_conv_node(node):\n",
    "    ret = 0\n",
    "    for j, x in enumerate(node.input):\n",
    "        id = get_model_id(x,g)\n",
    "        n = g.node[id]\n",
    "        if n.op in _IO_LIST:\n",
    "            pass\n",
    "        elif n.op in _OP_LIST:\n",
    "            if n.op in ['onnx::Gemm','onnx::Conv']:\n",
    "                ret = id\n",
    "            else:\n",
    "                ret = find_conv_node(n)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "last_layer = None\n",
    "modules_quant = [ ]           \n",
    "\n",
    "replace_graph = {}\n",
    "###################################################\n",
    "##### Mapping the graph ###########################\n",
    "###################################################\n",
    "for i,node in enumerate(g.node):\n",
    "    \n",
    "    if node.op in _IO_LIST:\n",
    "        print('************')\n",
    "        print(node.name)\n",
    "        print(node.op)\n",
    "        \n",
    "    if node.op in _OP_LIST:\n",
    "        print('************')\n",
    "        name = get_model_name(node.name)\n",
    "        module = m[name]\n",
    "        idx = get_model_id(node.name,g)\n",
    "        print(i, idx, m[v])\n",
    "        print(node.name,node.op)\n",
    "        \n",
    "        if node.op in ['onnx::Gemm','onnx::Conv']:\n",
    "            layer_quant_descr = {}\n",
    "            # saving per-node characteristics\n",
    "            layer_quant_descr['w_bits'] = weight_bits\n",
    "            layer_quant_descr['fold_type'] = batch_fold_type\n",
    "\n",
    "            temp = getattr(submodel, 'quant_type', quant_type)\n",
    "            if temp in _Availble_Quant_Type :\n",
    "                layer_quant_descr['quant_type'] = temp\n",
    "            else:\n",
    "                print('Type of quantization not recognized')\n",
    "\n",
    "            layer_quant_descr['conv'] = module\n",
    "            layer_quant_descr['weight'] = module.weight.data.clone()\n",
    "            layer_quant_descr['w_clip'] = None\n",
    "            layer_quant_descr['id'] = idx\n",
    "            layer_quant_descr['leaf'] = 0\n",
    "\n",
    "            # import per-layer config from external config file\n",
    "            idx_layer = len(param_to_quantize)\n",
    "            for item_dict in add_config:\n",
    "                if item_dict['layer'] == idx_layer:\n",
    "                    if 'w_bits' in item_dict.keys():\n",
    "                        layer_quant_descr['w_bits'] = item_dict['w_bits'] \n",
    "\n",
    "                    if 'fold_type' in item_dict.keys():\n",
    "                        layer_quant_descr['fold_type'] = item_dict['fold_type'] \n",
    "\n",
    "                    if 'quant_type' in item_dict.keys():\n",
    "                        layer_quant_descr['quant_type'] = item_dict['quant_type'] \n",
    "\n",
    "            if module.bias is None:\n",
    "                layer_quant_descr['bias'] = False\n",
    "            else:\n",
    "                layer_quant_descr['bias'] = module.bias.data.clone()\n",
    "            layer_quant_descr['bias_bits'] = bias_bits\n",
    "            layer_quant_descr['batch_norm'] = None\n",
    "            layer_quant_descr['act'] = None\n",
    "            layer_quant_descr['quant_act'] = None\n",
    "\n",
    "            # append into deployment graph\n",
    "            quant_layer = copy.deepcopy(module)\n",
    "            modules_quant.append(quant_layer)\n",
    "            layer_quant_descr['quant_conv'] = quant_layer\n",
    "\n",
    "            #PACT needs extra parameters for learning quantization range\n",
    "            if layer_quant_descr['quant_type'] == 'PerLayerAsymPACT':\n",
    "                layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_min_thr'] = nn.Parameter( module.weight.data.min().cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "            elif layer_quant_descr['quant_type'] == 'PerLayerSymPACT':\n",
    "                layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "            elif layer_quant_descr['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "                weight = module.weight.data\n",
    "                max_weight,_ = weight.reshape(weight.size(0), -1).max(1)\n",
    "                min_weight,_ = weight.reshape(weight.size(0), -1).min(1)\n",
    "                layer_quant_descr['w_max_thr'] = nn.Parameter( max_weight.cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_min_thr'] = nn.Parameter( min_weight.cuda(),requires_grad=True )\n",
    "                layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "                layer_quant_descr['w_min_mat'] = None\n",
    "                layer_quant_descr['w_max_mat'] = None\n",
    "                if type(module) == nn.Linear:\n",
    "                    print('PerLayerAsymPACT chennel with fixed batch')\n",
    "                    out_ch = weight.size(0)\n",
    "                    quant_act = ScaledClippedLinearQuantizationChannelBias(out_ch, clip_val=False, bias=False)\n",
    "                    layer_quant_descr['quant_act'] = quant_act\n",
    "                    modules_quant.append(quant_act)\n",
    "\n",
    "            # append the latest node\n",
    "            param_to_quantize.append(layer_quant_descr)\n",
    "            replace_graph[name] = [module, module]\n",
    "\n",
    "        elif node.op in ['onnx::BatchNormalization']:\n",
    "            id_layer = find_conv_node(node)\n",
    "            for item in param_to_quantize:\n",
    "                if item['id'] == id_layer:\n",
    "                    item['batch_norm'] = module\n",
    "                    break\n",
    "                    \n",
    "            replace_graph[name] = [module, None]\n",
    "\n",
    "        elif node.op in ['onnx::AveragePool']:\n",
    "            modules_quant.append(module) # temporary - this should me merged into or previous layer\n",
    "            replace_graph[name] = [module, module]\n",
    "\n",
    "        elif node.op in ['onnx::Clip']:\n",
    "            \n",
    "            id_layer = find_conv_node(node)\n",
    "            for item in param_to_quantize:\n",
    "                if item['id'] == id_layer:\n",
    "                    last_layer = item\n",
    "                    break\n",
    "\n",
    "            last_layer['act'] = module\n",
    "\n",
    "            # quantized activations\n",
    "            if type(module) in [ClippedLinearQuantization, LearnedClippedLinearQuantization, QuantActive, nn.ReLU6]:\n",
    "\n",
    "                act_bits = 8\n",
    "                #check if number of bits need to be changed\n",
    "                for item_dict in add_config:\n",
    "                    if item_dict['layer'] == idx_layer:\n",
    "                        if 'a_bits' in item_dict.keys():\n",
    "                            act_bits = item_dict['a_bits'] \n",
    "                            module.num_bits = act_bits                                 \n",
    "\n",
    "                if last_layer['fold_type'] == 'folding_thresh':\n",
    "                    quant_act = ScaledThresholdsQuantization4d()\n",
    "                    quant_act.num_bits = act_bits\n",
    "                    quant_act.n_thresholds = 2**act_bits -1\n",
    "\n",
    "                elif last_layer['fold_type'] == 'ICN':\n",
    "                    out_ch = last_layer['conv'].out_channels\n",
    "                    quant_act = ScaledClippedLinearQuantizationChannel(out_ch,clip_val=2**act_bits -1)\n",
    "\n",
    "                else:\n",
    "                    quant_act = ScaledClippedLinearQuantization(clip_val=2**act_bits -1)\n",
    "\n",
    "                last_layer['act_o_bits'] = act_bits\n",
    "\n",
    "            else:\n",
    "                print('Supported activation layer but no method is here yet!')\n",
    "\n",
    "\n",
    "            replace_graph[name] = [LearnedClippedLinearQuantization(init_act_clip_val=6,num_bits=act_bits), quant_act]\n",
    "\n",
    "            last_layer['quant_act'] = quant_act\n",
    "\n",
    "            modules_quant.append(quant_act)\n",
    "            \n",
    "        else:\n",
    "            print('Else name = ', name)\n",
    "            replace_graph[name] = [module, module]\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "n_quantize_layers = len(param_to_quantize)\n",
    "\n",
    "print('This is the quantized network: ')\n",
    "print(deployment_model)\n",
    "for i,item in enumerate(param_to_quantize):\n",
    "    print('***** Layer', i,'**********')\n",
    "    print(item['batch_norm'])\n",
    "    print(item['id'])\n",
    "    print(item['quant_conv'])\n",
    "    print(item['quant_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': [mobilenet_real(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (13): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (14): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (15): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (16): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (17): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (18): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (19): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (20): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (21): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (22): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (23): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (24): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (25): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (26): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  ), mobilenet_real(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (11): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (13): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (14): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (15): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (16): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (17): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (18): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (19): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (20): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (21): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (22): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (23): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (24): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (25): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (26): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6()\n",
       "      )\n",
       "      (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )],\n",
       " 'fc': [Linear(in_features=1024, out_features=1000, bias=True),\n",
       "  Linear(in_features=1024, out_features=1000, bias=True)],\n",
       " 'model.0.0': [Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "  Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)],\n",
       " 'model.0.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.0.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.1.0': [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False),\n",
       "  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)],\n",
       " 'model.1.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.1.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.10.0': [Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.10.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.10.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.11.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False),\n",
       "  Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)],\n",
       " 'model.11.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.11.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.12.0': [Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.12.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.12.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.13.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.13.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.13.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.14.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.14.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.14.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.15.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.15.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.15.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.16.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.16.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.16.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.17.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.17.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.17.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.18.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.18.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.18.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.19.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.19.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.19.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.2.0': [Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.2.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.2.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.20.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.20.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.20.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.21.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.21.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.21.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.22.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.22.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.22.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.23.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False),\n",
       "  Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)],\n",
       " 'model.23.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.23.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.24.0': [Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.24.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.24.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.25.0': [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False),\n",
       "  Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)],\n",
       " 'model.25.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.25.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.26.0': [Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.26.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.26.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.27': [AvgPool2d(kernel_size=7, stride=7, padding=0),\n",
       "  AvgPool2d(kernel_size=7, stride=7, padding=0)],\n",
       " 'model.3.0': [Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False),\n",
       "  Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)],\n",
       " 'model.3.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.3.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.4.0': [Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.4.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.4.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.5.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False),\n",
       "  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)],\n",
       " 'model.5.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.5.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.6.0': [Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.6.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.6.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.7.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False),\n",
       "  Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)],\n",
       " 'model.7.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.7.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.8.0': [Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "  Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)],\n",
       " 'model.8.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.8.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)],\n",
       " 'model.9.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False),\n",
       "  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)],\n",
       " 'model.9.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  None],\n",
       " 'model.9.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "  tensor([6.], requires_grad=True), inplace),\n",
       "  ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-582fb400180b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_sub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model.0'"
     ]
    }
   ],
   "source": [
    "fake_model = model\n",
    "deployment_model = copy.deepcopy(model)\n",
    "\n",
    "for name_sub,submodel in model.named_children():\n",
    "    # map the sub-graph \n",
    "    modules_quant = [] \n",
    "    modules_fake = []\n",
    "    for name, module in submodel.named_modules():\n",
    "        if has_children(module) is False:\n",
    "            if name is not '':\n",
    "                n = name_sub+'.'+name\n",
    "            else:\n",
    "                n = name_sub\n",
    "            v = replace_graph[n]\n",
    "            fake = v[0]\n",
    "            other = v[1]\n",
    "            if fake is not None:\n",
    "                modules_fake.append(fake)\n",
    "            \n",
    "            if other is not None:\n",
    "                modules_quant.append(other)\n",
    "            \n",
    "    \n",
    "    fake_model._modules[name_sub] = nn.Sequential(*modules_fake)\n",
    "    deployment_model._modules[name_sub] = nn.Sequential(*modules_quant)\n",
    "\n",
    "\n",
    "print(deployment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model.0.0': [Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
       "   Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]},\n",
       " {'model.0.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.0.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.1.0': [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False),\n",
       "   Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)]},\n",
       " {'model.1.1': [BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.1.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.2.0': [Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.2.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.2.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.3.0': [Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False),\n",
       "   Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)]},\n",
       " {'model.3.1': [BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.3.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.4.0': [Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.4.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.4.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.5.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False),\n",
       "   Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)]},\n",
       " {'model.5.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.5.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.6.0': [Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.6.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.6.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.7.0': [Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False),\n",
       "   Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)]},\n",
       " {'model.7.1': [BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.7.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.8.0': [Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.8.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.8.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.9.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False),\n",
       "   Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)]},\n",
       " {'model.9.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.9.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.10.0': [Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.10.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.10.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.11.0': [Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False),\n",
       "   Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)]},\n",
       " {'model.11.1': [BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.11.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.12.0': [Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.12.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.12.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.13.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.13.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.13.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.14.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.14.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.14.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.15.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.15.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.15.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.16.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.16.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.16.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.17.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.17.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.17.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.18.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.18.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.18.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.19.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.19.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.19.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.20.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.20.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.20.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.21.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.21.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.21.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.22.0': [Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.22.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.22.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.23.0': [Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False),\n",
       "   Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)]},\n",
       " {'model.23.1': [BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.23.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.24.0': [Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.24.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.24.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.25.0': [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False),\n",
       "   Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)]},\n",
       " {'model.25.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.25.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.26.0': [Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       "   Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)]},\n",
       " {'model.26.1': [BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "   None]},\n",
       " {'model.26.2': [LearnedClippedLinearQuantization(num_bits=8, clip_val=Parameter containing:\n",
       "   tensor([6.], requires_grad=True), inplace),\n",
       "   ScaledClippedLinearQuantization(M0=1, N0=1, clip_val=255, inplace)]},\n",
       " {'model.27': [AvgPool2d(kernel_size=7, stride=7, padding=0),\n",
       "   AvgPool2d(kernel_size=7, stride=7, padding=0)]},\n",
       " {'fc': [Linear(in_features=1024, out_features=1000, bias=True),\n",
       "   Linear(in_features=1024, out_features=1000, bias=True)]}]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU6()\n",
      "AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name_sub,submodel in model.named_children():\n",
    "    # map the sub-graph \n",
    "    for name, module in submodel.named_modules():\n",
    "        if has_children(module) is False:\n",
    "            print(module)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be an iterable, not mobilenet_real",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-ab779122673a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: type object argument after * must be an iterable, not mobilenet_real"
     ]
    }
   ],
   "source": [
    "list(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Folding:  False Batch Folding Delay:  0 Type ('folding_weights',)\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'has_children' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-369a4149dbc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# map the sub-graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhas_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mlayer_quant_descr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'has_children' is not defined"
     ]
    }
   ],
   "source": [
    "from models.quantized_modules import ABC_MxNBinConv, ABC_binarize, Quantize, QuantActive, LinQuantActive, BinActive\n",
    "from models.linear_quantized_modules import ClippedLinearQuantization, LearnedClippedLinearQuantization, ScaledClippedLinearQuantization, ScaledThresholdsQuantization4d, \\\n",
    "                                            Conv2d_SAME, ScaledClippedLinearQuantizationChannel, ScaledClippedLinearQuantizationChannelBias\n",
    "\n",
    "_Availble_Quant_Type = ['BNN', 'XNORNET', 'PerLayerAsymMinMax', 'PerLayerSymPACT', 'PerLayerAsymPACT', 'PerChannelsAsymMinMax', None]\n",
    "_Supported_Activ_Funct = [nn.ReLU6, nn.ReLU, ClippedLinearQuantization, LearnedClippedLinearQuantization, BinActive]\n",
    "\n",
    "\n",
    "\n",
    "quant_type =  'PerLayerAsymPACT'\n",
    "weight_bits= 8\n",
    "bias_bits = 8\n",
    "batch_fold_type = 'folding_weights', \n",
    "batch_fold_delay = 0 \n",
    "act_bits = 8\n",
    "add_config = [] \n",
    "\n",
    "\n",
    "# generate quantized model\n",
    "deployment_model = copy.deepcopy(model)\n",
    "param_to_quantize = []\n",
    "batch_fold = False\n",
    "\n",
    "print('Batch Folding: ', batch_fold, 'Batch Folding Delay: ', batch_fold_delay, 'Type', batch_fold_type )\n",
    "\n",
    "#start\n",
    "last_layer = None\n",
    "modules_quant = [ ]\n",
    "\n",
    "print(add_config)\n",
    "\n",
    "\n",
    "\n",
    "for name_sub,submodel in model.named_children():\n",
    "\n",
    "    # map the sub-graph \n",
    "    for name, module in submodel.named_modules():\n",
    "        if has_children(module) is False:\n",
    "\n",
    "            layer_quant_descr = {}\n",
    "\n",
    "            if type(module) in [nn.Conv2d, nn.Linear, Conv2d_SAME]:\n",
    "\n",
    "                # saving per-node characteristics\n",
    "                layer_quant_descr['name'] = name\n",
    "                layer_quant_descr['w_bits'] = weight_bits\n",
    "                layer_quant_descr['fold_type'] = batch_fold_type\n",
    "\n",
    "                temp = getattr(submodel, 'quant_type', quant_type)\n",
    "                if temp in _Availble_Quant_Type :\n",
    "                    layer_quant_descr['quant_type'] = temp\n",
    "                else:\n",
    "                    print('Type of quantization not recognized')\n",
    "\n",
    "                layer_quant_descr['conv'] = module\n",
    "                layer_quant_descr['weight'] = module.weight.data.clone()\n",
    "                layer_quant_descr['w_clip'] = None\n",
    "\n",
    "                # import per-layer config from external config file\n",
    "                idx_layer = len(param_to_quantize)\n",
    "                for item_dict in add_config:\n",
    "                    if item_dict['layer'] == idx_layer:\n",
    "                        if 'w_bits' in item_dict.keys():\n",
    "                            layer_quant_descr['w_bits'] = item_dict['w_bits'] \n",
    "\n",
    "                        if 'fold_type' in item_dict.keys():\n",
    "                            layer_quant_descr['fold_type'] = item_dict['fold_type'] \n",
    "\n",
    "                        if 'quant_type' in item_dict.keys():\n",
    "                            layer_quant_descr['quant_type'] = item_dict['quant_type'] \n",
    "\n",
    "                if module.bias is None:\n",
    "                    layer_quant_descr['bias'] = False\n",
    "                else:\n",
    "                    layer_quant_descr['bias'] = module.bias.data.clone()\n",
    "                layer_quant_descr['bias_bits'] = bias_bits\n",
    "                layer_quant_descr['batch_norm'] = None\n",
    "                layer_quant_descr['act'] = None\n",
    "                layer_quant_descr['quant_act'] = None\n",
    "\n",
    "                # append into deployment graph\n",
    "                quant_layer = copy.deepcopy(module)\n",
    "                modules_quant.append(quant_layer)\n",
    "                layer_quant_descr['quant_conv'] = quant_layer\n",
    "\n",
    "                #PACT needs extra parameters for learning quantization range\n",
    "                if layer_quant_descr['quant_type'] == 'PerLayerAsymPACT':\n",
    "                    layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_min_thr'] = nn.Parameter( module.weight.data.min().cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                    layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "                elif layer_quant_descr['quant_type'] == 'PerLayerSymPACT':\n",
    "                    layer_quant_descr['w_max_thr'] = nn.Parameter( module.weight.data.max().cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "\n",
    "                elif layer_quant_descr['quant_type'] == 'PerChannelsAsymMinMax':\n",
    "                    weight = module.weight.data\n",
    "                    max_weight,_ = weight.reshape(weight.size(0), -1).max(1)\n",
    "                    min_weight,_ = weight.reshape(weight.size(0), -1).min(1)\n",
    "                    layer_quant_descr['w_max_thr'] = nn.Parameter( max_weight.cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_min_thr'] = nn.Parameter( min_weight.cuda(),requires_grad=True )\n",
    "                    layer_quant_descr['w_max_thr'].add(1).sum().backward() #fake to create grad\n",
    "                    layer_quant_descr['w_min_thr'].add(1).sum().backward() #fake to create grad\n",
    "                    layer_quant_descr['w_min_mat'] = None\n",
    "                    layer_quant_descr['w_max_mat'] = None\n",
    "                    if type(module) == nn.Linear:\n",
    "                        print('PerLayerAsymPACT chennel with fixed batch')\n",
    "                        out_ch = weight.size(0)\n",
    "                        quant_act = ScaledClippedLinearQuantizationChannelBias(out_ch, clip_val=False, bias=False)\n",
    "                        layer_quant_descr['quant_act'] = quant_act\n",
    "                        modules_quant.append(quant_act)\n",
    "\n",
    "                # append the latest node\n",
    "                param_to_quantize.append(layer_quant_descr)\n",
    "                last_layer = param_to_quantize[-1]\n",
    "\n",
    "            elif (type(module) in [nn.BatchNorm2d]):\n",
    "                if last_layer is not None:\n",
    "                    last_layer['batch_norm'] = module\n",
    "\n",
    "            elif type(module) in [nn.AvgPool2d]:\n",
    "                modules_quant.append(module) # temporary - this should me merged into or previous layer\n",
    "\n",
    "            elif type(module) in _Supported_Activ_Funct:\n",
    "\n",
    "                # check if a quantized features an activation function\n",
    "                if last_layer is not None:\n",
    "\n",
    "                    last_layer['act'] = module\n",
    "\n",
    "                    # quantized activations\n",
    "                    if type(module) in [ClippedLinearQuantization, LearnedClippedLinearQuantization, QuantActive]:\n",
    "\n",
    "                        act_bits = module.num_bits\n",
    "                        #check if number of bits need to be changed\n",
    "                        for item_dict in add_config:\n",
    "                            if item_dict['layer'] == idx_layer:\n",
    "                                if 'a_bits' in item_dict.keys():\n",
    "                                    act_bits = item_dict['a_bits'] \n",
    "                                    module.num_bits = act_bits                                 \n",
    "\n",
    "                        if last_layer['fold_type'] == 'folding_thresh':\n",
    "                            quant_act = ScaledThresholdsQuantization4d()\n",
    "                            quant_act.num_bits = act_bits\n",
    "                            quant_act.n_thresholds = 2**act_bits -1\n",
    "\n",
    "                        elif last_layer['fold_type'] == 'ICN':\n",
    "                            out_ch = last_layer['conv'].out_channels\n",
    "                            quant_act = ScaledClippedLinearQuantizationChannel(out_ch,clip_val=2**act_bits -1)\n",
    "\n",
    "                        else:\n",
    "                            quant_act = ScaledClippedLinearQuantization(clip_val=2**act_bits -1)\n",
    "\n",
    "                        last_layer['act_o_bits'] = act_bits\n",
    "\n",
    "                    # full-precision activations\n",
    "                    elif  type(module) is nn.ReLU6:\n",
    "                        quant_act = nn.ReLU6(inplace=True)\n",
    "\n",
    "                    # binary acivation\n",
    "                    elif  type(module) is BinActive :\n",
    "                        act_bits = 1\n",
    "                        quant_act = BinActive()\n",
    "\n",
    "                    else:\n",
    "                        print('Supported activation layer but no method is here yet!')\n",
    "\n",
    "                        \n",
    "\n",
    "                    last_layer['quant_act'] = quant_act\n",
    "\n",
    "                    modules_quant.append(quant_act)\n",
    "\n",
    "            else:\n",
    "                print(type(module), 'not supported yet!')\n",
    "\n",
    "        else:\n",
    "            #if type(module) is not nn.Sequential:\n",
    "            last_layer = None\n",
    "\n",
    "    deployment_model._modules[name_sub] = nn.Sequential(*modules_quant)\n",
    "    modules_quant = [  ] \n",
    "\n",
    "\n",
    "n_quantize_layers = len(param_to_quantize)\n",
    "\n",
    "print('This is the quantized network: ')\n",
    "print(deployment_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.0\n",
      "model.0.0\n",
      "model.0.1\n",
      "model.0.2\n",
      "model.1\n",
      "model.1.0\n",
      "model.1.1\n",
      "model.1.2\n",
      "model.2\n",
      "model.2.0\n",
      "model.2.1\n",
      "model.2.2\n",
      "model.3\n",
      "model.3.0\n",
      "model.3.1\n",
      "model.3.2\n",
      "model.4\n",
      "model.4.0\n",
      "model.4.1\n",
      "model.4.2\n",
      "model.5\n",
      "model.5.0\n",
      "model.5.1\n",
      "model.5.2\n",
      "model.6\n",
      "model.6.0\n",
      "model.6.1\n",
      "model.6.2\n",
      "model.7\n",
      "model.7.0\n",
      "model.7.1\n",
      "model.7.2\n",
      "model.8\n",
      "model.8.0\n",
      "model.8.1\n",
      "model.8.2\n",
      "model.9\n",
      "model.9.0\n",
      "model.9.1\n",
      "model.9.2\n",
      "model.10\n",
      "model.10.0\n",
      "model.10.1\n",
      "model.10.2\n",
      "model.11\n",
      "model.11.0\n",
      "model.11.1\n",
      "model.11.2\n",
      "model.12\n",
      "model.12.0\n",
      "model.12.1\n",
      "model.12.2\n",
      "model.13\n",
      "model.13.0\n",
      "model.13.1\n",
      "model.13.2\n",
      "model.14\n",
      "model.14.0\n",
      "model.14.1\n",
      "model.14.2\n",
      "model.15\n",
      "model.15.0\n",
      "model.15.1\n",
      "model.15.2\n",
      "model.16\n",
      "model.16.0\n",
      "model.16.1\n",
      "model.16.2\n",
      "model.17\n",
      "model.17.0\n",
      "model.17.1\n",
      "model.17.2\n",
      "model.18\n",
      "model.18.0\n",
      "model.18.1\n",
      "model.18.2\n",
      "model.19\n",
      "model.19.0\n",
      "model.19.1\n",
      "model.19.2\n",
      "model.20\n",
      "model.20.0\n",
      "model.20.1\n",
      "model.20.2\n",
      "model.21\n",
      "model.21.0\n",
      "model.21.1\n",
      "model.21.2\n",
      "model.22\n",
      "model.22.0\n",
      "model.22.1\n",
      "model.22.2\n",
      "model.23\n",
      "model.23.0\n",
      "model.23.1\n",
      "model.23.2\n",
      "model.24\n",
      "model.24.0\n",
      "model.24.1\n",
      "model.24.2\n",
      "model.25\n",
      "model.25.0\n",
      "model.25.1\n",
      "model.25.2\n",
      "model.26\n",
      "model.26.0\n",
      "model.26.1\n",
      "model.26.2\n",
      "model.27\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for item,v in model.named_modules():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be an iterable, not mobilenet_real",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ab779122673a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: type object argument after * must be an iterable, not mobilenet_real"
     ]
    }
   ],
   "source": [
    "list(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = 'mobilenet_real/Sequential[model]/Sequential[0]/Conv2d[0]/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.find('[')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_children of mobilenet_real(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6()\n",
      "    )\n",
      "    (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (8): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (9): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (10): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (11): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (12): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (13): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (14): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (15): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (16): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (17): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (18): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (19): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (20): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (21): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (22): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (23): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (24): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (25): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (26): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6()\n",
      "  )\n",
      "  (27): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      ")>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of Sequential(\n",
      "  (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6()\n",
      ")>\n",
      "<bound method Module.named_children of Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)>\n",
      "<bound method Module.named_children of BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)>\n",
      "<bound method Module.named_children of ReLU6()>\n",
      "<bound method Module.named_children of AvgPool2d(kernel_size=7, stride=7, padding=0)>\n",
      "<bound method Module.named_children of Linear(in_features=1024, out_features=1000, bias=True)>\n"
     ]
    }
   ],
   "source": [
    "for item in model.modules():\n",
    "    print(item.named_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-134-0615a994443e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-134-0615a994443e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    model.model.1.1\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mobilenet_real/Sequential[model]/Sequential[24]/Conv2d[0]/237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
